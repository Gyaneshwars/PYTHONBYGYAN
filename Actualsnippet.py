import hashlib
import pandas as pd
import numpy as np
import time
import functools
import hashlib
import time
import functools
import datetime as dt
from functools import wraps
import json
from decimal import Decimal
import ast
import re
from datetime import datetime
import math
from sys import argv
import os
import warnings
from functools import reduce
warnings.simplefilter(action='ignore', category=FutureWarning)


PEO_FORMAT = '%Y/%m/%d'
extractedData_parsed = None
historicalData_parsed = None
isDataParsed = False

def get_dataItemIds_list(dataItemId_key, parameters):
    """
    Function to read LHS and RHS dataItemIds and return list of dataItemIds.
    
    """
    parameter_dataItemIds = []
    if dataItemId_key in parameters.keys():
        for element in parameters[dataItemId_key]:
            dataItemIds = element.get('value', "").split(",")
            for dataItemId in dataItemIds:
                dataItemId = str(dataItemId).strip()
                if (dataItemId not in parameter_dataItemIds) and (dataItemId != ''):
                    parameter_dataItemIds.extend([dataItemId])
                        
    return parameter_dataItemIds


def get_dataItemIds_list2(dataItemId_key, parameters):
    """
    Function to read LHS and RHS dataItemIds and return list of dataItemIds.
    """
    parameter_dataItemIds = []
    if dataItemId_key in parameters.keys():
        for element in parameters[dataItemId_key]:
            dataItemIds = element.get('value', "").split(",")
            for dataItemId in dataItemIds:
                dataItemId = str(dataItemId).strip()
                if (dataItemId != ''):
                    parameter_dataItemIds.extend([dataItemId])
                        
    return parameter_dataItemIds

def get_country_list(country_key, parameters):
    """
    Function to read LHS and RHS dataItemIds and return list of dataItemIds.
    """
    parameter_country = []
    if country_key in parameters.keys():
        for element in parameters[country_key]:
            countries = element.get('value', "").split(",")
            for country in countries:
                country = str(country).strip()
                if (country not in parameter_country) and (country != ''):
                    parameter_country.extend([country])
                        
    return parameter_country

def get_scale_list(scale_key, parameters):
    """
    Function to read scalelist and return list of scale.
    """
    parameter_scale = []
    if scale_key in parameters.keys():
        for element in parameters[scale_key]:
            scales = element.get('value', "").split(",")
            for scale in scales:
                scale = str(scale).strip()
                if (scale not in parameter_scale) and (scale != ''):
                    parameter_scale.extend([scale])
                        
    return parameter_scale

def get_parameter_value(parameters, key):
    """
    Function to return list of parameter values
    """
    try:
        param_value=[]
        for pam in parameters[key]:
            param_value.append(pam.get('value'))
    except:
        param_value=''

    return param_value

def get_parameter_id(parameters, key):
    """
    Function to return list of parameter ids.
    """
    try:
        param_id=[]
        for pam in parameters[key]:
            param_id.append(pam.get('id'))
    except:
        param_id=''

    return param_id 

def execute_operator(lhsval, inputvalue, operator):
    """
    Function to execute operator on two values
    """
    if (operator == "!="):
        return lhsval != inputvalue
    elif (operator == ">"):
        return lhsval > inputvalue
    elif (operator == "<"):
        return lhsval < inputvalue
    elif (operator == "=="):
        return lhsval == inputvalue
    elif (operator == ">="):
        return lhsval >= inputvalue
    elif (operator == "<="):
        return lhsval <= inputvalue
    else:
        return True

def str_to_float(x):
    """
    Function to convert string to float
    """
    try:
        x = float(x)
    except:
        x = 0.0
    return x

# def get_scaled_value(value, scale):
#     """
#     Function to scale value based on the scales_dict
#     """
#     scales_dict = {'actual': 1, 'thousand': 1000, 'million': 1000000, 'billion': 1000000000, 'trillion': 1000000000000, 'tenth': 0.1, \
#                     'hundredth': 0.01, 'thousandth': 0.001, 'tenthousandth': 0.0001, 'dozen': 12, 'hundred': 100, 'lakh': 100000, \
#                     'crore': 10000000, 'bit': 12.5, 'score': 20, 'half': 0.5, 'pair': 2, 'gross': 144, 'ten': 10, 'myraid': 10000, \
#                     'millionth': 0.000001, 'billionth': 0.000000001, 'percendataItemIde': 100, 'fiveHundred': 500, 'hundred Million': 100000000,'none':1}
#     try:
#         if str(scale).lower() in scales_dict.keys():
#             return float(value) * scales_dict[str(scale).lower()]
#         else:
#             return 0
#     except:
#         return 0

def get_scaled_value(fullyAdjValue, scaleId):
    """
    Function to scale value based on the scales_dict
    """
    scales_dict = {5: 0.001,6: 0.0001,7: 1000000000000,8: 0.1,9: 100,10: 100000,11: 10000000,12: 10,13: 10000,14: 0.00001,16: 100,17: 0.001,18: 1,19: 1000,20: 173.913,21: 0.1739,22: 0.0001739,23: 7.33333,24: 7333.3333,25: 7333333.3,26: 0.00002381,27: 0.02381,28: 23.81,29: 0.0001667,30: 0.1667,31: 166.6667,32: 0.005885778,33: 5.886,34: 5886,35: 0.001,36: 1,37: 0.0001667,4: 0.01,3: 1,2: 1000,1: 1000000000,0: 1000000,38: 0.1667,39: 1000,40: 0.000000167,41: 166.6667,42: 157.68333,43: 0.15768,44: 0.00015768,45: 7.0922,46: 7092.1983,47: 7092198.3333,48: 0.001,49: 1,50: 1000,52: 0.00000016667,53: 0.0001667,54: 0.1666666667,55: 166.6666667,56: 166666.666667,57: 166666.6667,58: 166666.6667,59: 7.3333333,60: 7333.3333,61: 7333333.3,62: 100000000,63: 0.0073333333,64: 0.00588577778333333,65: 5.88577778333333,66: 5885.77778333333,67: 0.001,68: 1,69: 1000,70: 1000000,71: 0.0010978,72: 1.0978,73: 1097.8,74: 1097800,75: 35.29523,76: 35295.23,77: 35295230,78: 35295230000,79: 32.01963,80: 32019.63,81: 32019630,82: 32019630000,83: 3.586198,84: 3586.198,85: 3586198,86: 3586198000,87: 0.03529523,88: 0.01601,89: 16.01,90: 16010,91: 16010000,92: 0.0000353,93: 0.0353,94: 35.3,95: 35300,96: 35.29523,97: 35295.23,98: 35295230,99: 0.001,100: 0.01,101: 0.1,102: 1,103: 10,104: 100,105: 1000,106: 10000,107: 1000000,108: 0.00160934,109: 0.0160934,110: 0.160934,111: 1.60934,112: 16.0934,113: 160.934,114: 1609.34,115: 16093.4,116: 1609340,118: 0.00006,119: 0.00001,120: 0.001,121: 1,122: 0.006,123: 6,124: 1000,125: 0.000006,126: 6000,127: 6000000,128: 0.00006,129: 0.00001,-1:1}
    try:
        if scaleId in scales_dict.keys():
            return float(fullyAdjValue) * scales_dict[(scaleId)]
        else:
            return 0
    except:
        return 0

def correct_parameter_bools(df):
    """
    Function to correct the 'ALL' parameter booleans.
    
    This function corrects the 'ALL' parameter boolean in output dataframe in case the child row values are empty 
    for a particular PEO and FPO.

    Parameters
    ----------
    1. df: Pandas DataFrame
        df contains the parsed json output.
    
    Returns
    --------
    1. df: Pandas DataFrame
        Correct pandas dataframe with corrections made for 'ALL' parameter boolean.
    """
    # Reset Dataframe indices.
    df = df.reset_index(drop=True)

    # Get all child rows with null values.
    null_child = df[(df['is_child_row']==1) & (pd.to_numeric(df['value'], errors='coerce').isin([0.0, np.nan]))]

    if len(null_child) > 0:
        # Groupby child rows based on peo, fpo and parent instance and  
        # consider only those instances where count is equal to number of children of that parent.
        null_child = null_child[null_child.groupby(['peo', 'fpo', 'parent_instance'])['peo'].transform('size') == null_child['num_children']]

        if len(null_child) > 0:
            # Get the indexes for which all_bool needs to be changed to 0.
            remove_indexes = list(null_child.index)

            # Get indexes of parent rows corresponding to children with null values.
            null_child = null_child.drop_duplicates(subset=['peo', 'fpo', 'parent_instance'])
            null_child['instance'] = null_child['parent_instance']
            new_indexes = list(df.merge(null_child, on=['peo', 'fpo', 'instance'], how='left', indicator=True).query('_merge == "both"').index)

            # Change all_bool accoridngly.
            df.loc[remove_indexes, 'all_bool'] = 0
            df.loc[new_indexes, 'all_bool'] = 1
    
    return df


def currency_converter(currency_from, currency_to , value):

    if (currency_from==currency_to):
        return value*1
    if (currency_from in list(currencyConversion_parsed['from'])):
        temp = currencyConversion_parsed[currencyConversion_parsed['from']==currency_from]
        if currency_to in list(temp['to']):
            factor=temp[(temp['from']==currency_from) & (temp['to']==currency_to)]['factor'].iloc[0]
            return value*factor
    elif (currency_to in list(currencyConversion_parsed['from'])):
        temp = currencyConversion_parsed[currencyConversion_parsed['from']==currency_to]
        if currency_from in list(temp['to']):
            factor=1/(temp[(temp['from']==currency_to) & (temp['to']==currency_from)]['factor'].iloc[0])
            return value*factor

def get_metadata_items(metadata_rows, metadata, num_rows):
    """
    Function to get metadata values from metadata dictionary.

    Takes input the metadata dictionary, and returns dictionary of metadata values to be extracted.

    Parameters
    ---------
    1. metadata_rows: dict
        dict with keys as metadata keys to extract and values as list of extracted values.
    2. metadata: dict
        metadata dictionary of a single history in json.
    3. num_rows: int
        Count of values extracted from the json.
    
    Returns
    -------
    1. metadata_rows: dict
        metadata dict with appended values for the particular metadata
    """
    for key in metadata_rows.keys():
        # Append metadata value to metadata key list num_rows times to maintain output df length.
        metadata_rows[key].extend([metadata.get(key)]*num_rows)
    return metadata_rows




def parse_extracted_data(extracted_data, convert_to_df = False):
    """
    Function to parse extracted data jsons.
    Boolean convert_to_df is True if only extracted_data is to be parsed.
    By default convert_to_df is False, so that complete output of history can be converted to DF in a single command. 
    Parameters
    ----------
    1. extracted_data: dict
        Dict of extracted data from the json to be parsed.
    2. convert_to_df: bool
        Boolean to check if the output is to be converted to a dataframe or returned as list of rows.
    
    Returns
    -------
    1. output: list or DataFrame
        Contains rows of parsed extracted data.
    """
    #print(extracted_data)
    output = []
    # Define output df column names in case empty DF is to be initialized.
    column_names = ["primaryParentFlag", "tradingItemName", "tradingItemId","periodTypeId","fiscalQuarter","fiscalYear","actualizedDate","periodEndDate","estimatePeriodId","parentFlag","accountingStandardDesc","auditTypeId","auditTypeName","fiscalChainSeriesId","splitFactor","team","userName","securityName","TickerSymbol","exchangeSymbol","tidCurrency","lastTradedDate","tradingItemStatus","tidPrimaryFlag","peo","value","currency","scale","consValue","consNotes","consScale","consCurrency"]
    # Loop through dataItemIds, parent instance and childrows in-case cildwors are present.
    for dataItemId in extracted_data:
        
        for parent_id, parent_inst in extracted_data[dataItemId].items():
            # Ignore parent instances for with isChildRow is True. This is an error in JSON.
            #if ('values' in parent_inst.keys()) and (not parent_inst['isChildRow']):
            if ('values' in parent_inst.keys()):
                values = parent_inst['values']
                # temp_dict has datapoints common for all values in values list. Append temp_dict to each value in values. 
                temp_dict = {'dataItemId': parent_inst['dataItemId'], 'description': parent_inst.get('description'),'dataItemFlag': parent_inst.get('dataItemFlag')}
                [value.update(temp_dict) for value in values]
                output.extend(values)
    # conver_to_df if parsing is not being done for historicalData.
    if convert_to_df:
        output = pd.DataFrame(output)
        if len(output)==0:
            output = pd.DataFrame([], columns=column_names)
        if len(output) > 0:
            output['value_scaled'] = output.apply(lambda row: get_scaled_value(row['fullyAdjValue'], row['scaleId']), axis=1)
        # Correct all_bool for null chil value cases.
        #output = correct_parameter_bools(output)
    return output

def parse_historical_data(historicalData):
    """
    Function to parse complete historicalData.
    
    Loops over each history and calls the function to parse extracted data.
    Parameters
    ----------
    1. historicalData: list
        List of dictionaries of complete historical data.
    
    Returns
    -------
    1. output: Pandas DataFrame
        Dataframe of parsed historical data rows along with metadata and some custom columns.
    """
    # Define metadata_column_names. Can add to the list of more keys need to be extracted.
    # Define all column names and initialize metadata_columns.
    metadata_column_names = ["versionId","companyId","researchContributorId","companyName","industry","industryId","country","latestActualizedPeo","latestPeriodType","latestPeriodYear","fiscalYearEnd","filingDate","language","heading","versionFormat","documentId","sourceId","companyrank","priorityid","tier","primaryEpsFlag","feedFileId"]
    metadata_columns = {name:[] for name in metadata_column_names}
    extracted_data_columns =["primaryParentFlag", "tradingItemName", "tradingItemId","periodTypeId","fiscalQuarter","fiscalYear","actualizedDate","periodEndDate","estimatePeriodId","parentFlag","accountingStandardDesc","auditTypeId","auditTypeName","fiscalChainSeriesId","splitFactor","team","userName","securityName","TickerSymbol","exchangeSymbol","tidCurrency","lastTradedDate","tradingItemStatus","tidPrimaryFlag","peo","value","currency","scale","consValue","consNotes","consScale","consCurrency"]
    column_names = extracted_data_columns + metadata_column_names + ['periodEndDate_parsed']
    output = []
    # Loop over all histories and call parse_extracted_data.
    # For each history, parse the metadata and update metadata columns.
    for hist in historicalData:
        parsed_hist = parse_extracted_data(hist['extractedData'])
        output.extend(parsed_hist)
        metadata_columns = get_metadata_items(metadata_columns,  hist['metadata'], len(parsed_hist))
    output = pd.DataFrame(output)
    if len(output) > 0:
        output['value_scaled'] = output.apply(lambda row: get_scaled_value(row['fullyAdjValue'], row['scaleId']), axis=1)
    # Merge metadata_columns with output dataframe.
    output = pd.concat([output, pd.DataFrame(metadata_columns)], axis=1)
    if len(output)==0:
        return pd.DataFrame([], columns=column_names)
    # Parse periodEndDate and peo.
    output['periodEndDate_parsed'] = pd.to_datetime(output['periodEndDate'])
    #output['peo_parsed'] = pd.to_datetime(output['peo'])
    # Correct all_bool for null chil value cases.
    #output = correct_parameter_bools(output)
    return output

def parse_conversion_data(conversionData, convert_to_df = False):
    """
    Function to parse extracted data jsons.
    Boolean convert_to_df is True if only extracted_data is to be parsed.
    By default convert_to_df is False, so that complete output of history can be converted to DF in a single command. 
    Parameters
    ----------
    1. extracted_data: dict
        Dict of extracted data from the json to be parsed.
    2. convert_to_df: bool
        Boolean to check if the output is to be converted to a dataframe or returned as list of rows.
    Returns
    -------
    1. output: list or DataFrame
        Contains rows of parsed extracted data.
    """
    output = []
    # Define output df column names in case empty DF is to be initialized.
    column_names = ["from","to","factor"]
    # Loop through dataItemIds, parent instance and childrows in-case cildwors are present.
    for dataItemId in conversionData:
        for parent_id, parent_inst in conversionData[dataItemId].items():
            if parent_id=='filingDate':
                filingDate=parent_inst
            if parent_id=='values':
                values=parent_inst
                temp_dict={'filingDate': filingDate}
                [value.update(temp_dict) for value in values]
                output.extend(values)
    # conver_to_df if parsing is not being done for historicalData.
    if convert_to_df:
        output = pd.DataFrame(output)
        if len(output)==0:
            output = pd.DataFrame([], columns=column_names)
    return output

def measure_time(f):

    @functools.wraps(f)
    def timed(*args, **kw):
        ts = time.time()
        result = f(*args, **kw)
        te = time.time()

        print( "{0} {1:2.2f} sec".format(f.__name__, te-ts))
        return result

    return timed


def fetch_comp(a, historicalData, filingMetadata):
    import datetime
    def months(d1, d2):
        return d1.month - d2.month + 12*(d1.year - d2.year)
    filings_list=[]
    ref_filings={}
    comp_type=a

    if comp_type==1:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]=='Reg' \
                    and filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and filingMetadata['metadata']['fiscalQuarter']==hist['metadata']['fiscalQuarter']:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==2:
        pass

    elif comp_type==3:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:2]=='PR' \
                    and filingMetadata['metadata']['periodType']=='ANNUAL' \
                    and filingMetadata['metadata']['docType'][:3]== hist['metadata']['docType'][:3] \
                    and filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and filingMetadata['metadata']['periodType']!=hist['metadata']['periodType'] \
                    and hist['metadata']['fiscalQuarter']=='4' \
                    and filingMetadata['metadata']['fiscalQuarter']=='4':
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==4:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:2]=='PR' \
                    and filingMetadata['metadata']['docType']!=hist['metadata']['docType'] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],PEO_FORMAT))) < 15):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==5:
        for hist in historicalData:
            if ('amd' in filingMetadata['metadata']['docType'].lower() or 'amend' in filingMetadata['metadata']['docType'].lower()) \
                    and filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and ('amd' not in hist['metadata']['docType'].lower() and 'amend' not in filingMetadata['metadata']['docType'].lower()):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==6:
        for hist in historicalData:
            if ('amd' in filingMetadata['metadata']['docType'].lower() or 'amend' in filingMetadata['metadata']['docType'].lower()) \
                    and filingMetadata['metadata']['docType'][:3]== hist['metadata']['docType'][:3] \
                    and filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType']:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==7:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and (((int(filingMetadata['metadata']['fiscalQuarter']) - int(hist['metadata']['fiscalQuarter']) == 1 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                    PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                            PEO_FORMAT))) < 5) \
                          or (filingMetadata['metadata']['fiscalQuarter'] == '1' and hist['metadata']['fiscalQuarter'] == '3' \
                              and (abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                         PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                 PEO_FORMAT))) >= 12 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                               PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                       PEO_FORMAT))) <= 15))) \
                         or (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                        PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                PEO_FORMAT))) >= 11 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                              PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                                                                      PEO_FORMAT))) <= 15)) \
                    and ((7 in filingMetadata['metadata']['filingtypeid'] and filingMetadata['metadata']['filingtypeid'] == hist['metadata']['filingtypeid'] or (7 not in filingMetadata['metadata']['filingtypeid']))):
                filings_list.append(hist['metadata']['filingId'])



    elif comp_type==8:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]== hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                    PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                            PEO_FORMAT))) < 15):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==9:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT))) < 14:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==10:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>=datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                        PEO_FORMAT):
                filings_list.append(hist['metadata']['filingId'])



    elif comp_type==11:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>=datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                        PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType'] != 'ANNUAL' \
                    and filingMetadata['metadata']['periodType']!=hist['metadata']['periodType'] \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT))) < 12:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==12:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType'] == 'ANNUAL' \
                    and filingMetadata['metadata']['periodType']!=hist['metadata']['periodType'] \
                    and filingMetadata['metadata']['fiscalQuarter']=='4' \
                    and hist['metadata']['fiscalQuarter']=='3' \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT))) < 5:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==13:

        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and (( filingMetadata['metadata']['periodType']==hist['metadata']['periodType']) \
                         or ( filingMetadata['metadata']['periodType'] == 'ANNUAL' and hist['metadata']['periodType'] != 'ANNUAL')) \
                    and ((int(filingMetadata['metadata']['fiscalQuarter']) - int(hist['metadata']['fiscalQuarter']) == 1 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                   PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                           PEO_FORMAT))) < 5) \
                         or (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] \
                             and filingMetadata['metadata']['periodType'] == hist['metadata']['periodType'] \
                             and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                            PEO_FORMAT))) >= 12 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                                                          PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                                  PEO_FORMAT))) <= 15)) \
                    and ((7 in filingMetadata['metadata']['filingtypeid'] and filingMetadata['metadata']['filingtypeid'] == hist['metadata']['filingtypeid'] or (7 not in filingMetadata['metadata']['filingtypeid']))):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==14:

        for hist in historicalData:
            if datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                              PEO_FORMAT) \
                    and ((filingMetadata['metadata']['periodType']==hist['metadata']['periodType']) \
                         or (filingMetadata['metadata']['periodType'] == 'ANNUAL' and hist['metadata']['periodType'] != 'ANNUAL')) \
                    and ((int(filingMetadata['metadata']['fiscalQuarter']) - int(hist['metadata']['fiscalQuarter']) == 1 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                   PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                           PEO_FORMAT))) < 5) \
                         or (filingMetadata['metadata']['fiscalQuarter'] == '1' and hist['metadata']['fiscalQuarter'] == '3' and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                       PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                               PEO_FORMAT))) >= 5 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                                            PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                                                                                    PEO_FORMAT))) <= 7) \
                         or (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                        PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                PEO_FORMAT))) >= 12 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                              PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                                                                      PEO_FORMAT))) <= 15)) \
                    and (( 7 in filingMetadata['metadata']['filingtypeid'] and filingMetadata['metadata']['filingtypeid'] == hist['metadata']['filingtypeid'] or (7 not in filingMetadata['metadata']['filingtypeid']))):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==15:

        for hist in historicalData:
            if filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==16:

        for hist in historicalData:
            if abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                          PEO_FORMAT)))<25:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==17:

        for hist in historicalData:
            if (hist['metadata']['amendmentFlag'] == 0 or 7 in hist['metadata']['filingtypeid']) \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and ((filingMetadata['metadata']['periodType'] == hist['metadata']['periodType']) \
                         or (filingMetadata['metadata']['periodType'] == 'ANNUAL' and hist['metadata']['periodType'] != 'ANNUAL') \
                         and ((int(filingMetadata['metadata']['fiscalQuarter']) - int(hist['metadata']['fiscalQuarter']) == 1 \
                               and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                         PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                 PEO_FORMAT))) < 5) \
                              or (filingMetadata['metadata']['fiscalQuarter'] == '1' and hist['metadata']['fiscalQuarter'] == '3' \
                                  and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                            PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                    PEO_FORMAT))) >= 5 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                 PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                         PEO_FORMAT))) <= 7) \
                              or (b.fiscalQuarter == c.fiscalQuarter and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                               PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                       PEO_FORMAT))) >= 11 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                                                                     PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                                                             PEO_FORMAT))) <= 13)) \
                         and ((7 in filingMetadata['metadata']['filingtypeid'] \
                               and filingMetadata['metadata']['filingtypeid'] == hist['metadata']['filingtypeid']) or (7 not in filingMetadata['metadata']['filingtypeid']))):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==18:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]=='Reg' \
                    and filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and filingMetadata['metadata']['fiscalQuarter']==hist['metadata']['fiscalQuarter']:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==19:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT))) < 7 \
                    and filingMetadata['metadata']['fiscalQuarter'] in ['2','3'] \
                    and int(hist['metadata']['fiscalQuarter'])== int(filingMetadata['metadata']['fiscalQuarter']) - 1:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==20:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType'] == 'ANNUAL' \
                    and filingMetadata['metadata']['periodType']!=hist['metadata']['periodType'] \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT))) < 12:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==21:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]=='Reg' \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and (filingMetadata['metadata']['fiscalQuarter'] == hist['metadata']['fiscalQuarter'] and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                    PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                            PEO_FORMAT))) < 15):
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==22:

        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and filingMetadata['metadata']['amendmentFlag'] == hist['metadata']['amendmentFlag'] \
                    and ((filingMetadata['metadata']['periodType'] == 'ANNUAL' and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                         PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                 PEO_FORMAT))) < 26) \
                         or (abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                        PEO_FORMAT))) < 8)):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==23:
        for hist in historicalData:
            if filingMetadata['metadata']['docType'][:3]== hist['metadata']['docType'][:3] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and ((int(filingMetadata['metadata']['fiscalQuarter']) - int(hist['metadata']['fiscalQuarter']) == 1 \
                          and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                         PEO_FORMAT))) < 5) \
                         or (filingMetadata['metadata']['fiscalQuarter']==hist['metadata']['fiscalQuarter'] \
                             and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                             and (abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                        PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                PEO_FORMAT))) >= 12 and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'],
                                                                                                                                                                              PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                                                                                      PEO_FORMAT))) <= 15))):
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==24:
        for hist in historicalData:
            if abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                          PEO_FORMAT)))<=13:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==25:
        for hist in historicalData:
            if filingMetadata['metadata']['periodEndDate']==hist['metadata']['periodEndDate'] \
                    and abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                                   PEO_FORMAT)))<=12:
                filings_list.append(hist['metadata']['filingId'])


    elif comp_type==26:
        for hist in historicalData:
            if 7 in filingMetadata['metadata']['filingtypeid'] \
                    and datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT)>datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                       PEO_FORMAT) \
                    and filingMetadata['metadata']['periodType'] == 'ANNUAL' \
                    and months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                               PEO_FORMAT))<=12:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==27:
        for hist in historicalData:
            if abs(months(datetime.datetime.strptime(filingMetadata['metadata']['periodEndDate'], PEO_FORMAT), datetime.datetime.strptime(hist['metadata']['periodEndDate'],
                                                                                                                                          PEO_FORMAT)))<=13 \
                    and filingMetadata['metadata']['periodType']==hist['metadata']['periodType'] \
                    and filingMetadata['metadata']['docType'][:3]!= hist['metadata']['docType'][:3]:
                filings_list.append(hist['metadata']['filingId'])

    elif comp_type==28:
        for hist in historicalData:
            if filingMetadata['metadata']['periodType']=='QUARTERLY' \
                    and hist['metadata']['periodType']=='ANNUAL' \
                    and filingMetadata['metadata']['periodEndDate']>hist['metadata']['periodEndDate']:
                filings_list.append(hist['metadata']['filingId'])


    return filings_list

def add_method(cls):
    def decorator(func):
        from functools import wraps
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            return func(*args, **kwargs)
        setattr(cls, func.__name__, wrapper)
        # Note we are not binding func, but wrapper which accepts self but does exactly the same as func
        return func # returning func means func can still be used normally
    return decorator

class Validation(object):
    def generateHash(self, ruleName, result):
        return hashlib.sha256((ruleName + str(result)).encode()).hexdigest()

    def appendError(self, errorMap, errorMessage, ruleName, associationId):
        if errorMessage:
            self.appendMessages(errorMap, {"error": errorMessage}, ruleName, associationId)

    def appendMessage(self, errorMap, result, ruleName, associationId):
        if result:
            errorobject = {"ruleName": ruleName, "isError": True, "result": result, "hash": self.generateHash(ruleName, result), "associationId": associationId}
            errorMap.append(errorobject)
        else:
            errorobject = {"ruleName": ruleName, "isError": False, "result": result, "hash": self.generateHash(ruleName, result), "associationId": associationId}
            errorMap.append(errorobject)

    def appendMessages(self, errorMap, result, ruleName, associationId):
        if isinstance(result, list):
            # append an object is empty list convert to object
            if not result:
                self.appendMessage(errorMap, {}, ruleName, associationId)
            for r in result:
                self.appendMessage(errorMap, r, ruleName, associationId)
        else:
            self.appendMessage(errorMap, result, ruleName, associationId)

    def filterdataItemIdLevel(self, executionData):
        if executionData["ruleType"] == "dataItemId" and not executionData["isAggregateRule"]:
            return True
        else:
            return False

    def filterAggregatedataItemId(self, executionData):
        if executionData["ruleType"] == "dataItemId" and executionData["isAggregateRule"]:
            return True
        else:
            return False

    def filterDocumentLevelRules(self, executionData):
        if executionData["ruleType"] == "document":
            return True
        else:
            return False

    def filterCalculationLevelRules(self, executionData):
        if executionData["ruleType"] == "calculation":
            return True
        else:
            return False

    def getFilingMetadata(self, extractedData):
        import json
        if type(extractedData['filingMetadata']) is not dict:
            filingMetadata = json.loads(extractedData['filingMetadata'])
        else:
            filingMetadata = extractedData['filingMetadata']
        return filingMetadata

    def validate(self, extractedData, executionData):
        errorMap = []
        self.rundataItemIdLevelRules(errorMap, executionData, extractedData)
        self.runAggregatedataItemIdRules(errorMap, executionData, extractedData)
        self.runDocumentLevelRules(errorMap, executionData, extractedData)
        self.runCalculationLevelRules(errorMap, executionData, extractedData)

        return errorMap

    def runCalculationLevelRules(self, resultMap, executionData, extractedData):
        for rule in filter(self.filterCalculationLevelRules, executionData):
            ruleName = rule['ruleName']
            associationId = rule['associationId']
            try:
                # print("Calculation Rule -> " + rule['ruleName'] + " Primary dataItemId -> " + rule['primarydataItemId'])
                functionargs = {'dataItemId1': extractedData['extractedData'][rule['primarydataItemId']], 'filingMetadata': self.getFilingMetadata(extractedData), 'historicalData': extractedData['historicalData']}

                if len(rule['associateddataItemIds']) > 0:
                    dataItemIdnumber =2
                    for supportingdataItemId in rule['associateddataItemIds']:
                        if supportingdataItemId in extractedData['extractedData']:
                            functionargs['dataItemId' + str(dataItemIdnumber)] = extractedData['extractedData'][supportingdataItemId]
                        else:
                            functionargs['dataItemId' + str(dataItemIdnumber)] = {}
                        dataItemIdnumber = dataItemIdnumber + 1

                self.addParameterValuesToArgs(functionargs, rule)

                func = getattr(self, rule['ruleName'])
                if func:
                    #print('runCalculationLevelRules:', ruleName)
                    calculationresult = self.executeFunctionWithArgs(func, functionargs)
                    if 'error' in calculationresult:
                        self.appendError(resultMap, calculationresult['error'], ruleName, associationId)

                    must = {"value", "description", "type"}
                    if len(calculationresult) >= len(must) and all(key in calculationresult for key in must):
                        self.appendMessages(resultMap, calculationresult, ruleName, associationId)
                    else:
                        self.appendError(resultMap, "value, description and type are required attributes in calculation results", ruleName,
                                         associationId)
            except KeyError as k:
                self.appendError(resultMap, "Key Error -> " + str(k), ruleName, associationId)
            except Exception as e:
                self.appendError(resultMap, str(e), ruleName, associationId)

    def validateHighlight(self, highlight):
        highlightmust = {"versionId", "filingDate"}
        if not (len(highlight) > 0 and all(key in highlight for key in highlightmust)):

            raise ValueError('versionId and filingDate are required attributes for highlight')

        allowed = {"versionid", "filingDate", "dataItemId"}
        if not (len(highlight) > 0 and any(key in highlight for key in allowed)):
            raise ValueError('At least one of "versionid", "filingDate", "dataItemId" attribute is required for highlight')
        else:
            headermust = {"peo",  "scale", "currency"}
            if 'header' in highlight.keys() and not (len(highlight['header']) >= len(headermust) and all(key in highlight['header'] for key in headermust)):
                raise ValueError('[peo,  scale, currency] are required attributes for each header highlights')
            rowmust = {"companyid"}
            if 'row' in highlight.keys() and not (len(highlight['row']) >= len(rowmust) and all(key in highlight['row'] for key in rowmust)):
                raise ValueError('[companyid] is required attributes for each row highlights')
            cellmust = {"peo", "scale", "value", "currency"}
            if 'cell' in highlight.keys() and not (len(highlight['cell']) >= len(cellmust) and all(key in highlight['cell'] for key in cellmust)):
                raise ValueError('peo, scale, value, currency  are required attributes for each cell highlights')
            dataItemIdmust = {"dataItemId"}
            if 'dataItemId' in highlight.keys() and not (len(highlight['dataItemId']) >= len(dataItemIdmust) and all(key in highlight['dataItemId'] for key in dataItemIdmust)):
                raise ValueError('[dataItemId] is required attribute for each row')


    def validateErrors(self, errors):
        for error in errors:
            must = {"error", "highlights"}
            if len(error) == 0:
                return
            elif not (len(error) >= len(must) and all(key in error for key in must)):
                raise ValueError('[error and highlights] are required attributes for each error')
            else:
                for highlight in error['highlights']:
                    self.validateHighlight(highlight)

    def runDocumentLevelRules(self, errorMap, executionData, extractedData):
        for rule in filter(self.filterDocumentLevelRules, executionData):
            ruleName = rule['ruleName']
            associationId = rule['associationId']
            #print("Document -> " + ruleName)
            try:
                functionargs = {'filingMetadata': self.getFilingMetadata(extractedData), 'extractedData': extractedData['extractedData'], 'historicalData': extractedData['historicalData']}
                func = getattr(self, rule['ruleName'])
                #print("Check1")
                if func:
                    self.addParameterValuesToArgs(functionargs, rule)
                    #print('runDocumentLevelRules:', ruleName)
                    errors = self.executeFunctionWithArgs(func, functionargs)
                    self.validateErrors(errors)
                    self.appendMessages(errorMap, errors, ruleName, associationId)
                
            except ValueError as v:
                #print("Check Value Error")
                self.appendError(errorMap, "Invalid Error Check -> " + str(v), ruleName, associationId)
            except KeyError as k:
                #print("Check Key Error")
                self.appendError(errorMap, "Key Error -> " + str(k), ruleName, associationId)
            except Exception as e:
                #print("Check Error")
                self.appendError(errorMap, str(e), ruleName, associationId)

    def rundataItemIdLevelRules(self, errorMap, executionData, extractedData):
        for rule in filter(self.filterdataItemIdLevel, executionData):
            ruleName = rule['ruleName']
            associationId = rule['associationId']
            #print("dataItemId Level Rule --> " + rule['ruleName'] + " Primary dataItemId -> " + rule['primarydataItemId'])

            for fieldId, fieldValue in extractedData['extractedData'][rule['primarydataItemId']].items():
                try:
                    # print("Rule -> " + rule['ruleName'] + " dataItemId -> " + str(fieldValue['dataItemId']) + " fieldId -> " + fieldId)
                    functionargs = {'dataItemId1': fieldValue, 'filingMetadata': self.getFilingMetadata(extractedData), 'historicalData': extractedData['historicalData']}
                    if len(rule['associateddataItemIds']) > 0:
                        dataItemIdnumber = 2
                        for supportingdataItemId in rule['associateddataItemIds']:
                            if supportingdataItemId in extractedData['extractedData'] and len(extractedData['extractedData'][supportingdataItemId]) > 0:
                                functionargs['dataItemId' + str(dataItemIdnumber)] = list(extractedData['extractedData'][supportingdataItemId].values())[0]
                            else:
                                functionargs['dataItemId' + str(dataItemIdnumber)] = {}
                            dataItemIdnumber = dataItemIdnumber + 1

                    func = getattr(self, rule['ruleName'])
                    if func:
                        self.addParameterValuesToArgs(functionargs, rule)
                        #print('rundataItemIdLevelRules:', ruleName)
                        errors = self.executeFunctionWithArgs(func, functionargs)
                        self.validateErrors(errors)
                        self.appendMessages(errorMap, errors, ruleName, associationId)
                except ValueError as v:
                    self.appendError(errorMap, "Invalid Error Check -> " + str(v), ruleName, associationId)
                except KeyError as k:
                    self.appendError(errorMap, "Key Error -> " + str(k), ruleName, associationId)
                except Exception as e:
                    self.appendError(errorMap, str(e), ruleName, associationId)

    def runAggregatedataItemIdRules(self, errorMap, executionData, extractedData):
        for rule in filter(self.filterAggregatedataItemId, executionData):
            ruleName = rule['ruleName']
            associationId = rule['associationId']
            try:
                # print("Aggregate Rule -> " + rule['ruleName'] + " Primary dataItemId -> " + rule['primarydataItemId'])
                functionargs = {'dataItemId1': extractedData['extractedData'][rule['primarydataItemId']], 'filingMetadata': self.getFilingMetadata(extractedData), 'historicalData': extractedData['historicalData']}

                if len(rule['associateddataItemIds']) > 0:
                    dataItemIdnumber =2
                    for supportingdataItemId in rule['associateddataItemIds']:
                        if supportingdataItemId in extractedData['extractedData']:
                            functionargs['dataItemId' + str(dataItemIdnumber)] = extractedData['extractedData'][supportingdataItemId]
                        else:
                            functionargs['dataItemId' + str(dataItemIdnumber)] = {}
                        dataItemIdnumber = dataItemIdnumber + 1

                func = getattr(self, rule['ruleName'])
                if func:
                    self.addParameterValuesToArgs(functionargs, rule)
                    #print('runAggregatedataItemIdRules:', ruleName)
                    errors = self.executeFunctionWithArgs(func, functionargs)
                    self.validateErrors(errors)
                    self.appendMessages(errorMap, errors, ruleName, associationId)
            except ValueError as v:
                self.appendError(errorMap, "Invalid Error Check -> " + str(v), ruleName, associationId)
            except KeyError as k:
                self.appendError(errorMap, "Key Error -> " + str(k), ruleName, associationId)
            except Exception as e:
                self.appendError(errorMap, str(e), ruleName, associationId)

    def addParameterValuesToArgs(self, functionargs, rule):
        if rule['numberOfAdditionalParameters'] > 0:
            if rule['additionalParameters']:
                functionargs["parameters"] = rule['additionalParameters']
            else:
                functionargs["parameters"] = {}

        if rule["hasAssociatedStrings"]:
            if "associatedStrings" in rule and len(rule['associatedStrings']) > 0:
                functionargs["associatedStrings"] = rule['associatedStrings']
            else:
                functionargs["associatedStrings"] = []

    def executeFunctionWithArgs(self, func, functionargs):
        # print("Executing Rule -> " + func.__name__ + " functionargs -> " + str(functionargs))
        return func(**functionargs)
#Estimates Error Checks 
@add_method(Validation)
def EST_56E(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    ni_gaap=get_dataItemIds_list('TAG1', parameters) #ni_gaap_df
    ni_nor=get_dataItemIds_list('TAG2', parameters) #ni_nor_df
    eps_gaap=get_dataItemIds_list('TAG3', parameters) #eps_gaap_df
    eps_nor=get_dataItemIds_list('TAG4', parameters) #eps_nor_df
    operator=get_dataItemIds_list('Operation', parameters) #['>','<']
    try:
        # companyid=filingMetadata['metadata']['companyId']
        
        ni_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        ni_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                          
        eps_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        eps_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                          

    
        # print(ni_gaap_df)
        # print(ni_nor_df)
        # print(eps_gaap_df)
        # print(eps_nor_df)
        
        #print(ni_gaap_df['value_scaled'],ni_nor_df['value_scaled'],eps_gaap_df['value_scaled'],eps_nor_df['value_scaled'])

        ni_merged_df=pd.merge(ni_gaap_df,ni_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        eps_merged_df=pd.merge(eps_gaap_df,eps_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        merged_df=pd.merge(ni_merged_df,eps_merged_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')

        # print(ni_merged_df)
        # print(eps_merged_df)

        # print(merged_df)
        # print(merged_df[['value_scaled_x_x','value_scaled_y_x','value_scaled_x_y','value_scaled_y_y']])
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        # if merged_df is not None:
        for ind,row in merged_df.iterrows():
             if (execute_operator(row['value_scaled_x_x'],row['value_scaled_y_x'],operator[0]) & execute_operator(row['value_scaled_x_y'],row['value_scaled_y_y'],operator[1])):
                 peos.append(row['peo'])
                 diff='NA'
                 perc='NA'
                 tid.append(row['tradingItemId_x'])
                 parentflag.append(row['parentFlag'])
                 accounting.append(row['accountingStandardDesc'])
                 fyc.append(row['fiscalChainSeriesId'])
        for ind,row in merged_df.iterrows():
             if (execute_operator(row['value_scaled_y_x'],row['value_scaled_x_x'],operator[0]) & execute_operator(row['value_scaled_y_y'],row['value_scaled_x_y'],operator[1])):
                 peos.append(row['peo'])
                 diff='NA'
                 perc='NA'
                 tid.append(row['tradingItemId_y'])
                 parentflag.append(row['parentFlag'])
                 accounting.append(row['accountingStandardDesc'])
                 fyc.append(row['fiscalChainSeriesId'])
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
        temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(ni_gaap)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                           &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))
                                          |((extractedData_parsed['dataItemId'].isin(ni_nor)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                            &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))
                                          |((extractedData_parsed['dataItemId'].isin(eps_gaap)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                            &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))
                                          |((extractedData_parsed['dataItemId'].isin(eps_nor)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                            &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))))].drop_duplicates()
            
        temp1_revised=temp1.dropna()                                                                                                                                                                                                      

        for ind, row in temp1_revised.iterrows():
            result = {"highlights": [], "error": "NI GAAP & NI Normalized Increased whereas EPS GAAP & EPS Normalized decreases and vice versa"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
            errors.append(result)      
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors
    
    
#Estimates Error Checks 
@add_method(Validation)
def EST_57(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    recommendation = get_dataItemIds_list('LHSdataItemIds', parameters)
    targetprice = get_dataItemIds_list('RHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)

    try:

        filingdate = filingMetadata['metadata']['filingDate']
        contributor = filingMetadata['metadata']['researchContributorId']
        companyid = filingMetadata['metadata']['companyId']
               
        current_rating = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(recommendation)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId','value','tradingItemId','accountingStandardDesc']]
        
        # current_rating['numaric_value']=current_rating['value'].str[-2].astype(int)
        # current_rating['numeric_value'] = current_rating['value'].str[-2:].astype(int)
        current_rating['numeric_value'] = current_rating['value'].str.rstrip('.0').astype(int)
        
        # print(current_rating)
        # current_rating['numeric_value'] = current_rating['value'].apply(lambda x: float(re.search(r'\((\d+\.\d+)\)', x).group(1)))

        previous_rating = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(recommendation)) & (historicalData_parsed['researchContributorId'] == contributor)& (historicalData_parsed['companyId'] == companyid) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][
            ['dataItemId','value','tradingItemId','accountingStandardDesc', 'filingDate']]
        
        # previous_rating['numaric_value']=int(previous_rating['value'].str[-2])
        #previous_rating['numeric_value'] = previous_rating['value'].str[-2:].astype(int)
        
        previous_rating['numeric_value'] = previous_rating['value'].str.rstrip('.0').astype(int)
        
        print(previous_rating)
        
        # previous_rating['numeric_value'] = previous_rating['value'].apply(lambda x: float(re.search(r'\((\d+\.\d+)\)', x).group(1)))
        
        current_tp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(targetprice)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId','value_scaled','currency','tradingItemId','accountingStandardDesc']]
        previous_tp = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(targetprice)) & (historicalData_parsed['researchContributorId'] == contributor) & (historicalData_parsed['companyId'] == companyid)& (
                                                      historicalData_parsed['filingDate'] < filingdate) & (historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId','value_scaled','currency','tradingItemId','accountingStandardDesc', 'filingDate']]
                                        
        
        # print(current_tp)
        # print(previous_tp)
        
        maxprevious_rating_1 = previous_rating.groupby(['dataItemId', 'accountingStandardDesc', 'tradingItemId'])['filingDate'].max().reset_index()

        maxprevious_rating = previous_rating[(previous_rating['filingDate'].isin(maxprevious_rating_1['filingDate']))]

        maxprevious_tp_1 = previous_tp.groupby(['dataItemId', 'accountingStandardDesc', 'tradingItemId'])['filingDate'].max().reset_index()

        maxprevious_tp = previous_tp[(previous_tp['filingDate'].isin(maxprevious_tp_1['filingDate']))]
        
        merged_rating=pd.merge(current_rating, maxprevious_rating,on=['dataItemId','tradingItemId','accountingStandardDesc'],how='inner')

        merged_tp=pd.merge(current_tp, maxprevious_tp,on=['dataItemId','tradingItemId','accountingStandardDesc'],how='inner')
        
        # print(merged_rating)
        # print(merged_tp)
        
        base_currency=merged_tp.currency_x.mode()[0]
        merged_tp['value_scaled_y']=merged_tp.apply(lambda x: currency_converter(currency_from=x['currency_x'], currency_to=base_currency, value=x['value_scaled_y']),axis=1)
        
        # print(merged_tp)
        
        merged_df = pd.merge(merged_rating, merged_tp, on=['accountingStandardDesc', 'tradingItemId'], how='inner')
        
        # print(merged_df)

        
        dataItemId_rating = []
        dataItemId_tp = []
        previousdate = []
        tid = []
        accounting = []
        diff = []
        perc = []
        
        for ind, row in merged_df.iterrows():
            if ((execute_operator(row['filingDate_x'], row['filingDate_y'], operator[0])) & (execute_operator(row['numaric_value_x'], row['numaric_value_y'], operator[1]))&(execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[1]))):
                dataItemId_rating.append(row['dataItemId_x'])
                dataItemId_tp.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_x'])
                accounting.append(row['accountingStandardDesc'])
                tid.append(row['tradingItemId'])
                diff='NA'
                perc='NA'
                
            if ((execute_operator(row['filingDate_x'], row['filingDate_y'], operator[0]))&(execute_operator(row['numaric_value_x'], row['numaric_value_y'], operator[2]))&(execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[2]))):
                dataItemId_rating.append(row['dataItemId_x'])
                dataItemId_tp.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_x'])
                accounting.append(row['accountingStandardDesc'])
                tid.append(row['tradingItemId'])
                diff='NA'
                perc='NA'

        diff_df = pd.DataFrame({"dataItemId_x": dataItemId_rating,"dataItemId_y": dataItemId_tp,"filingDate": previousdate, "diff": diff, "perc": perc})
        
        
        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId_x'])) & (
                                          extractedData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                          extractedData_parsed['tradingItemId'].isin(tid)) &  (
                                                  extractedData_parsed['value'] != "") & (
                                          extractedData_parsed['value'].notnull()))][
            ['dataItemId', 'value', 'parentFlag', 'accountingStandardDesc',
             'tradingItemId', 'fiscalChainSeriesId', 'team', 'description', 'tradingItemName']]

                                              
        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId_y'])) & (
                                          extractedData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                          extractedData_parsed['tradingItemId'].isin(tid)) &  (
                                                  extractedData_parsed['value'] != "") & (
                                          extractedData_parsed['value'].notnull()))][
            ['dataItemId', 'value','scale','currency', 'parentFlag', 'accountingStandardDesc',
             'tradingItemId', 'fiscalChainSeriesId', 'team', 'description', 'tradingItemName']]

        temp3 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId_x'])) &  (
                                           historicalData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                           historicalData_parsed['tradingItemId'].isin(tid)) &
                                               (historicalData_parsed['value'] != "") & (
                                           historicalData_parsed['value'].notnull()) & (
                                           historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (
                                                   historicalData_parsed['researchContributorId'] == contributor))][
            ['dataItemId', 'value', 'parentFlag', 'accountingStandardDesc',
             'tradingItemId', 'fiscalChainSeriesId',  'team', 'description', 'tradingItemName', 'versionId', 'companyId', 'feedFileId', 'filingDate']]

        temp4 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId_y'])) &  (
                                           historicalData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                           historicalData_parsed['tradingItemId'].isin(tid)) &
                                               (historicalData_parsed['value'] != "") & (
                                           historicalData_parsed['value'].notnull()) & (
                                           historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (
                                                   historicalData_parsed['researchContributorId'] == contributor))][
            ['dataItemId', 'value', 'scale','currency','parentFlag', 'accountingStandardDesc',
             'tradingItemId', 'fiscalChainSeriesId',  'team', 'description', 'tradingItemName', 'versionId', 'companyId', 'feedFileId', 'filingDate']]
     
        temp1_revised = temp1.dropna()

        for ind, row in temp1_revised.iterrows():
            result = {"highlights": [],
                      "error": "Non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                         "cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],
                                                  "currency": 'NA'}, "section": row['team'],
                                         "filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                           "description": row["description"],
                                           "versionId": filingMetadata['metadata']['versionId'],
                                           "companyid": filingMetadata['metadata']['companyId'],
                                           "feedFileId": filingMetadata['metadata']['feedFileId'],
                                           "peo": 'NA',
                                           "diff": 'NA',
                                           "percent": 'NA'}
            result["checkGeneratedForList"] = [
                {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                 "fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',
                 "value": row["value"], "units": 'NA', "currency": 'NA',
                 "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                 "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                 "diff": 'NA',
                 "percent": 'NA'}]
            errors.append(result)

        temp2_revised = temp2.dropna()

        for ind, row in temp2_revised.iterrows():
            result = {"highlights": [],
                      "error": "non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                         "cell": {"peo": 'NA', "scale": row['scale'], "value": row['value'],
                                                  "currency": row['currency']}, "section": row['team'],
                                         "filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                           "description": row["description"],
                                           "versionId": filingMetadata['metadata']['versionId'],
                                           "companyid": filingMetadata['metadata']['companyId'],
                                           "feedFileId": filingMetadata['metadata']['feedFileId'],
                                           "peo": 'NA',
                                           "diff": 'NA',
                                           "percent": 'NA'}
            result["checkGeneratedForList"] = [
                {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                 "fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',
                 "value": row["value"], "units": row['scale'], "currency": row['currency'],
                 "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                 "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                 "diff": 'NA',
                 "percent": 'NA'}]
            errors.append(result)
            
            temp3_revised = temp3.dropna()

            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],
                          "error": "non periodic data items captured values' combination is not meeting up the standard definition"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                             "cell": {"peo":'NA', "scale": 'NA', "value": row['value'],
                                                      "currency": 'NA'}, "section": row['team'],
                                             "filingId": row['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                               "description": row["description"], "versionId": row['versionId'],
                                               "companyid": row['companyId'], "feedFileId": row['feedFileId'],
                                               "peo": 'NA',
                                               "diff": 'NA',
                                               "percent": 'NA'}
                result["checkGeneratedForList"] = [
                    {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                     "fiscalYear":'NA', "fiscalQuarter": 'NA', "peo": 'NA',
                     "value": row["value"], "units": 'NA', "currency": 'NA',
                     "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                     "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                     "refFilingId": row["versionId"], "refFilingDate": row["filingDate"],
                     "estimatePeriodId": 'NA',
                     "diff": 'NA',
                     "percent": 'NA'}]
            errors.append(result)

            temp4_revised = temp4.dropna()

            for ind, row in temp4_revised.iterrows():
                result = {"highlights": [],
                          "error": "non periodic data items captured values' combination is not meeting up the standard definition"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                             "cell": {"peo":'NA', "scale": row['scale'], "value": row['value'],
                                                      "currency": row['currency']}, "section": row['team'],
                                             "filingId": row['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                               "description": row["description"], "versionId": row['versionId'],
                                               "companyid": row['companyId'], "feedFileId": row['feedFileId'],
                                               "peo": 'NA',
                                               "diff": 'NA',
                                               "percent": 'NA'}
                result["checkGeneratedForList"] = [
                    {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                     "fiscalYear":'NA', "fiscalQuarter": 'NA', "peo": 'NA',
                     "value": row["value"], "units": row['scale'], "currency": row['currency'],
                     "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                     "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                     "refFilingId": row["versionId"], "refFilingDate": row["filingDate"],
                     "estimatePeriodId": 'NA',
                     "diff": 'NA',
                     "percent": 'NA'}]
            errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors


#Estimates Error Checks 
@add_method(Validation)
def EST_102(historicalData,filingMetadata,extractedData,parameters):
    errors = []   
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #[!=]
    
    # print(left_dataItemIds_list)
    
    try:
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']
        
        # print(extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))
        # print(historicalData_parsed['companyId'])
        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','scale','currency','currencyId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        # print(current)

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(current['dataItemId']))&(historicalData_parsed['estimatePeriodId'].isin(current['estimatePeriodId']))&(historicalData_parsed['peo'].isin(current['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['companyId']==companyid))][['dataItemId','peo','estimatePeriodId','value','scale','currency','currencyId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate']].drop_duplicates()
        
        # print(current)
        # print(previous)
        
        # maxprevious1=previous.groupby(['researchContributorId'])['filingDate'].max().reset_index()
        
        maxprevious1=previous.groupby(['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
        
        # print(maxprevious)

        # if ((len(current)>0) & (len(maxprevious)>0)):
        merged_df=pd.merge(current,maxprevious,on=['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner').drop_duplicates()
        
        # print(merged_df)
        # print(merged_df[['dataItemId','peo','value_x','value_y','currency_x','currency_y','currencyId_x','currencyId_y']])

        dataItemIds=[]
        previousdate=[]
        parentflag=[]
        peos=[]
        curx=[]
        cury=[]
        AS=[]
        fyc=[]
        diff=[]
        perc=[]
        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['currencyId_x'],row['currencyId_y'],operator[0]):
                peos.append(row['peo'])
                curx.append(row['currencyId_x'])
                cury.append(row['currencyId_y'])
                dataItemIds.append(row['dataItemId'])
                previousdate.append(row['filingDate'])
                parentflag.append(row['parentFlag'])
                AS.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'  
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"filingDate":previousdate,"diff":diff,"perc":perc})
        print(diff_df)
        

        if len(diff_df)>0:
            # diff_df['curcomb']=diff_df.apply(lambda x:'%s%s%s' % (x['dataItemId'],x['peo'],x['currencyId']),axis=1)
            # historicalData_parsed['curcomb']=historicalData_parsed.apply(lambda x:'%s%s%s' % (x['dataItemId'],x['peo'],x['currencyId']),axis=1)
    
            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(AS))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','fiscalYear','fiscalQuarter','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','fiscalChainSeriesId']].drop_duplicates()
            # print(temp1)

            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(AS))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['researchContributorId']==contributor))][['dataItemId','peo','value','scale','currency','fiscalYear','fiscalQuarter','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate','fiscalChainSeriesId']].drop_duplicates()
            # print(temp2)

            temp1_revised=temp1.dropna()    
            temp2_revised=temp2.dropna() 
            
            # print(temp1_revised)

            for ind, row in temp1_revised.iterrows():

                result = {"highlights": [], "error": "Currency Difference between current and Previous document"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "units": row['scale'], "value": row['value'], "currency": row['currency'],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)

            for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Currency Difference between current and Previous document"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "units": row['scale'], "value": row['value'], "currency": row['currency'],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)
            
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors
    
#Estimates Error Checks 
@add_method(Validation)        
def EST_56G(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBITDA
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT
    tag_list=get_dataItemIds_list('TAG1', parameters) #DA

    try:

        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT
        DA_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag_list))&(historicalData_parsed['periodTypeId']!=1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['fiscalYear'].isin(lhs_df['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #DA


        if ((len(lhs_df)>0) & (len(rhs_df)>0)& (len(DA_df)>0)):

            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        
            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','value_scaled'],how='inner')

            merged_DA_df = pd.merge(DA_df,merged_df,on=['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear'],how='inner')
            

            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            dataitem=[]

            for ind, row in merged_DA_df.iterrows():
                if (row['value_scaled_x']!=0):
                    peos.append(row['peo_y'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    dataitem.append(row['dataItemId_x'])

            diff_df=pd.DataFrame({"peo":peos,"dataitem":dataitem}).drop_duplicates()
            

            if len(diff_df)>0:
                diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataitem'],x['peo']),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','fiscalYear','fiscalQuarter','fiscalChainSeriesId']]
                temp1_revised=temp1.dropna()
                
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "EBIT estimate = EBITDA estimate in FY but D&A Actual available in related actualized Qs"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_35(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # tags
    operator = get_dataItemIds_list('Operation', parameters)
    try:
        documentdate=filingMetadata['metadata']['filingDate']
        
        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(dataItemId_list)))&
                                     (extractedData_parsed['value'] != "") &
                                     (extractedData_parsed['value'].notnull())&
                                     (extractedData_parsed['peo']!= "")&
                                     (extractedData_parsed['peo'].notnull()))][['dataItemId', 'peo', 'fiscalChainSeriesId']]
        temp['companyId'] = filingMetadata['metadata']['companyId']

        # print(temp)
        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list)) & (historicalData_parsed['value'] != "")&
                                          (historicalData_parsed['value'].notnull())&
                                          (historicalData_parsed['peo']!="")&
                                          (historicalData_parsed['filingDate'] < documentdate)&
                                          (historicalData_parsed['peo'].notnull()))][['dataItemId', 'peo','fiscalChainSeriesId', 'filingDate','companyId']]
        
        
        
        maxprevious = previous.groupby(['companyId'])['filingDate'].max().reset_index()

        previous = previous[previous['filingDate'].isin(maxprevious['filingDate'])]

        # print(previous)

        merged_df = pd.merge(temp, previous, on=['companyId'], how='inner')

        # print(merged_df)

        filingdate = []
        diff = []
        perc = []
        series1 = []
        series2 = []
        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'], row['fiscalChainSeriesId_y'], operator[0]):
                filingdate.append(row['filingDate'])
                difference = 'NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc = 'NA'

        diff_df = pd.DataFrame(
            {"diff": diff, "perc": perc, "filingDate": filingdate, "curseries": series1, "preseries": series2}).drop_duplicates()
        #print(diff_df)
        temp1 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(series1))][
            ['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
        #print(temp1)
        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (
            historicalData_parsed['fiscalChainSeriesId'].isin(series2)) )][
            ['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team', 'versionId', 'feedFileId',
             'filingDate', 'companyId']].drop_duplicates()


        if len(temp1) > 0 and len(temp2) > 0:
            temp1_revised = temp1.dropna()
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],
                          "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},
                                             "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},
                                             "section": row['team'],
                                             "filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA",
                                               "versionId": filingMetadata['metadata']['versionId'],
                                               "companyid": filingMetadata['metadata']['companyId'],
                                               "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [
                    {"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA",
                     "peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA",
                     "accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],
                     "fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

            temp2_revised = temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},
                                             "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},
                                             "section": row['team'], "filingId": row['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA",
                                               "versionId": row['versionId'], "companyid": row['companyId'],
                                               "feedFileId": row['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [
                    {"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA",
                     "peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA",
                     "accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],
                     "fiscalChainSeries": row["fiscalChainSeriesId"], "refFilingId": row["versionId"],
                     "refFilingDate": row["filingDate"], "estimatePeriodId": "NA", "diff": "NA", "percent": "NA"}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_22D(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list = get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)  # ["!="]
    Threshold = get_parameter_value(parameters, 'Min_Threshold') #['0']
    try:
        quarters_semis = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list)) & (extractedData_parsed['value'] == '0.0')&(extractedData_parsed['value'] != '') &(extractedData_parsed['value'].notnull())&((extractedData_parsed["periodTypeId"] == 2) | (extractedData_parsed["periodTypeId"] == 10)))][['dataItemId', 'peo', 'value_scaled', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'periodTypeId', 'fiscalYear']].drop_duplicates()

        fiscal_year = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['value'] != '0.0')&(extractedData_parsed['value'] != '')&(extractedData_parsed['value'].notnull())& (extractedData_parsed["periodTypeId"] == 1))][['dataItemId', 'peo', 'value_scaled', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'periodTypeId', 'fiscalYear']]

        print(quarters_semis)
        print(fiscal_year)

        if ((len(quarters_semis) > 0) & (len(fiscal_year) > 0 )):

            quarters_semis = quarters_semis.groupby(['dataItemId', 'fiscalYear', 'periodTypeId', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId']).agg(PEO_Count=('peo', 'count'), PEO_Sum=('value_scaled','sum')).reset_index()
        
           
            merged_df = pd.merge(quarters_semis, fiscal_year,on=['dataItemId', 'fiscalYear', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', ], how='inner')
            
            print(merged_df)
        
            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            diff=[]
            perc=[]
            dataItemIds=[]
            
            if merged_df is not None:
                for ind, row in merged_df.iterrows():
                    if row['periodTypeId_x'] == 2:
                        if row['PEO_Count'] == 4:
                            if row['value_scaled']!=0:
                                if execute_operator(float(row['PEO_Sum']), float(Threshold[0]), operator[0]):
                                        peos.append(row['fiscalYear'])
                                        dataItemIds.append(row['dataItemId'])
                                        tid.append(row['tradingItemId'])
                                        parentflag.append(row['parentFlag'])
                                        accounting.append(row['accountingStandardDesc'])
                                        fyc.append(row['fiscalChainSeriesId'])
                                        diff='NA'
                                        perc='NA'
                for ind, row in merged_df.iterrows():
                    if row['periodTypeId_x'] == 10:
                        if row['PEO_Count'] == 2:
                            if row['value_scaled']!=0:
                                if execute_operator(float(row['PEO_Sum']), float(Threshold[0]), operator[0]):
                                        peos.append(row['fiscalYear'])
                                        dataItemIds.append(row['dataItemId'])
                                        tid.append(row['tradingItemId'])
                                        parentflag.append(row['parentFlag'])
                                        accounting.append(row['accountingStandardDesc'])
                                        fyc.append(row['fiscalChainSeriesId'])
                                        diff='NA'
                                        perc='NA'
                diff_df = pd.DataFrame({"fiscalYear": peos, "diff": diff, "perc": perc, 'dataItemId': dataItemIds})
        

                if len(diff_df)>0:
                    diff_df['peocomb']=diff_df.apply(lambda x: '%s%s' % (x['dataItemId'],x['fiscalYear']),axis=1)
                    extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x: '%s%s' % (x['dataItemId'],x['fiscalYear']),axis=1 )
        
                    temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['tradingItemId'].isin(tid)) &(extractedData_parsed['periodTypeId'] == 1)&(extractedData_parsed['parentFlag'].isin(parentflag))&
                                                  (extractedData_parsed['accountingStandardDesc'].isin(accounting)) &(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) &(extractedData_parsed['value'] != "") &
                                                  (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear', 'fiscalQuarter','tradingItemName','peocomb']]
        
                    temp1_revised = temp1.dropna()
                    
                    for ind, row in temp1_revised.iterrows():
                    #if row['value'] != 0:
                        result = {"highlights": [], "error": "The all Quarters and Semi annuals are captured zero but Fiscal Years are having value(PDF Only)."}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'],"currency": row['currency']}, "section": row['team'], "filingId": filingMetadata['metadata']['versionId']})
                        result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb'] == row["peocomb"]]['diff'].iloc[0], "percent":diff_df[diff_df['peocomb'] == row["peocomb"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter": row["fiscalQuarter"], "peo": row["peo"],"value": row["value"], "units": row["scale"], "currency": row["currency"],"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb'] == row["peocomb"]]['diff'].iloc[0],"percent": diff_df[diff_df['peocomb'] == row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors
    
#Estimates Error Checks 
@add_method(Validation) 
def EST_16A(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters) #['<=']
    Threshold=get_parameter_value(parameters,'Max_Threshold') #90
    
    try:
        
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        
        # print(filingdate)
        # print(contributor)
        # print(historicalData_parsed['researchContributorId'])
        
          
        
        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate']].drop_duplicates()
        
        # print(current)
        # print(previous)
        
        previous['daysdiff']=abs((pd.to_datetime(filingdate)-pd.to_datetime(previous['filingDate'])).dt.days)

        # print(current)
        # print(previous)        
        
        maxprevious1=previous.groupby(['dataItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))].drop_duplicates()
        
        #maxprevious['days']=abs((pd.to_datetime(filingdate)-pd.to_datetime(maxprevious['filingDate'])).dt.days)
        
        maxprevious = maxprevious.copy()
        current['peocomb']=current.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        maxprevious['peocomb']=maxprevious.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        
        # print(current)
        # print(maxprevious)
        
        temp=maxprevious[~((maxprevious['peocomb'].isin(current['peocomb']))&(maxprevious['parentFlag'].isin(current['parentFlag']))
                         &(maxprevious['accountingStandardDesc'].isin(current['accountingStandardDesc']))&(maxprevious['fiscalChainSeriesId'].isin(current['fiscalChainSeriesId'])))]
        
        # print(temp)
        
        dataItemIds=[]
        previousdate=[]
        parentflag=[]
        peo=[]
        AS=[]
        fyc=[]
        diff=[]
        perc=[]        
        for ind, row in temp.iterrows():
            if execute_operator(row['daysdiff'],float(Threshold[0]),operator[0]):
                peo.append(row['peo'])
                dataItemIds.append(row['dataItemId'])
                previousdate.append(row['filingDate'])
                parentflag.append(row['parentFlag'])
                AS.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'  
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peo,"filingDate":previousdate,"parentFlag":parentflag,"diff":diff,"perc":perc})
        
        # print(diff_df)
        if len(diff_df)>0:
            diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # print(diff_df)
            historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # previous['peocomb']=previous.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # print(historicalData_parsed)
       
            temp1 = historicalData_parsed[((historicalData_parsed['peocomb'].isin(diff_df['peocomb'])) &(historicalData_parsed['parentFlag'].isin(diff_df['parentFlag']))&(historicalData_parsed['accountingStandardDesc'].isin(AS))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
            # temp1 = previous[((previous['peocomb'].isin(diff_df['peocomb'])) &(previous['parentFlag'].isin(parentflag))&(previous['accountingStandardDesc'].isin(AS))&(previous['fiscalChainSeriesId'].isin(fyc))&(previous['value']!="")&(previous['value'].notnull())&(previous['filingDate'].isin(diff_df['filingDate'])))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','fiscalChainSeriesId','fiscalYear','filingDate']].drop_duplicates()
            # print(temp1)
            temp1_revised=temp1.dropna() 

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "PEO not in current document"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors


#Estimates Error Checks 
@add_method(Validation) 
def EST_60(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    #dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    try:

        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="E")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(~(extractedData_parsed['parentFlag'].isin(historicalData_parsed['parentFlag']))
                                        |~(extractedData_parsed['tradingItemId'].isin(historicalData_parsed['tradingItemId']))
                                        |~(extractedData_parsed['accountingStandardDesc'].isin(historicalData_parsed['accountingStandardDesc']))
                                        |~(extractedData_parsed['fiscalChainSeriesId'].isin(historicalData_parsed['fiscalChainSeriesId']))
                                        ))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']]
        

        if len(current)>0:
            dataItemIds=[]
            parentflag=[]
            accounting=[]
            peos=[]
            tid=[]
            fyc=[]        
            diff=[]
            perc=[]
            for ind,row in current.iterrows():
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                peos.append(row['peo'])
                diff='NA'
                perc='NA'
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc,'peo':peos})
            
            print(diff_df)

            temp= extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemIds)) &(extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()

            temp_revised=temp.dropna()

            for ind, row in temp_revised.iterrows():
                result = {"highlights": [], "error": "New flavor captured for the company"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)                    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_193(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    
    try:
        
        # if filingMetadata['metadata']['latestPeriodType']=="FY":
        latestfy=int(filingMetadata['metadata']['latestPeriodYear'])
        # else:
        #     latestfy=int(filingMetadata['metadata']['latestPeriodYear'])-1        
                
        # if filingMetadata['metadata']['latestPeriodType']=="FY":
        #     latestactualPEO=pd.to_datetime(filingMetadata['metadata']['latestActualizedPeo']).date()
        #     MinEstimatePEO = pd.to_datetime(latestactualPEO+pd.DateOffset(years=1)).date()
        #     # print(MinEstimatePEO)

        # print(latestfy)    
        # temp_df = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(dataItemId_list)) &(extractedData_parsed["periodTypeId"] == 1) &(pd.to_datetime(extractedData_parsed["periodEndDate"]).dt.date == MinEstimatePEO) &((extractedData_parsed['value'] == "") | (extractedData_parsed['value'].isnull()))][['dataItemId', 'peo', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter', 'periodEndDate']].drop_duplicates()

        temp_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['fiscalYear']==latestfy) &(extractedData_parsed["periodTypeId"] == 1))][['dataItemId', 'peo', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter', 'periodEndDate']].drop_duplicates()

        # print(temp_df1)
        temp_df2=temp_df1.groupby(['dataItemId','parentFlag','accountingStandardDesc'])['periodEndDate'].min().reset_index()
        # print(temp_df)
        # print(temp_df2)
        
        filter_df= temp_df1[(temp_df1['dataItemId'].isin(temp_df2['dataItemId']))&(temp_df1['parentFlag'].isin(temp_df2['parentFlag']))&(temp_df1['accountingStandardDesc'].isin(temp_df2['accountingStandardDesc']))&(temp_df1['periodEndDate'].isin(temp_df2['periodEndDate']))&((temp_df1['value']=="") | (temp_df1['value'].isnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','periodEndDate']]
        
        # print(filter_df)
        
        min_peos=[]
        diff=[]
        perc=[]

        
        if filter_df is not None:
            for ind,row in filter_df.iterrows():           
                min_peos.append(row['peo'])  
                difference='NA'
                diff.append(difference)
                perc='NA'
            
        diff_df=pd.DataFrame({"peo":min_peos,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            temp1=extractedData_parsed[(extractedData_parsed['peo'].isin(min_peos))]

            temp1_revised=temp1.dropna()
        
            for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Min Period is Greater than FY1"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
                    result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)                       
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_333(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list = get_dataItemIds_list('LHSdataItemIds', parameters)
    country1 = get_dataItemIds_list('COUNTRY_INCLUDE', parameters)  # Japan
    try:
        
        if (filingMetadata['metadata']['country'] not in country1):
            temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['parentFlag']=='Parent')&(extractedData_parsed['value'] != "")&(extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId']]
            # print(temp)

            if len(temp) > 0:
                # dataItemIds = []
                # peos = []
                # tid = []
                parentflags = []
                # accounting = []
                # fyc = []
                diff = []
                perc = []

                for ind, row in temp.iterrows():
                    # dataItemIds.append(row['dataItemId'])
                    # peos.append(row['peo'])
                    # tid.append(row['tradingItemId'])
                    parentflags.append(row['parentFlag'])
                    # accounting.append(row['accountingStandardDesc'])
                    # fyc.append(row['fiscalChainSeriesId'])
                    diff = 'NA'
                    perc = 'NA'

                diff_df = pd.DataFrame({"parentflag": parentflags})

                temp1 = extractedData_parsed[((extractedData_parsed['parentFlag'].isin(parentflags)))][['parentFlag','team', 'description','accountingStandardDesc', 'scale', 'currency']]  #[['dataItemId', 'value', 'peo', 'scale', 'currency', 'parentFlag', 'accountingStandardDesc','tradingItemId', 'team', 'description', 'tradingItemName', 'fiscalYear', 'fiscalQuarter','fiscalChainSeriesId']].drop_duplicates()

                temp1_revised = temp1.dropna()
                
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Parent data collected for other Asian comp"}
                    result["highlights"].append({"parentConsolidatedFlag": row['parentFlag'], "row": {"accountingStdDesc": row['accountingStandardDesc']}, "section": row['team'], "scale": row['scale'],  "currency": row['currency'],
                                                 "filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"] = {"statement": "", "tag": row['parentFlag'],"description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],
                                                   "feedFileId": filingMetadata['metadata']['feedFileId'],
                                                   "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"] = [
                        {"tag": row['parentFlag'], "description": row["description"], "accountingStdDesc": row["accountingStandardDesc"],
                         "parentConsolidatedFlag": row["parentFlag"],
                         "diff": 'NA',
                         "percent": 'NA'}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_332(historicalData,filingMetadata,extractedData,parameters):
        errors = []
        left_list=get_dataItemIds_list('LHSdataItemIds', parameters)
        operator=get_dataItemIds_list('Operation', parameters)
        countries=get_dataItemIds_list('COUNTRY_INCLUDE',parameters)
        try:
            
            if filingMetadata['metadata']['country'] in countries:
                consolidated = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_list))&(extractedData_parsed['parentFlag']=='Consolidated')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currencyId','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].drop_duplicates()
                parent = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_list))&(extractedData_parsed['parentFlag']=='Parent')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currencyId','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].drop_duplicates()                          
                
                # print(consolidated)
                # print(parent)
                
                merged_df=pd.merge(consolidated,parent,on=['dataItemId','peo','accountingStandardDesc','fiscalChainSeriesId','tradingItemId'],how='inner')
                
                base_currency=merged_df.currency_x.mode()[0]
                merged_df['value_scaled_y']=merged_df.apply(lambda x: currency_converter(currency_from=x['currency_y'], currency_to=base_currency, value=x['value_scaled_y']),axis=1)
    
                # print(merged_df)
    
                dataItemIds=[]
                peos=[]
                accounting=[]
                tid=[]
                fyc=[]        
                diff=[]
                perc=[]
                for ind,row in merged_df.iterrows():                    
                    if execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[0]):    
                        dataItemIds.append(row['dataItemId'])
                        peos.append(row['peo'])
                        tid.append(row['tradingItemId'])
                        accounting.append(row['accountingStandardDesc'])
                        fyc.append(row['fiscalChainSeriesId'])
                        diff='NA'
                        perc='NA'
                diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"diff":diff,"perc":perc})
                
                # print(diff_df)
                
                temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()

                temp1_revised=temp1.dropna()
                
                                                                                                                                                                                                                                              
                for ind, row in temp1_revised.iterrows():                        
                    result = {"highlights": [], "error": "consolidate data lessthan Parent data for Korean and Japan Companies"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)
         
            print(errors) 
            return errors                                                                   
        except Exception as e:
            print(e)
            return errors  

#Estimates Error Checks 
@add_method(Validation) 
def EST_63(historicalData,filingMetadata,extractedData,parameters):
    errors = []    
    try:
        companyid=filingMetadata['metadata']['companyId']
        contributor=filingMetadata['metadata']['researchContributorId']
        
        temp0 = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="E")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','currencyId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
 
        print(temp0)
        
        if len(temp0)>0:
            
            temp1=historicalData_parsed[((historicalData_parsed['companyId']==companyid)&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(historicalData_parsed['dataItemFlag']=="E"))][['dataItemId','peo','value','currencyId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()

            print(temp1)

            final=temp0[~(temp0['dataItemId'].isin(temp1['dataItemId'])|temp0['currencyId'].isin(temp1['currencyId'])|temp0['fiscalChainSeriesId'].isin(temp1['fiscalChainSeriesId'])|temp0['accountingStandardDesc'].isin(temp1['accountingStandardDesc'])|temp0['parentFlag'].isin(temp1['parentFlag']))]
            
            print(final)
              
            dataItemIds=[]                                                                                                                                                                                         
            peos=[]
            diff=[]
            perc=[]
            currencyid=[]
            parentflags=[]
            accountingStandard=[]
    
            #if final is not None:
            for ind,row in final.iterrows():
                dataItemIds.append(row['dataItemId'])
                currencyid.append(row['currencyId'])
                parentflags.append(row['parentFlag'])
                accountingStandard.append(row['accountingStandardDesc'])
                peos.append(row['peo'])               
                difference='NA'
                diff.append(difference)
                perc='NA'
            
            diff_df=pd.DataFrame({"peo":peos,"currencyId":currencyid,"diff":diff,"perc":perc,"parentFlag": parentflags})      
            
            print(diff_df)
            
            if len(diff_df)>0:
                final1=extractedData_parsed[((extractedData_parsed['parentFlag'].isin(diff_df[parentflags])&extractedData_parsed['currencyId'].isin(diff_df[currencyid])&extractedData_parsed['accountingStandardDesc'].isin(accountingStandard)))][['parentFlag','team', 'description','accountingStandardDesc', 'scale', 'currencyId', 'currency']].drop_duplicates()    #[['dataItemId', 'peo', 'value', 'scale', 'currency','currencyId', 'parentFlag', 'accountingStandardDesc',
                                      #'tradingItemId', 'team', 'description', 'tradingItemName', 'fiscalYear', 'fiscalQuarter','fiscalChainSeriesId']]
                
                final1_revised = final1.dropna()
                
                print(final1)
                
                for ind, row in final1_revised.iterrows():
                    # result = {"highlights": [], "error": "Initiation at Company Level for different flavors"}
                    # result["highlights"].append({"parentConsolidatedFlag": row['parentFlag'], "row": {"accountingStdDesc": row['accountingStandardDesc'],"currencyId": row['currencyId']}, "section": row['team'], "scale": row['scale'],  "currency": row['currency'],"currencyId": row['currencyId'],
                    #                              "filingId": filingMetadata['metadata']['versionId']})
                    # result["checkGeneratedFor"] = {"statement": "", "tag": row['parentFlag'],"description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],
                    #                                "feedFileId": filingMetadata['metadata']['feedFileId'],
                    #                                "diff": 'NA', "percent": 'NA'}
                    # result["checkGeneratedForList"] = [
                    #     {"tag": row['parentFlag'], "description": row["description"], "accountingStdDesc": row["accountingStandardDesc"],
                    #      "parentConsolidatedFlag": row["parentFlag"],
                    #      "diff": 'NA',
                    #      "percent": 'NA'}]
                    errors.append(result)                    
                    result = {"highlights": [], "error": "Initiation at Company Level for different flavors"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
                    result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_11A(historicalData,filingMetadata,extractedData,parameters):
        errors = []
        left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
               
        try:
            quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']]
            fiscalyear = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear']]                          
            
            quarter['peocomb']=quarter[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            fiscalyear['peocomb']=fiscalyear[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            quarter_revised=quarter[~(quarter['peocomb'].isin(fiscalyear['peocomb']))]
            
            if len(quarter_revised)>0:
                
                quarter_revised=quarter_revised.drop_duplicates()
                quarter_revised_new=quarter_revised.dropna()

                for ind, row in quarter_revised_new.iterrows():
    
                    result = {"highlights": [], "error": "Value exist for quarters but not for fiscal year"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)
                                  
            print(errors) 
            return errors                                                                   
        except Exception as e:
            print(e)
            return errors    


#Estimates Error Checks 
@add_method(Validation) 
def EST_65(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list = get_dataItemIds_list('LHSdataItemIds', parameters)  # dataitemslist
    right_dataItemId_list = get_dataItemIds_list('RHSdataItemIds', parameters)


    try:
        if filingMetadata['metadata']['latestPeriodType']=='Q4':
            latestfy=filingMetadata['metadata']['latestPeriodYear']
            
        else:
            latestfy=int(filingMetadata['metadata']['latestPeriodYear'])-1

        # print(latestfy)
        
        epsactual = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['fiscalYear']==latestfy)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'value_scaled', 'tradingItemId', 'periodEndDate']].drop_duplicates()
        
        # print(epsactual)
        if len(epsactual)>0:
            if (epsactual['value_scaled']<0).all():
                temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','value','team','tradingItemId','accountingStandardDesc','description','tradingItemName']].drop_duplicates()
                # print(temp)
                if len(temp) > 0:

                    temp_revised = temp.dropna()

                    for ind, row in temp_revised.iterrows():
                        result = {"highlights": [], "error": "Base Year Negative_LTGR"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']},"cell": {"peo": 'NA', "scale": 'NA',"value": row['value'], "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId']})
                        result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],"description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'],"peo": 'NA', "diff": "NA", "percent": "NA"}
                        result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"],"tradingItemId": row["tradingItemId"],"fiscalYear": 'NA',"fiscalQuarter": 'NA', "peo": 'NA', "value": row["value"], "units": 'NA',"currency": 'NA', "tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": 'NA', "fiscalChainSeries": 'NA', "diff": "NA", "percent": "NA"}]
                        errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)     
def EST_57(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    recommendation = get_dataItemIds_list('LHSdataItemIds', parameters)
    targetprice = get_dataItemIds_list('RHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #[!=,>,<]

    try:

        filingdate = filingMetadata['metadata']['filingDate']
        contributor = filingMetadata['metadata']['researchContributorId']
        companyid = filingMetadata['metadata']['companyId']
               
        current_rating = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(recommendation)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId','value','tradingItemId','accountingStandardDesc']].drop_duplicates()
        
        # print(current_rating)
        
        # current_rating['numaric_value']=current_rating['value'].str[-2].astype(int)
        # current_rating['numeric_value'] = current_rating['value'].str[-2:].astype(int)
        # current_rating['numeric_value'] = current_rating['value'].str.rstrip('.0').astype(int)
        
        
        current_rating['numeric_value'] = current_rating['value'].apply(lambda x: float(re.search(r'\((\d+\.\d+)\)', x).group(1)))
        # print(current_rating)

        previous_rating = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(recommendation)) & (historicalData_parsed['researchContributorId'] == contributor)& (historicalData_parsed['companyId'] == companyid) & (historicalData_parsed['filingDate'] < filingdate) & (historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId','value','tradingItemId','accountingStandardDesc', 'filingDate']].drop_duplicates()
        
        # previous_rating['numaric_value']=int(previous_rating['value'].str[-2])
        #previous_rating['numeric_value'] = previous_rating['value'].str[-2:].astype(int)
        
        # previous_rating['numeric_value'] = previous_rating['value'].str.rstrip('.0').astype(int)
        
        
        
        previous_rating['numeric_value'] = previous_rating['value'].apply(lambda x: float(re.search(r'\((\d+\.\d+)\)', x).group(1)))
        # print(previous_rating)
        
        current_tp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(targetprice)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId','value_scaled','currency','tradingItemId','accountingStandardDesc']].drop_duplicates()
        
        previous_tp = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(targetprice)) & (historicalData_parsed['researchContributorId'] == contributor) & (historicalData_parsed['companyId'] == companyid)& (
                                                      historicalData_parsed['filingDate'] < filingdate) & (historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId','value_scaled','currency','tradingItemId','accountingStandardDesc', 'filingDate']].drop_duplicates()
                                        
        
        # print(current_tp)
        # print(previous_tp)
        
        maxprevious_rating_1 = previous_rating.groupby(['dataItemId','tradingItemId'])['filingDate'].max().reset_index() # 'accountingStandardDesc', 

        maxprevious_rating = previous_rating[(previous_rating['filingDate'].isin(maxprevious_rating_1['filingDate']))]
        
        # print(maxprevious_rating)

        maxprevious_tp_1 = previous_tp.groupby(['dataItemId', 'tradingItemId'])['filingDate'].max().reset_index() # 'accountingStandardDesc',

        maxprevious_tp = previous_tp[(previous_tp['filingDate'].isin(maxprevious_tp_1['filingDate']))]
        
        # print(maxprevious_tp)
        
        merged_rating=pd.merge(current_rating, maxprevious_rating,on=['dataItemId','tradingItemId'],how='inner') #,'accountingStandardDesc'

        merged_tp=pd.merge(current_tp, maxprevious_tp,on=['dataItemId','tradingItemId'],how='inner') #,'accountingStandardDesc'
        
        # print(merged_rating)
        # print(merged_tp)
        
        base_currency=merged_tp.currency_x.mode()[0]
        merged_tp['value_scaled_y']=merged_tp.apply(lambda x: currency_converter(currency_from=x['currency_x'], currency_to=base_currency, value=x['value_scaled_y']),axis=1)
        
        # print(merged_tp)
        
        merged_df = pd.merge(merged_rating, merged_tp, on=['tradingItemId'], how='inner') #'accountingStandardDesc', 
        
        # print(merged_df)
        # print(merged_df[['filingDate_x','filingDate_y','numeric_value_x','numeric_value_y','value_scaled_x','value_scaled_y']])
        

        
        dataItemId_rating = []
        dataItemId_tp = []
        previousdate = []
        tid = []
        diff = []
        perc = []
        
        for ind, row in merged_df.iterrows():
            if ((execute_operator(row['filingDate_x'], row['filingDate_y'], operator[0])) & (execute_operator(row['numeric_value_x'], row['numeric_value_y'], operator[1]))&(execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[1]))):
                dataItemId_rating.append(row['dataItemId_x'])
                dataItemId_tp.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_x'])
                tid.append(row['tradingItemId'])
                diff='NA'
                perc='NA'
                
            if ((execute_operator(row['filingDate_x'], row['filingDate_y'], operator[0]))&(execute_operator(row['numeric_value_x'], row['numeric_value_y'], operator[2]))&(execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[2]))):
                dataItemId_rating.append(row['dataItemId_x'])
                dataItemId_tp.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_x'])
                tid.append(row['tradingItemId'])
                diff='NA'
                perc='NA'

        diff_df = pd.DataFrame({"dataItemId_x": dataItemId_rating,"dataItemId_y": dataItemId_tp,"filingDate": previousdate, "diff": diff, "perc": perc})
        
        
        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId_x'])) & (extractedData_parsed['tradingItemId'].isin(tid)) &  (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'value', 'parentFlag', 'accountingStandardDesc','tradingItemId', 'fiscalChainSeriesId', 'team', 'description', 'tradingItemName']].drop_duplicates()

                                              
        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId_y'])) &  (extractedData_parsed['tradingItemId'].isin(tid)) &  (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'value','scale','currency', 'parentFlag', 'accountingStandardDesc','tradingItemId', 'fiscalChainSeriesId', 'team', 'description', 'tradingItemName']].drop_duplicates()

        temp3 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId_x'])) & (historicalData_parsed['tradingItemId'].isin(tid)) &(historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()) & (historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (historicalData_parsed['researchContributorId'] == contributor))][['dataItemId', 'value', 'parentFlag', 'accountingStandardDesc','tradingItemId', 'fiscalChainSeriesId',  'team', 'description', 'tradingItemName', 'versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

        temp4 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId_y'])) &  (historicalData_parsed['tradingItemId'].isin(tid)) &(historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()) & (historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (historicalData_parsed['researchContributorId'] == contributor))][['dataItemId', 'value', 'scale','currency','parentFlag', 'accountingStandardDesc','tradingItemId', 'fiscalChainSeriesId',  'team', 'description', 'tradingItemName', 'versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()
     
        temp1_revised = temp1.dropna()

        for ind, row in temp1_revised.iterrows():
            result = {"highlights": [],"error": "Non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'],"peo": 'NA',"diff": 'NA',"percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA',"percent": 'NA'}]
            errors.append(result)

        temp2_revised = temp2.dropna()

        for ind, row in temp2_revised.iterrows():
            result = {"highlights": [],"error": "Non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'],"peo": 'NA',"diff": 'NA',"percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA',"percent": 'NA'}]
            errors.append(result)
            
        temp3_revised = temp3.dropna()

        for ind, row in temp3_revised.iterrows():
            result = {"highlights": [],"error": "Non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'],"peo": 'NA',"diff": 'NA',"percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA',"percent": 'NA'}]
            errors.append(result)

        temp4_revised = temp4.dropna()

        for ind, row in temp4_revised.iterrows():
            result = {"highlights": [],"error": "Non periodic data items captured values' combination is not meeting up the standard definition"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'],"peo": 'NA',"diff": 'NA',"percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA',"percent": 'NA'}]
            errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors    

#Estimates Error Checks 
@add_method(Validation)  
def EST_35(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # tags
    operator = get_dataItemIds_list('Operation', parameters)
    try:
        documentdate=filingMetadata['metadata']['filingDate']

        
        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(dataItemId_list)))&(extractedData_parsed['value'] != "") &(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!= "")&(extractedData_parsed['peo'].notnull()))][['dataItemId', 'peo', 'fiscalChainSeriesId']].drop_duplicates()
        # print(temp)
        
        temp['companyId'] = filingMetadata['metadata']['companyId']

        # print(temp)
        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['value'] != "")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId'].isin(temp['companyId']))&(historicalData_parsed['peo']!="")&(historicalData_parsed['filingDate'] < documentdate)&(historicalData_parsed['peo'].notnull()))][['dataItemId', 'peo','fiscalChainSeriesId', 'filingDate','companyId']].drop_duplicates()
        
        # print(previous)
        
        maxprevious = previous.groupby(['companyId'])['filingDate'].max().reset_index()
        # print(maxprevious)

        previous = previous[previous['filingDate'].isin(maxprevious['filingDate'])]

        # print(previous)

        merged_df = pd.merge(temp, previous, on=['companyId'], how='inner')

        # print(merged_df)

        filingdate = []
        diff = []
        perc = []
        series1 = []
        series2 = []
        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'], row['fiscalChainSeriesId_y'], operator[0]):
                filingdate.append(row['filingDate'])
                difference = 'NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc = 'NA'

        diff_df = pd.DataFrame({"diff": diff, "perc": perc, "filingDate": filingdate, "curseries": series1, "preseries": series2}).drop_duplicates()
        #print(diff_df)
        temp1 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(series1))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
        #print(temp1)
        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (historicalData_parsed['fiscalChainSeriesId'].isin(series2)) )][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team', 'versionId', 'feedFileId','filingDate', 'companyId']].drop_duplicates()


        if len(temp1) > 0 and len(temp2) > 0:
            temp1_revised = temp1.dropna()
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],"error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

            temp2_revised = temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)


        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors
    
#Estimates Error Checks 
@add_method(Validation) 
def EST_56D(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #CFO
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #FCF
    tag_list=get_dataItemIds_list('TAG1', parameters) #CAPEX
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #CFO
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #FCF
        CAPEX_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #CAPEX

        # print(lhs_df)
        # print(rhs_df)
        # print(CAPEX_df)

        if (len(lhs_df)>0 & len(rhs_df)>0 & len(CAPEX_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            CAPEX_df["value_scaled"] = CAPEX_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
        
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_CAPEX_df = pd.merge(CAPEX_df,merged_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        # print(merged_CAPEX_df)
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        #if merged_CAPEX_df is not None:
        for ind,row in merged_CAPEX_df.iterrows():
            if (row['value_scaled']!=0):
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    peos.append(row['peo'])
                    difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(float(round(difference)))
                    perc.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
    
        if len(diff_df)>0:
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear', 'fiscalQuarter','tradingItemName']].drop_duplicates()

            temp1_revised=temp1.dropna()
                    
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "CFO =FCF and Capex available"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_47A(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #['>]
    variation=get_parameter_value(parameters,'Max_Threshold') #100%
    
    try:
        yesscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['scaleId']!=-1)&(extractedData_parsed['currencyId']!=-1)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
        volume = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['scaleId']!=-1)&((extractedData_parsed['currencyId']==-1)|(extractedData_parsed['currencyId']==0))&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]

        noscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['scaleId']==-1)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
        
        # print(yesscale)
        print(volume)
        print(noscale)
        #&(extractedData_parsed['dataItemFlag']=="E")
        
        if len(yesscale)>0:
            yesscale['consValue_scaled'] = yesscale.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
            
            base_currency=yesscale.consCurrency.mode()[0]

            yesscale["value_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            yesscale["consValue_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
        
        if len(volume)>0:
            volume['consValue_scaled'] = volume.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
    
        if len(noscale)>0: 
            noscale['value_scaled'] = pd.to_numeric(noscale['value_scaled'],errors = 'coerce')
            noscale['consValue_scaled'] = pd.to_numeric(noscale['consValue'],errors = 'coerce')
            
            
        if (len(yesscale)>0 or len(noscale)>0 or len(volume)>0):

            frames = [yesscale, volume,noscale]
            temp = pd.concat(frames)[['dataItemId','peo','value_scaled','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId','consValue_scaled']]
            
            temp['consensusvariation']=((abs(((temp['value_scaled']).astype(float))-((temp['consValue_scaled']).astype(float))))/(abs(temp[['value_scaled','consValue_scaled']])).min(axis=1))*100
            
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)

            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            
            for ind,row in temp.iterrows():
                if execute_operator(float(row['consensusvariation']),float(variation[0]),operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])  
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId']) 
                    diff.append(float(round(row['consensusvariation'])))
                    perc.append(float(round(row['consensusvariation'])))
                    
    
            diff_df=pd.DataFrame({"peo":peos,"dataItemId":dataItemIds,"diff":diff,"perc":perc})
                        

            diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                
            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) & (extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates() 
            

            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
    
                result = {"highlights": [], "error": "%Variance with consensus"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                # result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                # result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)                          
                # errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors   
            
#Estimates Error Checks 
@add_method(Validation) 
def EST_22C(historicalData,filingMetadata,extractedData,parameters):
    #Json10
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters) #['!=]

    try:
        FQ = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())  & ((extractedData_parsed["periodTypeId"] == 2)|(extractedData_parsed["periodTypeId"] == 10)))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 

        FY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list)) &(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())& (extractedData_parsed["periodTypeId"] == 1))][['dataItemId','peo','scale','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        # print(FQ)
        # print(FY)

        FQ["valuesign"]=np.sign((FQ['value']).astype(float))
        FY["valuesign"]=np.sign((FY['value']).astype(float))

        # print(FQ)
        # print(FY)

        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId',],how='inner')

        # print(merged_df)
        # print(merged_df[['periodTypeId_x','periodTypeId_y']])

        merged_df_PEO_count= merged_df.groupby(['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId_x']).agg(quarter_sum=('valuesign_x','sum'),fy_sum=('valuesign_y','sum')).reset_index() 



        # print(merged_df_PEO_count)

        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff='NA'
        perc='NA'
        
        for ind,row in merged_df_PEO_count.iterrows():
            if (row['periodTypeId_x']==2 and abs(row['fy_sum'])==4 and abs(row['quarter_sum'])==4):
                if execute_operator(row['quarter_sum'],row['fy_sum'],operator[0]):
                    peos.append(row['fiscalYear'])
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])

        for ind,row in merged_df_PEO_count.iterrows():
            if (row['periodTypeId_x']==10 and abs(row['fy_sum'])==2 and abs(row['quarter_sum'])==2):
                if execute_operator(row['quarter_sum'],row['fy_sum'],operator[0]):
                    peos.append(row['fiscalYear'])
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])


        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"fiscalYear":peos,"diff":diff,"perc":perc})
        
        diff_df['peocomb']=diff_df.apply(lambda x: '%s%s' % (x['dataItemId'],x['fiscalYear']),axis=1)
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x: '%s%s' % (x['dataItemId'],x['fiscalYear']),axis=1 )
        
        temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) & (extractedData_parsed['tradingItemId'].isin(tid))
                                                  &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))
                                                  &(extractedData_parsed['value'] != "") &(extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear', 'fiscalQuarter','tradingItemName','peocomb']] #.drop_duplicates()    

        temp1_revised=temp1.dropna()
        
        for ind, row in temp1_revised.iterrows():
            if row['value']!=0:
                result = {"highlights": [], "error": "All Quarter OR Semis values are collected in one sign and Fiscal Year value collected in another sign."}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"],  "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)                    

        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_10G(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #REVENUE
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT

    try:
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #REVENUE
        
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        
        # print(lhs_df)
        # print(rhs_df)
        
        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
            
        # print(merged_df)
        
        dataItemIds_x=[]    
        dataItemIds_y=[]        
        peos=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        diff='NA'
        perc='NA'

        for ind,row in merged_df.iterrows():
                if row['value_scaled_x']==0.0:
                    if row['value_scaled_y']>0:
                        dataItemIds_x.append(row['dataItemId_x'])
                        dataItemIds_y.append(row['dataItemId_y'])
                        peos.append(row['peo'])
                        tid.append(row['tradingItemId'])
                        parentflag.append(row['parentFlag'])
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
        diff_df=pd.DataFrame({"dataItemId_x":dataItemIds_x,"dataItemId_y":dataItemIds_y,"peo":peos,"diff":diff,"perc":perc}) 
                           
        temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()

        temp1_revised=temp1.dropna()

        for ind, row in temp1_revised.iterrows():
            result = {"highlights": [], "error": "Revenue Estimate is Zero and EBIT in Positive"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
            errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_18B(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #CAPEX
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #FCF
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CAPEX
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        

        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        #if merged_df is not None:
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])               
                difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(difference)))
                perc.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
       
        if len(diff_df)>0:
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['fiscalChainSeriesId','dataItemId', 'peo', 'value', 'scale','currency','accountingStandardDesc','parentFlag','team','description','tradingItemId', 'fiscalYear', 'fiscalQuarter','tradingItemName']]
    
            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Same values collected for Capex and Free cash flow"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_11B(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag1_list=get_dataItemIds_list('TAG1', parameters) # NI GAAP
    tag2_list=get_dataItemIds_list('TAG2', parameters) # NI Normalized
    tag3_list=get_dataItemIds_list('TAG3', parameters) # EPS GAAP
    tag4_list=get_dataItemIds_list('TAG4', parameters) # EPS Normalized
    tag5_list=get_dataItemIds_list('LHSdataItemIds', parameters) # EBT GAAP
    tag6_list=get_dataItemIds_list('RHSdataItemIds', parameters) # EBT Normalized
    operator = get_dataItemIds_list('Operation', parameters) #["=="],["!="] 
    max_threshold=get_parameter_value(parameters,'Max Threshold')  #[4]
    min_threshold=get_parameter_value(parameters,'Min_Threshold') #[1]
    try:

                
        eps_gaap_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag3_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        eps_nor_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag4_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        eps_fy = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag3_list))|(extractedData_parsed['dataItemId'].isin(tag4_list)))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 

        ni_gaap_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        ni_nor_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        ni_fy = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag1_list))|(extractedData_parsed['dataItemId'].isin(tag2_list)))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 

        
        ebt_gaap_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag5_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        ebt_nor_quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag6_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

        ebt_fy = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag5_list))|(extractedData_parsed['dataItemId'].isin(tag6_list)))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 

        merged_eps_quarter=pd.merge(eps_gaap_quarter,eps_nor_quarter,on=['peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        eps_fy_group=eps_fy.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        eps_quarter_group=merged_eps_quarter.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        eps_merged_df=pd.merge(eps_fy_group, eps_quarter_group,on=['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear'],how='inner')


        merged_ni_quarter=pd.merge(ni_gaap_quarter,ni_nor_quarter,on=['peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        ni_fy_group=ni_fy.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        ni_quarter_group=merged_ni_quarter.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        ni_merged_df=pd.merge(ni_fy_group, ni_quarter_group,on=['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear'],how='inner')
        
                            
        fiscalyear=[]
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]        
        diff=[]
        perc=[]
        for ind,row in eps_merged_df.iterrows():

            if ((execute_operator(row['peo_count_x'],float(min_threshold[0]),operator[0]))&(execute_operator(row['peo_count_y'],float(max_threshold[0]),operator[0]))):

                fiscalyear.append(row['fiscalYear'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                tid.append(row['tradingItemId'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'

        diff_df1=pd.DataFrame({"fiscalYear":fiscalyear,"diff":diff,"perc":perc})

        eps = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag3_list))|(extractedData_parsed['dataItemId'].isin(tag4_list)))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) & (extractedData_parsed['fiscalYear'].isin(diff_df1['fiscalYear']))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','tradingItemName','fiscalYear','fiscalQuarter']]

        fiscalyear=[]
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]        
        diff=[]
        perc=[]
        for ind,row in ni_merged_df.iterrows():
 
            if ((execute_operator(row['peo_count_x'],float(min_threshold[0]),operator[0]))&(execute_operator(row['peo_count_y'],float(max_threshold[0]),operator[0]))):

                fiscalyear.append(row['fiscalYear'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'

        diff_df2=pd.DataFrame({"fiscalYear":fiscalyear,"diff":diff,"perc":perc})


        ni = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag1_list))|(extractedData_parsed['dataItemId'].isin(tag2_list)))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) & (extractedData_parsed['fiscalYear'].isin(diff_df2['fiscalYear']))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','tradingItemName','fiscalYear','fiscalQuarter']]

        merged_ebt_quarter=pd.merge(ebt_gaap_quarter,ebt_nor_quarter,on=['peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        ebt_fy_group=ebt_fy.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        ebt_quarter_group=merged_ebt_quarter.groupby(['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']).agg(peo_count=('peo','count')).reset_index()

        ebt_merged_df=pd.merge(ebt_fy_group, ebt_quarter_group,on=['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear'],how='inner')

        fiscalyear=[]
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]        
        diff=[]
        perc=[]
        for ind,row in ebt_merged_df.iterrows():

            if ((execute_operator(row['peo_count_x'],float(min_threshold[0]),operator[0]))&(execute_operator(row['peo_count_y'],float(max_threshold[0]),operator[0]))):

                fiscalyear.append(row['fiscalYear'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'

        diff_df3=pd.DataFrame({"fiscalYear":fiscalyear,"diff":diff,"perc":perc})

        ebt = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag5_list))|(extractedData_parsed['dataItemId'].isin(tag6_list)))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) & (extractedData_parsed['fiscalYear'].isin(diff_df3['fiscalYear']))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','tradingItemName','fiscalYear','fiscalQuarter']]
        
        eps_revised=eps.dropna()

        for ind, row in eps_revised.iterrows():

            result = {"highlights": [], "error": "All Q's duplicated but we have single flavor for FY"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df1[diff_df1['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df1[diff_df1['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
            errors.append(result)
        
        ni_revised=ni.dropna()

        for ind, row in ni_revised.iterrows():

            result = {"highlights": [], "error": "All Q's duplicated but we have single flavor for FY"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df2[diff_df2['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df2[diff_df2['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df2[diff_df2['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df2[diff_df2['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
            errors.append(result)
            
        ebt_revised=ebt.dropna()
        for ind, row in ebt_revised.iterrows():

            result = {"highlights": [], "error": "All Q's duplicated but we have single flavor for FY"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df3[diff_df3['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df3[diff_df3['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df3[diff_df3['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df3[diff_df3['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
            errors.append(result)  
            
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors    
    
#Estimates Error Checks 
@add_method(Validation) 
def EST_15A(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    try:
        temp0 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','scale','scaleId','value','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].drop_duplicates()

        temp1=temp0.groupby(['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'])['scaleId'].nunique().reset_index(name='countunits')

        if len(temp0)>0:
            merged_df=pd.merge(temp0,temp1[temp1['countunits']>1],on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner') 
            
            # print(temp0)
            # print(temp1)
            # print(merged_df)
            

            dataItemIds=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]
            peos=[]
            # scid1=[]
            # scid2=[]
            
            for ind, row in merged_df.iterrows():
                if row['countunits'] > 1:
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    peos.append(row['peo'])
                    # scid1.append(row['scaleId_x'])
                    # scid2.append(row['scaleId_y'])
                    diff='NA'
                    perc='NA'

                
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc})
            
            # print(diff_df)

            if len(diff_df)>0:
                temp2=extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['tradingItemId'].isin(tid))
                                                        &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))
                                                        &(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['fiscalChainSeriesId','dataItemId', 'value','peo', 'scale', 'scaleId','currency','accountingStandardDesc','parentFlag','team','description','tradingItemId', 'fiscalYear', 'fiscalQuarter','tradingItemName']].drop_duplicates()
        
                temp2_revised=temp2.dropna()
                
                # print(temp2_revised)

                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Unit Variation"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"], "tradingItemName": row["tradingItemName"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                    errors.append(result)
                    # result = {"highlights": [], "error": "Tags which have Units variation"}
                    # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    # result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    # result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    # errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_14(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    # operator = get_dataItemIds_list('Operation', parameters) #["==","!="]
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    try:
        temp0 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','scale','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']]
        
        # print(temp0)
        
        temp1=temp0.groupby(['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'])['currency'].nunique().reset_index(name='currencycount')

        # print(temp1)          
        if len(temp0)>0:
            merged_df=pd.merge(temp0,temp1[temp1['currencycount']>1],on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner') 

        # print(merged_df)

            dataItemIds=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]
        
        # if merged_df is not None:
            for ind, row in merged_df.iterrows():
                if row['currencycount'] > 1:
                    # if execute_operator(row['peo_x'],row['peo_y'],operator[1]):
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'

            
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc})
            
            if len(diff_df)>0:
                temp2=extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemIds))&(extractedData_parsed['tradingItemId'].isin(tid))
                                                          &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))
                                                          &(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear', 'fiscalQuarter','tradingItemName']]
        
                temp2_revised=temp2.dropna()
                
                for ind, row in temp2_revised.iterrows():
                                
                    result = {"highlights": [], "error": "Tags which have currency variation"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors
    
#Estimates Error Checks 
@add_method(Validation) 
def EST_60(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    # dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    try:

        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="E")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(~(extractedData_parsed['parentFlag'].isin(historicalData_parsed['parentFlag']))
                                        |~(extractedData_parsed['tradingItemId'].isin(historicalData_parsed['tradingItemId']))
                                        |~(extractedData_parsed['accountingStandardDesc'].isin(historicalData_parsed['accountingStandardDesc']))
                                        |~(extractedData_parsed['fiscalChainSeriesId'].isin(historicalData_parsed['fiscalChainSeriesId']))
                                        ))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']]  #(extractedData_parsed['dataItemFlag']=="E")
        

        # print(current)
        if len(current)>0:
            dataItemIds=[]
            parentflag=[]
            accounting=[]
            # peos=[]
            tid=[]
            fyc=[]        
            diff=[]
            perc=[]
            for ind,row in current.iterrows():
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                # peos.append(row['peo'])
                diff='NA'
                perc='NA'
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc}) #'peo':peos
            
            # print(diff_df)

            temp= extractedData_parsed[(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&((extractedData_parsed['parentFlag'].isin(parentflag))|(extractedData_parsed['tradingItemId'].isin(tid))|(extractedData_parsed['accountingStandardDesc'].isin(accounting))|(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))][['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','team','description','tradingItemName']].drop_duplicates()

            temp_revised=temp.dropna()

            for ind, row in temp_revised.iterrows():
                result = {"highlights": [], "error": "New flavor captured for the company"}
                # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                # result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                # result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA', "diff": 'NA', "percent": 'NA'}]
                # errors.append(result) 
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row["tradingItemId"],"tradingItemName": row['tradingItemName'],"accountingStdDesc": row['accountingStandardDesc'],"parentConsolidatedFlag": row['parentFlag'],"fiscalChainSeries": row['fiscalChainSeriesId'],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)                    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_10F(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBT_GAAP #EBT_Norm
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #ETR

    try:
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] 


        if ((len(lhs_df)>0) & (len(rhs_df)>0)):
            lhs_df['value_sign']=np.sign(lhs_df['value_scaled'])
            rhs_df['value_sign']=np.sign(rhs_df['value_scaled'])
            lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_sign'].sum().reset_index() #EBITDA
            rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_sign'].sum().reset_index() #EBIT  

            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

            if len (merged_df)>0:
                
                peos=[]
                diff=[]
                perc=[]
                tid=[]
                parentflag=[]
                accounting=[]
                fyc=[] 

                for ind,row in merged_df.iterrows():
                    if ((row['value_sign_x']<0 )& (row['value_sign_y']!=0)):
                        peos.append(row['peo'])
                        tid.append(row['tradingItemId'])
                        parentflag.append(row['parentFlag'])
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        diff='NA'
                        perc='NA'
                diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
                
                if len(diff_df)>0:

                    temp1 = extractedData_parsed[((extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','fiscalYear','fiscalQuarter','fiscalChainSeriesId']]
        
                    temp1_revised=temp1.dropna() 


                    for ind, row in temp1_revised.iterrows():
                        result = {"highlights": [], "error": "Effective Tax Rate  (%) collected when EBT  is negative"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                        errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_56I(historicalData,filingMetadata,extractedData,parameters):
    #JSON 15
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #CFO, CAPEX, M.CAPEX
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #FCF
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:

        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CFO, CAPEX, M.CAPEX

        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        # print(lhs_df)
        # print(rhs_df)


        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()        
                
        # print(lhs_df)
        # print(rhs_df)

        #if len(left_dataItemIds_list)==3:
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        # print(merged_df)


        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        #if merged_df is not None:
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])               
                difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
                perc.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
    
        if len(diff_df)>0:
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                    &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'scale', 'currency', 'description','team', 'fiscalYear', 'fiscalQuarter','tradingItemName']]
    
            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Free Cash Flow is equal to sum of Capex, M.capex and Cash from Operations"}
                # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
                # result["checkGeneratedFor"]={"statement": "", "dataItemId": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                # result["checkGeneratedForList"]=[{"dataItemId": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                # errors.append(result) 
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_56C(historicalData,filingMetadata,extractedData,parameters):
    #JSON 16
    errors = []
    left_dataItemIds_list = get_dataItemIds_list('LHSdataItemIds', parameters)  # Revenue
    right_dataItemIds_list = get_dataItemIds_list('RHSdataItemIds', parameters)  # EBIT
    tag_list = get_dataItemIds_list('TAG1', parameters)  # GM

    try:
        gm = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list)) &
                                   (extractedData_parsed['consValue']!="")&
                                   (extractedData_parsed['consValue'].notnull()))][['dataItemId', 'peo', 'parentFlag', 'consValue', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter']]  # GM

        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) &
                                       (extractedData_parsed['value'] != "") &
                                       (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
             'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter']]  # Revenue
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) &
                                       (extractedData_parsed['value'] != "")
                                       & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
             'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter']]  # EBIT

        print(gm)
        print(lhs_df)
        print(rhs_df)


        merged_df = pd.merge(lhs_df, rhs_df, on=['peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                                 'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear',
                                                 'value_scaled'], how='inner')
        
        # print(merged_df)
        merged_DA_df = pd.merge(gm, merged_df,
                                on=['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId',
                                    'peo'], how='inner')
        
        # print(merged_DA_df)

        if len(merged_DA_df)>0:
            peos = []
            tid = []
            parentflag = []
            accounting = []
            fyc = []
            dataitem_x = []
            dataitem_y = []
            diff=[]
            perc=[]

            for ind, row in merged_DA_df.iterrows():

                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                dataitem_x.append(row['dataItemId_x'])
                dataitem_y.append(row['dataItemId_y'])
                diff="NA"
                perc="NA"
                

            diff_df = pd.DataFrame({"peo": peos, "dataItemId_x": dataitem_x,"dataItemId_y": dataitem_y,"diff":diff,"perc":perc}).drop_duplicates()

            temp1 = extractedData_parsed[((extractedData_parsed['peo'].isin(diff_df['peo'])) &
                                          (extractedData_parsed['dataItemId'].isin(dataitem_x))&
                                          (extractedData_parsed['tradingItemId'].isin(tid)) &
                                          (extractedData_parsed['parentFlag'].isin(parentflag)) &
                                          (extractedData_parsed['accountingStandardDesc'].isin(accounting)) &
                                          (extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) &
                                          (extractedData_parsed['value'] != "") &
                                          (extractedData_parsed['value'].notnull()))][['fiscalChainSeriesId','dataItemId', 'peo', 'value', 'scale','currency','accountingStandardDesc','parentFlag','team','description','tradingItemId', 'fiscalYear', 'fiscalQuarter','tradingItemName']]

            temp1_revised = temp1.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],"error": "Same values collected for Revenue and EBIT where Gross Margin has consensus"}
                # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                # result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                # result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                # errors.append(result)
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_56G(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBITDA
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT
    tag_list=get_dataItemIds_list('TAG1', parameters) #DA

    try:

        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT
        DA_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag_list))&(historicalData_parsed['periodTypeId']!=1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['fiscalYear'].isin(lhs_df['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #DA

        print(lhs_df)
        print(rhs_df)
        print(DA_df)

        if ((len(lhs_df)>0) & (len(rhs_df)>0)& (len(DA_df)>0)):

            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        
            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','value_scaled'],how='inner')

            merged_DA_df = pd.merge(DA_df,merged_df,on=['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear'],how='inner')
            

            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            dataitem=[]
            diff=[]
            perc=[]
    
            for ind, row in merged_DA_df.iterrows():
                if (row['value_scaled_x']!=0):
                    peos.append(row['peo_y'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    dataitem.append(row['dataItemId_x'])
                    diff="NA"
                    perc="NA"
    
            diff_df=pd.DataFrame({"peo":peos,"dataitem":dataitem}).drop_duplicates()
            
    
            if len(diff_df)>0:
                diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataitem'],x['peo']),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
    
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','fiscalYear','fiscalQuarter','fiscalChainSeriesId']].drop_duplicates()
                temp1_revised=temp1.dropna()
                
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "EBIT estimate = EBITDA estimate in FY but D&A Actual available in related actualized Qs"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)
    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_333(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list = get_dataItemIds_list('LHSdataItemIds', parameters)
    country1 = get_dataItemIds_list('COUNTRY_INCLUDE', parameters)  # Japan
    try:
        
        if (filingMetadata['metadata']['country'] not in country1):
            temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['parentFlag']=='Parent')&(extractedData_parsed['value'] != "")&(extractedData_parsed['value'].notnull()))][['dataItemId', 'peo', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId']]
            # print(temp)

            if len(temp) > 0:
                # dataItemIds = []
                # peos = []
                # tid = []
                parentflags = []
                # accounting = []
                # fyc = []
                diff = []
                perc = []

                for ind, row in temp.iterrows():
                    # dataItemIds.append(row['dataItemId'])
                    # peos.append(row['peo'])
                    # tid.append(row['tradingItemId'])
                    parentflags.append(row['parentFlag'])
                    # accounting.append(row['accountingStandardDesc'])
                    # fyc.append(row['fiscalChainSeriesId'])
                    diff = 'NA'
                    perc = 'NA'

                diff_df = pd.DataFrame({"parentflag": parentflags})

                temp1 = extractedData_parsed[((extractedData_parsed['parentFlag'].isin(parentflags)))][['parentFlag','team', 'description','accountingStandardDesc', 'scale', 'currency']]  #[['dataItemId', 'value', 'peo', 'scale', 'currency', 'parentFlag', 'accountingStandardDesc','tradingItemId', 'team', 'description', 'tradingItemName', 'fiscalYear', 'fiscalQuarter','fiscalChainSeriesId']].drop_duplicates()

                temp1_revised = temp1.dropna()
                
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Parent data collected for other Asian comp"}
                    result["highlights"].append({"parentConsolidatedFlag": row['parentFlag'], "row": {"accountingStdDesc": row['accountingStandardDesc']}, "section": row['team'], "scale": row['scale'],  "currency": row['currency'],
                                                 "filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"] = {"statement": "", "tag": row['parentFlag'],"description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],
                                                   "feedFileId": filingMetadata['metadata']['feedFileId'],
                                                   "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"] = [
                        {"tag": row['parentFlag'], "description": row["description"], "accountingStdDesc": row["accountingStandardDesc"],
                         "parentConsolidatedFlag": row["parentFlag"],
                         "diff": 'NA',
                         "percent": 'NA'}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_102(historicalData,filingMetadata,extractedData,parameters):
    errors = []   
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #[!=]
    
    try:
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']
        
        # print(extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))
        # print(historicalData_parsed['companyId'])
        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','scale','currency','currencyId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        # print(current)

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(current['dataItemId']))&(historicalData_parsed['estimatePeriodId'].isin(current['estimatePeriodId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['peo'].isin(current['peo'])))][['dataItemId','peo','estimatePeriodId','value','scale','currency','currencyId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate']].drop_duplicates()
        
        # print(current)
        # print(previous)
        
        # maxprevious1=previous.groupby(['researchContributorId'])['filingDate'].max().reset_index()
        
        maxprevious1=previous.groupby(['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]

        if ((len(current)>0) & (len(maxprevious)>0)):
            merged_df=pd.merge(current,maxprevious,on=['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner').drop_duplicates()
        
        # print(merged_df)
        # print(merged_df[['dataItemId','peo','value_x','value_y','currency_x','currency_y','currencyId_x','currencyId_y']])

            dataItemIds=[]
            previousdate=[]
            parentflag=[]
            peos=[]
            curx=[]
            cury=[]
            AS=[]
            fyc=[]
            diff=[]
            perc=[]
            
            for ind, row in merged_df.iterrows():
                if execute_operator(row['currencyId_x'],row['currencyId_y'],operator[0]):
                    peos.append(row['peo'])
                    curx.append(row['currencyId_x'])
                    cury.append(row['currencyId_y'])
                    dataItemIds.append(row['dataItemId'])
                    previousdate.append(row['filingDate'])
                    parentflag.append(row['parentFlag'])
                    AS.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'  
                    
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"filingDate":previousdate,"diff":diff,"perc":perc})
            # print(diff_df)
            
    
            if len(diff_df)>0:
                # diff_df['curcomb']=diff_df.apply(lambda x:'%s%s%s' % (x['dataItemId'],x['peo'],x['currencyId']),axis=1)
                # historicalData_parsed['curcomb']=historicalData_parsed.apply(lambda x:'%s%s%s' % (x['dataItemId'],x['peo'],x['currencyId']),axis=1)
        
                temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(~(extractedData_parsed['currencyId'].isin(cury)))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(AS))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','fiscalYear','fiscalQuarter','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName']].drop_duplicates()
                # print(temp1)
    
                # temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(historicalData_parsed['currencyId'].isin(cury)) &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(AS))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['researchContributorId']==contributor))] [['dataItemId','value','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
                # print(temp2)
    
                temp1_revised=temp1.dropna()    
                # temp2_revised=temp2.dropna() 
                
                # print(temp1_revised)
    
                for ind, row in temp1_revised.iterrows():
    
                    result = {"highlights": [], "error": "Currency Difference between current and Previous document"}
                    # result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"section": row['team'], "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    # result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    # result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    # errors.append(result)
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)
                # for ind, row in temp2_revised.iterrows():
                #         result = {"highlights": [], "error": "Currency Difference between current and Previous document"}
                #         result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"section": row['team'], "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                #         result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                #         result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                #         errors.append(result)
            
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_5B(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)    
    scale_list=get_dataItemIds_list('Scale_list', parameters)
    try:

        res = [eval(i) for i in scale_list]
        print(res)
        non_periodic = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['tradingItemId']!=-1)&(extractedData_parsed['scaleId']!=-1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&~(extractedData_parsed['scaleId'].isin(res)))][['dataItemId','value','scaleId','currency','tradingItemId','accountingStandardDesc']]

        periodic = extractedData_parsed[((extractedData_parsed['tradingItemId']!=-1)&(extractedData_parsed['scaleId']!=-1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&~(extractedData_parsed['scaleId'].isin(res))&~(extractedData_parsed['dataItemId'].isin(dataItemId_list)))][['dataItemId','peo','value','scaleId','currency','tradingItemId','accountingStandardDesc','periodTypeId','fiscalYear','fiscalQuarter','parentFlag','fiscalChainSeriesId']]
        
        #print(non_periodic)
        if len(periodic)>0:
            dataItemIds=[]
            tid=[]
            peo=[]
            parentflag=[]
            fyc=[]
            accounting=[]
            diff=[]
            perc=[]
            for ind, row in periodic.iterrows():
    
                dataItemIds.append(row['dataItemId'])
                peo.append(row['peo'])
                parentflag.append(row['parentFlag'])
                tid.append(row['tradingItemId']) 
                fyc.append(row['fiscalChainSeriesId'])
                accounting.append(row['accountingStandardDesc'])
                diff='NA'                    
                perc='NA'        

            diff_df1=pd.DataFrame({"dataItemId":dataItemIds,"peo":peo,"diff":diff,"perc":perc,"tradingItemId":tid})   

    
            if len(diff_df1)>0:
                diff_df1['peocomb']=diff_df1.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
    
                temp0 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df1['peocomb']))&(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']]
                
                temp0_revised=temp0.dropna()
                
                for ind, row in temp0_revised.iterrows():
                    result = {"highlights": [], "error": "Per share data item captured other than in absolutes or 1/100"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df1[diff_df1['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df1[diff_df1['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
        
        if len(non_periodic)>0:
            dataItemIds=[]
            tid=[]
            accounting=[]
            diff=[]
            perc=[]
            for ind, row in non_periodic.iterrows():
    
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId']) 
                accounting.append(row['accountingStandardDesc'])
                diff='NA'                    
                perc='NA'        
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc,"tradingItemId":tid})


            if len(diff_df)>0:       
                temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName']]
    
                temp1_revised=temp1.dropna()
            
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Per share data item captured other than in absolutes or 1/100"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo":'NA', "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries":'NA', "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)
        
  
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors
#Estimates Error Checks 
@add_method(Validation) 
def EST_6A(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    operator=get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold') 
    
    try:
     
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']


        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=='E')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','parentFlag','accountingStandardDesc','tradingItemId']]


        previous = historicalData_parsed[((historicalData_parsed['dataItemFlag']=='E')&(historicalData_parsed['value']!="")&
                                          (historicalData_parsed['researchContributorId']==contributor)&
                                          (historicalData_parsed['filingDate']<filingdate)&
                                          (historicalData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate','researchContributorId']]
        
        previous['daysdiff'] = abs((pd.to_datetime(filingdate) - pd.to_datetime(previous['filingDate'])).dt.days)

      
        maxprevious=previous.groupby(['researchContributorId'])['filingDate'].max().reset_index()
        
        
        
        previousdoc=previous[(previous['filingDate'].isin(maxprevious['filingDate'])&(previous['researchContributorId']==contributor))]

        temp=previousdoc[~((previousdoc['dataItemId'].isin(current['dataItemId']))&(previousdoc['parentFlag'].isin(current['parentFlag']))
                         &(previousdoc['accountingStandardDesc'].isin(current['accountingStandardDesc']))&(previousdoc['tradingItemId'].isin(current['tradingItemId'])))]
        

        dataItemIds=[]
        previousdate=[]
        parentflag=[]
        AS=[]
        tid=[]
        diff=[]
        perc=[]        
        for ind, row in temp.iterrows():
            if execute_operator(row['daysdiff'],float(Threshold[0]),operator[0]):
                dataItemIds.append(row['dataItemId'])
                previousdate.append(row['filingDate'])
                parentflag.append(row['parentFlag'])
                AS.append(row['accountingStandardDesc'])
                tid.append(row['tradingItemId'])
                diff='NA'
                perc='NA'  
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"filingDate":previousdate,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
       

            temp1 = historicalData_parsed[((historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['dataItemId'].isin(diff_df['dataItemId'])) &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(AS))&(historicalData_parsed['tradingItemId'].isin(tid))
                                           &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))] [['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate']]
            

            temp1_revised=temp1.dropna()         
            for ind, row in temp1_revised.iterrows():
    
                result = {"highlights": [], "error": "Tag not in current document (Comparable)"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors
#Estimates Error Checks 
@add_method(Validation) 
def EST_61(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
        errors = []
        left_list=get_dataItemIds_list2('LHSdataItemIds', parameters)
        max_threshold=get_parameter_value(parameters,'Max Threshold') 
        min_threshold=get_parameter_value(parameters,'Min_Threshold') #100%
        operator = get_dataItemIds_list('Operation', parameters)     
        try:
            
            filingdate=filingMetadata['metadata']['filingDate']
            contributor=filingMetadata['metadata']['researchContributorId']
            
            lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','value_scaled','currencyId','parentFlag','tradingItemId']]

            previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(left_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','value','value_scaled','currency','tradingItemId','filingDate','currencyId','parentFlag']]
            
            maxprevious1=previous.groupby(['dataItemId'])['filingDate'].max().reset_index()
            
            maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
            merged_df=pd.merge(lhs_df,maxprevious,on=['dataItemId','parentFlag','tradingItemId'],how='inner')
            
            print(lhs_df)
            print(maxprevious)
            
            
            
            merged_df['variation']=((merged_df[['value_scaled_x', 'value_scaled_y']].max(axis=1) - merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) / merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) * 100
            
            print(merged_df)
            
            dataItemIds=[]
            parentflag=[]
            # accounting=[]
            tid=[]
            # fyc=[]        
            diff=[]
            perc=[]
            for ind,row in merged_df.iterrows():
                if ((execute_operator(row['variation'], float(min_threshold[0]), operator[0]))|(execute_operator(row['variation'], float(max_threshold[0]), operator[1]))):
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag']) 
                    # accounting.append(row['accountingStandardDesc']) 
                    # fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'

            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc})
            
            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId'])) &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','tradingItemName']]

            temp1_revised=temp1.dropna()
                                                                                                                                                                                                                                          
            for ind, row in temp1_revised.iterrows():
                    
                result = {"highlights": [], "error": " captured price target is not in given range"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": 'NA', "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)
         
            print(errors) 
            return errors                                                                   
        except Exception as e:
            print(e)
            return errors   
#Estimates Error Checks 
@add_method(Validation) 
def EST_13B(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    
    try:

        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        

        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','value','currency','tradingItemId']]
        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','value','currency','tradingItemId','filingDate']]
        maxprevious1=previous.groupby(['dataItemId'])['filingDate'].max().reset_index()
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
        merged_df=pd.merge(current,maxprevious,on=['dataItemId'],how='inner')
        
        print(merged_df)

        dataItemIds=[]
        previousdate=[]
        tid_x=[]
        tid_y=[]
        diff=[]
        perc=[]        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['tradingItemId_x'],row['tradingItemId_y'],operator[0]):
                tid_x.append(row['tradingItemId_x']) 
                tid_y.append(row['tradingItemId_y'])
                diff='NA'
                perc='NA'
                dataItemIds.append(row['dataItemId']) 
                previousdate.append(row['filingDate'])
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,'filingDate':previousdate,"diff":diff,"perc":perc})
        if len(diff_df)>0:

            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['tradingItemId'].isin(tid_x))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName']]
            
            temp1_revised=temp1.dropna()
           
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(historicalData_parsed['tradingItemId'].isin(tid_y))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['researchContributorId']==contributor))] [['dataItemId','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate']]

            temp2_revised=temp2.dropna() 
            
    
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Target price Trading item variation between current and previous document"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": 'NA', "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Target price Trading item variation between current and previous document"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": 'NA', "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo":'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
           
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_15G(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)        
    operator = get_dataItemIds_list('Operation', parameters) #[!=]
    
    try:
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']
        
        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(current['dataItemId']))&(historicalData_parsed['estimatePeriodId'].isin(current['estimatePeriodId']))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                          &(historicalData_parsed['peo'].isin(current['peo'])))][['dataItemId','peo','estimatePeriodId','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate']]
        
        
        # print(current)
        # print(previous)

        maxprevious=previous.groupby(['dataItemId','peo','scale','value','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'])['filingDate'].max().reset_index()

        # print(current)
        # print(previous)
        # print(maxprevious)

        merged_df=pd.merge(current,maxprevious,on=['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','value'],how='inner')
        
        # print(merged_df)
        # print(merged_df[['dataItemId','peo','currency_x','currency_y']])

        filingdate=[]
        dataItemIds=[]
        peos=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        diff=[]
        perc=[]

        #if merged_df is not None:
        for ind,row in merged_df.iterrows():
            if (execute_operator(row['currency_x'],row['currency_y'],operator[0]) | execute_operator(row['scale_x'],row['scale_y'],operator[0])):
                filingdate.append(row['filingDate'])
                dataItemIds.append(row['dataItemId'])
                peos.append(row['peo'])  
                tid.append(row['tradingItemId']) 
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                                       
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,"filingDate":filingdate})

        if len(diff_df)>0:       
            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemIds)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                                  &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']] 
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemIds))&(historicalData_parsed['companyId']==companyid) & (historicalData_parsed['peo'].isin(peos)) 
                                           & (historicalData_parsed['filingDate'].isin(filingdate))&(historicalData_parsed['tradingItemId'].isin(tid))&(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(accounting))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId']]
           
            temp1_revised=temp1.dropna()  
            temp2_revised=temp2.dropna()  
    
            for ind, row in temp1_revised.iterrows():
    
                result = {"highlights": [], "error": "Units length difference is more than 3 digits compare to previous documents"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
           
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Units length difference is more than 3 digits compare to previous documents"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors  
#Estimates Error Checks 
@add_method(Validation) 
def EST_33(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #Any Dataitemid
    operator = get_dataItemIds_list('Operation', parameters) #[">="]

    try:
        FQ = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==2))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]  #Quarters data
        # print(FQ)
        HY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==10))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]  #semi anual data
        # print(HY)
        FY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list)) &(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==1))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]  #Annual data
        # print(FY)
    
        
        if (((len(FY)>0) & (len(FQ)>0))):
            base_currency=FQ.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
        # merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        elif ((len(HY)>0) & (len(FQ)>0)):    
            base_currency=FQ.currency.mode()[0]
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            HY["value_scaled"] = HY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)            
        
        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')    
        merged_df1=pd.merge(FQ,HY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        # print(merged_df)
        # print(merged_df1)
            
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]

        if merged_df is not None:
            for ind,row in merged_df.iterrows():
                if  execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    diff.append(float(round(difference)))
                    peos.append(row['fiscalYear'])
                    dataItemIds.append(row['dataItemId'])
                    FQx.append(row['fiscalQuarter_x'])
                    FQy.append(row['fiscalQuarter_y'])
                    perc.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))    
        if merged_df1 is not None:
            for ind,row in merged_df1.iterrows():
                if  execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    diff.append(float(round(difference)))
                    peos.append(row['fiscalYear'])
                    dataItemIds.append(row['dataItemId'])
                    FQx.append(row['fiscalQuarter_x'])
                    FQy.append(row['fiscalQuarter_y'])
                    perc.append(float(round(difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))  
                    
        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy})
            
        # print(diff_df)

        if len(diff_df)>0:
            temp0 = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(dataItemIds) & extractedData_parsed['fiscalYear'].isin(peos)
                                            &(extractedData_parsed["value"] != "")&(extractedData_parsed['value'].notnull())&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','tradingItemName']]
            
            temp0_revised=temp0.dropna()

            for ind, row in temp0_revised.iterrows():
                result = {"highlights": [], "error": "Any of the quarter value is  equals or more than  to related fiscal year value"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['description'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)                           
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_35(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # tags
    operator = get_dataItemIds_list('Operation', parameters)
    try:
        documentdate=filingMetadata['metadata']['filingDate']

        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(dataItemId_list)))&(extractedData_parsed['value'] != "") &(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!= "")&(extractedData_parsed['peo'].notnull()))][['dataItemId', 'peo', 'fiscalChainSeriesId']].drop_duplicates()
        
        temp['companyId'] = filingMetadata['metadata']['companyId']
        
        print(temp)

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['value'] != "")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId'].isin(temp['companyId']))&(historicalData_parsed['peo']!="")&(historicalData_parsed['filingDate'] < documentdate)&(historicalData_parsed['peo'].notnull()))][['dataItemId', 'peo','fiscalChainSeriesId', 'filingDate','companyId']].drop_duplicates()
        
        print(previous)
        newfyc=temp[~((temp['fiscalChainSeriesId']).isin(previous['fiscalChainSeriesId']))]
        
        # print(newfyc)
        
        maxprevious = previous.groupby(['companyId','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        
        # print(maxprevious)

        previous = previous[previous['filingDate'].isin(maxprevious['filingDate'])]


        merged_df = pd.merge(temp, previous, on=['companyId'], how='inner')
        
        print(merged_df)

        filingdate = []
        diff = []
        perc = []
        series1 = []
        series2 = []
        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'], row['fiscalChainSeriesId_y'], operator[0]):
                filingdate.append(row['filingDate'])
                difference = 'NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc = 'NA'
                

        diff_df = pd.DataFrame({"diff": diff, "perc": perc, "filingDate": filingdate, "curseries": series1, "preseries": series2}).drop_duplicates()


        temp1 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(series1))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()

        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (historicalData_parsed['fiscalChainSeriesId'].isin(series2)) )][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team', 'versionId', 'feedFileId','filingDate', 'companyId']].drop_duplicates()

        if len(newfyc)>0:
            temp3 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(newfyc['fiscalChainSeriesId']))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
            
            # print(temp3)
            
            temp3_revised=temp3.dropna()
            
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],"error": "New fiscal Year series collected for this company"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

        if len(temp1) > 0 and len(temp2) > 0:
            temp1_revised = temp1.dropna()
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],"error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

            temp2_revised = temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)


        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors    

#Estimates Error Checks 
@add_method(Validation) 
def EST_59A(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold') 
    try:

        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']
        
        # print(filingdate)

        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','value','value_scaled','peo','currency','tradingItemId']].drop_duplicates()
        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','value','value_scaled','peo','currency','tradingItemId','filingDate']].drop_duplicates()

        current['peo']=current['peo'].fillna('NP: 1234')
        previous['peo']=previous['peo'].fillna('NP: 1234')        

        maxprevious1=previous.groupby(['dataItemId'])['filingDate'].max().reset_index()
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
        
        maxprevious = maxprevious.copy()
        # maxprevious['hours'] = abs(((pd.to_datetime('filingdate')) - (pd.to_datetime(maxprevious['filingDate']))).dt.total_seconds() / 3600)
        maxprevious.loc[:,'hours'] = abs((pd.to_datetime(filingdate) - pd.to_datetime(maxprevious['filingDate'])).dt.total_seconds() / 3600)

        # maxprevious['days']=abs((pd.to_datetime(filingdate)-pd.to_datetime(maxprevious['filingDate'])).dt.days)
        # maxprevious.loc[:,'hours'] = abs((pd.to_datetime(filingdate) - pd.to_datetime(maxprevious['filingDate'])).dt.total_seconds() / 3600)
        
        # print(current)
        # print(maxprevious)

        

        merged_df=pd.merge(current,maxprevious,on=['dataItemId','peo'],how='inner')
        
        # print(merged_df)


        dataItemIds=[]
        previousdate=[]
        tid_x=[]
        tid_y=[]
        diff=[]
        perc=[]
        peos=[]        
        for ind, row in merged_df.iterrows():
            if ((execute_operator(row['hours'],float(Threshold[0]),operator[0]))&(execute_operator(row['value_x'],row['value_y'],operator[1]))):  #&(execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1]))
                tid_x.append(row['tradingItemId_x']) 
                tid_y.append(row['tradingItemId_y'])
                diff='NA'
                perc='NA'
                dataItemIds.append(row['dataItemId']) 
                previousdate.append(row['filingDate'])
                peos.append(row['peo'])
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,'filingDate':previousdate,"diff":diff,"perc":perc})
        
        # print(diff_df)
        

        if len(diff_df)>0:

            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['tradingItemId'].isin(tid_x))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName']].drop_duplicates()
            
            temp1_revised=temp1.dropna()
           
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(historicalData_parsed['peo'].isin(diff_df['peo']))&(historicalData_parsed['tradingItemId'].isin(tid_y))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['researchContributorId']==contributor))] [['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp2_revised=temp2.dropna() 
            
    
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Two revisions on the same day"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": row['peo'],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Two revisions on the same day"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo":'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
           
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors  
    
#Estimates Error Checks 
@add_method(Validation) 
def EST_59(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold') 
    try:

        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']
        
        # print(filingdate)

        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','value','value_scaled','peo','currency','tradingItemId','parentFlag']].drop_duplicates()
        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','value','value_scaled','peo','currency','tradingItemId','parentFlag','filingDate']].drop_duplicates()

        current['peo']=current['peo'].fillna('NP: 1234')
        previous['peo']=previous['peo'].fillna('NP: 1234')        

        maxprevious1=previous.groupby(['dataItemId'])['filingDate'].max().reset_index()
        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
        
        maxprevious = maxprevious.copy()
        # maxprevious['hours'] = abs(((pd.to_datetime('filingdate')) - (pd.to_datetime(maxprevious['filingDate']))).dt.total_seconds() / 3600)
        maxprevious.loc[:,'hours'] = abs((pd.to_datetime(filingdate) - pd.to_datetime(maxprevious['filingDate'])).dt.total_seconds() / 3600)

        # maxprevious['days']=abs((pd.to_datetime(filingdate)-pd.to_datetime(maxprevious['filingDate'])).dt.days)
        # maxprevious.loc[:,'hours'] = abs((pd.to_datetime(filingdate) - pd.to_datetime(maxprevious['filingDate'])).dt.total_seconds() / 3600)
        
        # print(current)
        # print(maxprevious)

        

        merged_df=pd.merge(current,maxprevious,on=['dataItemId','peo','tradingItemId','parentFlag'],how='inner')
        
        # print(merged_df)


        dataItemIds=[]
        previousdate=[]
        tid=[]
        # tid_x=[]
        # tid_y=[]
        diff=[]
        perc=[]
        peos=[]        
        for ind, row in merged_df.iterrows():
            if ((execute_operator(row['hours'],float(Threshold[0]),operator[0]))&(execute_operator(row['value_x'],row['value_y'],operator[1]))):  #&(execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1]))
                tid.append(row['tradingItemId']) 
                # tid_y.append(row['tradingItemId_y'])
                diff='NA'
                perc='NA'
                dataItemIds.append(row['dataItemId']) 
                previousdate.append(row['filingDate'])
                peos.append(row['peo'])
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,'filingDate':previousdate,"diff":diff,"perc":perc})
        
        # print(diff_df)
        

        if len(diff_df)>0:

            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['tradingItemId'].isin(tid))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName']].drop_duplicates()
            
            temp1_revised=temp1.dropna()
           
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(diff_df['dataItemId']))&(historicalData_parsed['peo'].isin(diff_df['peo']))&(historicalData_parsed['tradingItemId'].isin(tid))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['researchContributorId']==contributor))] [['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','scale','currency','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp2_revised=temp2.dropna() 
            
    
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Multiple Target price value collected for the same Trading item from the same contributor on the same date"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": row['peo'],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Multiple Target price value collected for the same Trading item from the same contributor on the same date"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo":'NA',"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":'NA', "diff": diff_df[diff_df['dataItemId']==row["dataItemId"]]['diff'].iloc[0], "percent": diff_df[diff_df['dataItemId']==row["dataItemId"]]['perc'].iloc[0]}]
                errors.append(result)
           
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors  

#Estimates Error Checks 
@add_method(Validation) 
def EST_67(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #[Tags]
    operator = get_dataItemIds_list('Operation', parameters) #["<"]    
    try:
        latestactual = pd.to_datetime(filingMetadata['metadata']['latestActualizedPeo']).date()
        filingdate=filingMetadata['metadata']['filingDate']
        contributor=filingMetadata['metadata']['researchContributorId']
        companyid=filingMetadata['metadata']['companyId']        

        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()) )][['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['researchContributorId']==contributor)&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['filingDate']<filingdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate','periodEndDate']].drop_duplicates()


        maxprevious1=previous.groupby(['dataItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])['filingDate'].max().reset_index()

        maxprevious=previous[(previous['filingDate'].isin(maxprevious1['filingDate']))]
        
        maxprevious['periodEndDate']=pd.to_datetime(maxprevious['periodEndDate']).dt.date
        
        current['didpeocomb']=current.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        maxprevious['didpeocomb']=maxprevious.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        
      
        temp_df=maxprevious[~((maxprevious['didpeocomb'].isin(current['didpeocomb']))&(maxprevious['parentFlag'].isin(current['parentFlag']))
                        &(maxprevious['accountingStandardDesc'].isin(current['accountingStandardDesc']))&(maxprevious['fiscalChainSeriesId'].isin(current['fiscalChainSeriesId'])))]
        

        dataItemIds=[]
        previousdate=[]
        parentflag=[]
        peo=[]
        AS=[]
        fyc=[]
        diff=[]
        perc=[]      

        if temp_df is not None:    
            for ind, row in temp_df.iterrows():
                if execute_operator(latestactual, row['periodEndDate'], operator[0]):
                    peo.append(row['peo'])
                    dataItemIds.append(row['dataItemId'])
                    previousdate.append(row['filingDate'])
                    parentflag.append(row['parentFlag'])
                    AS.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'  
                    
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peo,"filingDate":previousdate,"diff":diff,"perc":perc})

        if len(diff_df)>0:
            diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
       
            temp1 = historicalData_parsed[((historicalData_parsed['peocomb'].isin(diff_df['peocomb'])) &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(AS))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['filingDate'].isin(diff_df['filingDate'])))] [['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
    
    
            temp1_new=temp1.dropna() 
    
        
            for ind, row in temp1_new.iterrows():
    
                result = {"highlights": [], "error": "Same Sign/Missing Tag"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"],  "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors    

#Estimates Error Checks 
@add_method(Validation) 
def EST_6B(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag1_list=get_dataItemIds_list('TAG1', parameters) # NI GAAP
    tag2_list=get_dataItemIds_list('TAG2', parameters) # NI Normalized
    tag3_list=get_dataItemIds_list('TAG3', parameters) # EPS GAAP
    tag4_list=get_dataItemIds_list('TAG4', parameters) # EPS Normalized
    tag5_list=get_dataItemIds_list('LHSdataItemIds', parameters) # EBT GAAP
    tag6_list=get_dataItemIds_list('RHSdataItemIds', parameters) # EBT Normalized
    operator = get_dataItemIds_list('Operation', parameters) #["=="],["!="] 

    try:

        filingdate = filingMetadata['metadata']['filingDate']
        contributor = filingMetadata['metadata']['researchContributorId']

        current_ni_gaap = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag1_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value','value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]

        current_ni_normalized = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag2_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]

        current_eps_gaap = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag3_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value','value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]

        current_eps_normalized = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag4_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]

        current_ebt_gaap = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag5_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value','value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]

        current_ebt_normalized = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag6_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']]


        previous_ni_gaap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_ni_gaap_group = previous_ni_gaap.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_ni_gaap = previous_ni_gaap[(previous_ni_gaap['filingDate'].isin(max_previous_ni_gaap_group['filingDate']))]
        


        previous_ni_normalized = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag2_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_ni_normalized_group = previous_ni_normalized.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_ni_normalized = previous_ni_normalized[(previous_ni_normalized['filingDate'].isin(max_previous_ni_normalized_group['filingDate']))]



        previous_eps_gaap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag3_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_eps_gaap_group = previous_eps_gaap.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_eps_gaap = previous_eps_gaap[(previous_eps_gaap['filingDate'].isin(max_previous_eps_gaap_group['filingDate']))]
        

        previous_eps_normalized = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag4_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_eps_normalized_group = previous_eps_normalized.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_eps_normalized = previous_eps_normalized[(previous_eps_normalized['filingDate'].isin(max_previous_eps_normalized_group['filingDate']))]

    

        previous_ebt_gaap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag5_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_ebt_gaap_group = previous_ebt_gaap.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_ebt_gaap = previous_ebt_gaap[(previous_ebt_gaap['filingDate'].isin(max_previous_ebt_gaap_group['filingDate']))]
        

        previous_ebt_normalized = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag6_list)) & (historicalData_parsed['researchContributorId'] == contributor) & (
                                                      historicalData_parsed['filingDate'] < filingdate) & (
                                                      historicalData_parsed['value'] != "") & (historicalData_parsed['value'].notnull()))][['dataItemId', 'peo','value', 'value_scaled', 'currency', 'parentFlag', 'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId', 'filingDate']]
                                 
        max_previous_ebt_normalized_group = previous_ebt_normalized.groupby(
            ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId'])[
            'filingDate'].max().reset_index()

        max_previous_ebt_normalized = previous_ebt_normalized[(previous_ebt_normalized['filingDate'].isin(max_previous_ebt_normalized_group['filingDate']))]
     

        
        merged_df_ni_current = pd.merge(current_ni_gaap, current_ni_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')

        merged_df_ni_previous = pd.merge(max_previous_ni_gaap, max_previous_ni_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')
        
        merged_df_ni=pd.merge(merged_df_ni_current,merged_df_ni_previous,on=['dataItemId_x','dataItemId_y','peo','parentFlag', 
                                                                       'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId'],how='inner')
       

        merged_df_eps_current = pd.merge(current_eps_gaap, current_eps_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')

        merged_df_eps_previous = pd.merge(max_previous_eps_gaap, max_previous_eps_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')
        
        merged_df_eps=pd.merge(merged_df_eps_current,merged_df_eps_previous,on=['dataItemId_x','dataItemId_y','peo','parentFlag', 
                                                                       'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId'],how='inner')
        
        merged_df_ebt_current = pd.merge(current_ebt_gaap, current_ebt_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')

        merged_df_ebt_previous = pd.merge(max_previous_ebt_gaap, max_previous_ebt_normalized,
                             on=[ 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                                 'fiscalChainSeriesId'], how='inner')
        
        merged_df_ebt=pd.merge(merged_df_ebt_current,merged_df_ebt_previous,on=['dataItemId_x','dataItemId_y','peo','parentFlag', 
                                                                       'accountingStandardDesc', 'tradingItemId','fiscalChainSeriesId'],how='inner')        

        dataItemId1 = []
        dataItemId2 = []
        previousdate = []
        parentflag = []
        tid = []
        accounting = []
        fyc = []
        peos = []
        diff = []
        perc = []
        for ind, row in merged_df_ni.iterrows():
            if ((execute_operator(row['value_scaled_x_x'], row['value_scaled_y_x'], operator[0])) 
                & (execute_operator(row['value_scaled_x_x'], row['value_scaled_y_y'], operator[1]))
                &(execute_operator(row['value_scaled_y_x'], row['value_scaled_x_y'], operator[1]))):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                dataItemId1.append(row['dataItemId_x'])
                dataItemId2.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_y'])


        for ind, row in merged_df_eps.iterrows():
            if ((execute_operator(row['value_scaled_x_x'], row['value_scaled_y_x'], operator[0])) 
                & (execute_operator(row['value_scaled_x_x'], row['value_scaled_y_y'], operator[1]))
                &(execute_operator(row['value_scaled_y_x'], row['value_scaled_x_y'], operator[1]))):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                dataItemId1.append(row['dataItemId_x'])
                dataItemId2.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_y'])
                

        
        for ind, row in merged_df_ebt.iterrows():
            if ((execute_operator(row['value_scaled_x_x'], row['value_scaled_y_x'], operator[0])) 
                & (execute_operator(row['value_scaled_x_x'], row['value_scaled_y_y'], operator[1]))
                &(execute_operator(row['value_scaled_y_x'], row['value_scaled_x_y'], operator[1]))):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                dataItemId1.append(row['dataItemId_x'])
                dataItemId2.append(row['dataItemId_y'])
                previousdate.append(row['filingDate_y'])
                
        diff_df = pd.DataFrame({"filingDate_y": previousdate,"dataItemId_x": dataItemId1,"dataItemId_y": dataItemId2, "peo": peos, "diff": diff, "perc": perc}) 

        if len(diff_df) > 0:
            diff_df['peocomb1'] = diff_df.apply(lambda x: '%s%s' % (x['dataItemId_x'], x['peo']), axis=1)
            diff_df['peocomb2'] = diff_df.apply(lambda x: '%s%s' % (x['dataItemId_y'], x['peo']), axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed.apply(lambda x: '%s%s' % (x['dataItemId'], x['peo']), axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed.apply(lambda x: '%s%s' % (x['dataItemId'], x['peo']), axis=1)

            temp1 = extractedData_parsed[(((extractedData_parsed['peocomb'].isin(diff_df['peocomb1']))|
                                          (extractedData_parsed['peocomb'].isin(diff_df['peocomb2'])))& (
                extractedData_parsed['parentFlag'].isin(parentflag)) & (
                                              extractedData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                              extractedData_parsed['tradingItemId'].isin(tid)) & (
                                              extractedData_parsed['fiscalChainSeriesId'].isin(fyc)) & (
                                                      extractedData_parsed['value'] != "") & (
                                              extractedData_parsed['value'].notnull()))][
                ['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc',
                 'tradingItemId', 'fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear',
                 'fiscalQuarter', 'tradingItemName']]
            temp1=temp1.drop_duplicates()                                      
            temp1_revised = temp1.dropna()

            temp2 = historicalData_parsed[(((historicalData_parsed['peocomb'].isin(diff_df['peocomb1']))|
                                           (historicalData_parsed['peocomb'].isin(diff_df['peocomb2'])))& (
                historicalData_parsed['parentFlag'].isin(parentflag)) & (
                                               historicalData_parsed['accountingStandardDesc'].isin(accounting)) & (
                                               historicalData_parsed['tradingItemId'].isin(tid)) & (
                                               historicalData_parsed['fiscalChainSeriesId'].isin(fyc)) & (
                                                       historicalData_parsed['value'] != "") & (
                                               historicalData_parsed['value'].notnull()) & (
                                               historicalData_parsed['filingDate'].isin(previousdate)) & (
                                                       historicalData_parsed['researchContributorId'] == contributor))][
                ['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag', 'accountingStandardDesc',
                 'tradingItemId', 'fiscalChainSeriesId', 'scale', 'currency', 'team', 'description', 'fiscalYear',
                 'fiscalQuarter', 'tradingItemName',  'versionId', 'companyId', 'feedFileId', 'filingDate']]
                                                           
            temp2=temp2.drop_duplicates()                                               
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],
                          "error": "Current Vs Previous doc where collected flavor is NORM=GAAP and GAAP=NORM"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                             "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'],
                                                      "currency": row['currency']}, "section": row['team'],
                                             "filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                               "description": row["description"],
                                               "versionId": filingMetadata['metadata']['versionId'],
                                               "companyid": filingMetadata['metadata']['companyId'],
                                               "feedFileId": filingMetadata['metadata']['feedFileId'],
                                               "peo": row["peo"],
                                               "diff": 'NA',
                                               "percent": 'NA'}
                result["checkGeneratedForList"] = [
                    {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                     "fiscalYear": row["fiscalYear"], "fiscalQuarter": row["fiscalQuarter"], "peo": row["peo"],
                     "value": row["value"], "units": row["scale"], "currency": row["currency"],
                     "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                     "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                     "diff": 'NA',
                     "percent": 'NA'}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Current Vs Previous doc where collected flavor is NORM=GAAP and GAAP=NORM"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},
                                             "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'],
                                                      "currency": row['currency']}, "section": row['team'],
                                             "filingId": row['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'],
                                               "description": row["description"], "versionId": row['versionId'],
                                               "companyid": row['companyId'], "feedFileId": row['feedFileId'],
                                               "peo": row["peo"],
                                               "diff": 'NA',
                                               "percent": 'NA'}
                result["checkGeneratedForList"] = [
                    {"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],
                     "fiscalYear": row["fiscalYear"], "fiscalQuarter": row["fiscalQuarter"], "peo": row["peo"],
                     "value": row["value"], "units": row["scale"], "currency": row["currency"],
                     "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],
                     "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],
                     "refFilingId": row["versionId"], "refFilingDate": row["filingDate"],
                     "estimatePeriodId": row["estimatePeriodId"],
                     "diff":'NA',
                     "percent": 'NA'}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def EST_35(historicalData,filingMetadata,extractedData,parameters):
    #JSON 20
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # tags
    operator = get_dataItemIds_list('Operation', parameters)
    try:
        documentdate=filingMetadata['metadata']['filingDate']

        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(dataItemId_list)))&(extractedData_parsed['value'] != "") &(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!= "")&(extractedData_parsed['peo'].notnull()))][['dataItemId', 'peo', 'fiscalChainSeriesId']].drop_duplicates()
        
        temp['companyId'] = filingMetadata['metadata']['companyId']
        
        print(temp)

        previous = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(dataItemId_list))&(historicalData_parsed['value'] != "")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId'].isin(temp['companyId']))&(historicalData_parsed['peo']!="")&(historicalData_parsed['filingDate'] < documentdate)&(historicalData_parsed['peo'].notnull()))][['dataItemId', 'peo','fiscalChainSeriesId', 'filingDate','companyId']].drop_duplicates()
        
        print(previous)
        newfyc=temp[~((temp['fiscalChainSeriesId']).isin(previous['fiscalChainSeriesId']))]
        
        # print(newfyc)
        
        maxprevious = previous.groupby(['companyId','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        
        # print(maxprevious)

        previous = previous[previous['filingDate'].isin(maxprevious['filingDate'])]


        merged_df = pd.merge(temp, previous, on=['companyId'], how='inner')
        
        print(merged_df)

        filingdate = []
        diff = []
        perc = []
        series1 = []
        series2 = []
        
        for ind, row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'], row['fiscalChainSeriesId_y'], operator[0]):
                filingdate.append(row['filingDate'])
                difference = 'NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc = 'NA'
                

        diff_df = pd.DataFrame({"diff": diff, "perc": perc, "filingDate": filingdate, "curseries": series1, "preseries": series2}).drop_duplicates()


        temp1 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(series1))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()

        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate'])) & (historicalData_parsed['fiscalChainSeriesId'].isin(series2)) )][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team', 'versionId', 'feedFileId','filingDate', 'companyId']].drop_duplicates()

        if len(newfyc)>0:
            temp3 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(newfyc['fiscalChainSeriesId']))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
            
            # print(temp3)
            
            temp3_revised=temp3.dropna()
            
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],"error": "New fiscal Year series collected for this company"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

        if len(temp1) > 0 and len(temp2) > 0:
            temp1_revised = temp1.dropna()
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],"error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)

            temp2_revised = temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)


        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors   

#Estimates Error Checks 
@add_method(Validation) 
def AG_99A(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actuals 20
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #dt1, dt2, dt3
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #dt4
    operator = get_dataItemIds_list('Operation', parameters) #["!="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")& (extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #dt1, dt2, dt3
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")& (extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #dt1, dt2, dt3
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                # lhs_df=lhs_df.append(lhs_df_missing_data,ignore_index=True)
                lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
       
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                # rhs_df=rhs_df.append(rhs_df_missing_data,ignore_index=True)
                rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
        

        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #dt1 + dt2 + dt3

        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #dt4

        # print(lhs_df)
        # print(rhs_df)            
        

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        # print(merged_df)

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[] 

        # if merged_df is not None:        
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])               
                difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(difference)
                perc.append((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100)
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
    
        temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))) 
                |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))))].drop_duplicates()

        temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (historicalData_parsed['peo'].isin(peos))&(historicalData_parsed['tradingItemId'].isin(tid))
                &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(accounting))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc)))  
                |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peo'].isin(peos))&(historicalData_parsed['tradingItemId'].isin(tid))
                &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(accounting))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc))))].drop_duplicates()

        temp1_revised=temp1.dropna()
        temp2_revised=temp2.dropna()
        
        for ind, row in temp1_revised.iterrows():
            result = {"highlights": [], "error": "Sum of three dataitems not equal to Fourth dataitem"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
            errors.append(result)
        for ind, row in temp2_revised.iterrows():
            result = {"highlights": [], "error": "Sum of three dataitems not equal to Fourth dataitem"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']}})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
            errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_331(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actuals 20
    """
    This check should fire when some pre-defined arithmetic relation between Parent and Consolidated data items fails

    LHSdataItemIds-(22315,22316,22318,22319,22320,22322,22323,22325,22326,22327,22329,22330,22334,22335,22338,22339,22340,22341,22363,22364,22365,22368,22370,22383,22384,22386,22387,22388,22390,22391,22393,22394,22395,22397,22398,22402,22403,22406,22407,22408,22409,22431,22432,22433,22436,22437,22443,22444,22446,22447,22448,22450,22451,22453,22454,22455,22457,22458,22462,22463,22466,22467,22468,22469,22491,22492,22493,22496,22497,22504,22505,22506,32490,32491,32492,100179,100186,100193,100200,100207,100214,100221,100235,100249,100256,100270,100284,104061,104082,104096,104103,104117,104124,105068,105075,105082,105103,105114,105125,105299,105307,105321,112028,114154,114161,114162,114163,114165,114172,114173,114174,114176,114183,114184,114185,114187,114194,114195,114196,114198,114205,114206,114207,114220,114227,114228,114229,114230,114231,114232,114233,114234,114235,115403,115406,115407,115408,115416,115419,115420,115421,115429,115432,115433,115434,115442,115445,115446,115447,115455,115458,115459,115460,115468,115471,115472,115473,115481,115484,115485,115486,115494,115497,115498,115499,115507,115510,115511,115512,115520,115523,115524,115525,115533,115536,115537,115538,115591,115594,115595,115596,600289,600290,600291,600292,600294,600295,600296,600298,600299,600300,600301,600302,600305,600306,600307,600308,600309,600310,600311,600312,600313,600315,600316,600317,600318,600319,600320,600322,600323,600324,600329,600336,600341,600342,600343,600344,600346,600347,600348,600350,600351,600352,600353,600354,600357,600358,600359,600360,600361,600362,600363,600364,600365,600367,600368,600369,600370,600371,600372,600374,600375,600376,600381,600388,600393,600394,600395,600396,600398,600399,600400,600402,600403,600404,600405,600406,600409,600410,600411,600412,600413,600414,600415,600416,600417,600419,600420,600421,600422,600423,600424,600426,600427,600428,600433,600440,600445,600446,600447,600448,600450,600451,600452,600454,600455,600456,600457,600458,600461,600462,600463,600464,600465,600466,600467,600468,600469,600471,600472,600473,600474,600475,600476,600478,600479,600480,600485,600492,601370,601377,601378,601379,601383,601390,601391,601392,601435,601442,601443,601444,601448,601455,601456,601457,602731,602732,602733,602734,602744,602745,602746,602747,602757,602758,602759,602760,602770,602771,602772,602773,602822,602823,602824,602825,602848,602849,602850,602851,602861,602862,602863,602864,602874,602875,602876,602877,602900,602901,602902,602903,602913,602914,602915,602916,602926,602927,602928,602929,602939,602940,602941,602942,602952,602953,602954,602955,602978,602979,602980,602981,602991,602992,602993,602994,603004,603005,603006,603007,603017,603018,603019,603020,603043,603044,603045,603046,603069,603070,603071,603072,603082,603083,603084,603085,603108,603109,603110,603111,603121,603122,603123,603124,603134,603135,603136,603137,603147,603148,603149,603150,603160,603161,603162,603163,603173,603174,603175,603176,603186,603187,603188,603189,603199,603200,603201,603202,603212,603213,603214,603215,603238,603239,603240,603241,603251,603252,603253,603254,603264,603265,603266,603267,603290,603291,603292,603293,603303,603304,603305,603306,603316,603317,603318,603319,603329,603330,603331,603332,603342,603343,603344,603345,603355,603356,603357,603358,603368,603369,603370,603371,603381,603382,603383,603384,603394,603395,603396,603397,603407,603408,603409,603410,603433,603434,603435,603436,603446,603447,603448,603449,603459,603460,603461,603462,604161,604162,604163,604164,604174,604175,604176,604177,604213,604214,604215,604216,604252,604253,604254,604255)
    Operation-"!="

    """    
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # DataitemId
    operator = get_dataItemIds_list('Operation', parameters) # ["!="]
   
    if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'value', 'peo'}.issubset(extractedData_parsed.columns))):
        
        companyid=filingMetadata['metadata']['companyId']

        Consolidated = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['parentFlag']=="Consolidated")][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
        parent = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['parentFlag']=="Parent")][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
        
        # print(Consolidated)
        # print(parent)

        if len(Consolidated)==0:
            Consolidated = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Consolidated")][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
        if Consolidated["dataItemId"].nunique()!=len(dataItemId_list):
            not_captured= [x for x in dataItemId_list if x not in set(Consolidated["dataItemId"])]
            Consolidated_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Consolidated"))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
            Consolidated = pd.concat([Consolidated,Consolidated_not_cap])
        extracted_tag_peo_count=extractedData_parsed['peo'].nunique()
        Consolidated_peo_count=Consolidated.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (Consolidated_peo_count['peocount']<extracted_tag_peo_count).any():
            missed_peo_tag=Consolidated_peo_count[(Consolidated_peo_count['peocount']<extracted_tag_peo_count)]['dataItemId']
            collected_peo=Consolidated[Consolidated['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            Consolidated_missing_data=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Consolidated") & ~(historicalData_parsed['peo'].isin(collected_peo['peo'])))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
            Consolidated=pd.concat([Consolidated,Consolidated_missing_data],ignore_index=True)


        if len(parent)==0:
            parent = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Parent")][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
        if parent["dataItemId"].nunique()!=len(dataItemId_list):            
            not_captured= [x for x in dataItemId_list if x not in set(parent["dataItemId"])]
            parent_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Parent"))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']] 
            parent = pd.concat([parent,parent_not_cap])
        extracted_tag_peo_count=extractedData_parsed['peo'].nunique()
        parent_peo_count=parent.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (parent_peo_count['peocount']<extracted_tag_peo_count).any():
            missed_peo_tag=parent_peo_count[(parent_peo_count['peocount']<extracted_tag_peo_count)]['dataItemId']
            collected_peo=parent[parent['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            parent_missing_data=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Parent")& ~(historicalData_parsed['peo'].isin(collected_peo['peo'])))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
            parent=pd.concat([parent,parent_missing_data],ignore_index=True)
       


        if len(Consolidated) > 0 and len(parent) > 0:
            base_currency = Consolidated.currency.mode()[0]
            
            if 'currency' in Consolidated.columns:
                Consolidated.loc[Consolidated['currency'] != 'NA', "value_scaled"] = Consolidated.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']),axis=1)
                Consolidated.loc[Consolidated['currency'] == 'NA', "value_scaled"] = Consolidated["value"]
            
            if 'currency' in parent.columns:
                parent.loc[parent['currency'] != 'NA', "value_scaled"] = parent.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']),axis=1)
                parent.loc[parent['currency'] == 'NA', "value_scaled"] = parent["value"]


        # print(Consolidated.dtypes)
        # print(parent.dtypes)
        
        merged_df=pd.merge(Consolidated,parent,on=['dataItemId','peo','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        
        # print(merged_df.dtypes)


        
        tags=[]
        peos=[]
        accounting=[]
        tid=[]
        fyc=[]        
        diff=[]
        perc=[]
        
        for ind,row in merged_df.iterrows():
            if (execute_operator(row['parentFlag_x'],row['parentFlag_y'],operator[0])) & (execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1])):
                tags.append(row['dataItemId'])
                peos.append(row['peo'])   
                tid.append(row['tradingItemId'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                difference='NA'
                diff.append((difference))
                perc='NA'
        
        diff_df=pd.DataFrame({"dataItemId":tags,"peo":peos,"tradingItemId":tid,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        # print(diff_df)
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[((historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','filingDate','versionId','companyId','feedFileId','peocomb']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            # print(temp1_revised)
            # print(temp2_revised)
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Parent and Consolidated values are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Parent and Consolidated values are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'], "filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result) 
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors 

#Estimates Error Checks 
@add_method(Validation) 
def AG_332(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actuals 20
    """
    This check should fire when some pre-defined arithmetic relation between Parent and Consolidated data items fails when japan & korean companies provided data available

    LHSdataItemIds-(22315,22316,22318,22319,22320,22322,22323,22325,22326,22327,22329,22330,22334,22335,22338,22339,22340,22341,22363,22364,22365,22368,22370,22383,22384,22386,22387,22388,22390,22391,22393,22394,22395,22397,22398,22402,22403,22406,22407,22408,22409,22431,22432,22433,22436,22437,22443,22444,22446,22447,22448,22450,22451,22453,22454,22455,22457,22458,22462,22463,22466,22467,22468,22469,22491,22492,22493,22496,22497,22504,22505,22506,32490,32491,32492,100179,100186,100193,100200,100207,100214,100221,100235,100249,100256,100270,100284,104061,104082,104096,104103,104117,104124,105068,105075,105082,105103,105114,105125,105299,105307,105321,112028,114154,114161,114162,114163,114165,114172,114173,114174,114176,114183,114184,114185,114187,114194,114195,114196,114198,114205,114206,114207,114220,114227,114228,114229,114230,114231,114232,114233,114234,114235,115403,115406,115407,115408,115416,115419,115420,115421,115429,115432,115433,115434,115442,115445,115446,115447,115455,115458,115459,115460,115468,115471,115472,115473,115481,115484,115485,115486,115494,115497,115498,115499,115507,115510,115511,115512,115520,115523,115524,115525,115533,115536,115537,115538,115591,115594,115595,115596,600289,600290,600291,600292,600294,600295,600296,600298,600299,600300,600301,600302,600305,600306,600307,600308,600309,600310,600311,600312,600313,600315,600316,600317,600318,600319,600320,600322,600323,600324,600329,600336,600341,600342,600343,600344,600346,600347,600348,600350,600351,600352,600353,600354,600357,600358,600359,600360,600361,600362,600363,600364,600365,600367,600368,600369,600370,600371,600372,600374,600375,600376,600381,600388,600393,600394,600395,600396,600398,600399,600400,600402,600403,600404,600405,600406,600409,600410,600411,600412,600413,600414,600415,600416,600417,600419,600420,600421,600422,600423,600424,600426,600427,600428,600433,600440,600445,600446,600447,600448,600450,600451,600452,600454,600455,600456,600457,600458,600461,600462,600463,600464,600465,600466,600467,600468,600469,600471,600472,600473,600474,600475,600476,600478,600479,600480,600485,600492,601370,601377,601378,601379,601383,601390,601391,601392,601435,601442,601443,601444,601448,601455,601456,601457,602731,602732,602733,602734,602744,602745,602746,602747,602757,602758,602759,602760,602770,602771,602772,602773,602822,602823,602824,602825,602848,602849,602850,602851,602861,602862,602863,602864,602874,602875,602876,602877,602900,602901,602902,602903,602913,602914,602915,602916,602926,602927,602928,602929,602939,602940,602941,602942,602952,602953,602954,602955,602978,602979,602980,602981,602991,602992,602993,602994,603004,603005,603006,603007,603017,603018,603019,603020,603043,603044,603045,603046,603069,603070,603071,603072,603082,603083,603084,603085,603108,603109,603110,603111,603121,603122,603123,603124,603134,603135,603136,603137,603147,603148,603149,603150,603160,603161,603162,603163,603173,603174,603175,603176,603186,603187,603188,603189,603199,603200,603201,603202,603212,603213,603214,603215,603238,603239,603240,603241,603251,603252,603253,603254,603264,603265,603266,603267,603290,603291,603292,603293,603303,603304,603305,603306,603316,603317,603318,603319,603329,603330,603331,603332,603342,603343,603344,603345,603355,603356,603357,603358,603368,603369,603370,603371,603381,603382,603383,603384,603394,603395,603396,603397,603407,603408,603409,603410,603433,603434,603435,603436,603446,603447,603448,603449,603459,603460,603461,603462,604161,604162,604163,604164,604174,604175,604176,604177,604213,604214,604215,604216,604252,604253,604254,604255)
    Operation-"!="
    COUNTRY_INCLUDE- "Korean and Japan Companies"

    """

    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # DataitemId
    operator = get_dataItemIds_list('Operation', parameters) # ["<"]
    country=get_dataItemIds_list('COUNTRY_INCLUDE',parameters) # Korean and Japan Companies
   
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):       
        if filingMetadata['metadata']['country'] in country: 
            
            companyid=filingMetadata['metadata']['companyId']

            Consolidated = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['parentFlag']=="Consolidated")][['dataItemId','value','peo','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
            parent = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['parentFlag']=="Parent")][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
            
            # print(Consolidated)
            # print(parent)

            if len(Consolidated)==0:
                Consolidated = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['parentFlag']=="Consolidated")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
            if Consolidated["dataItemId"].nunique()!=len(dataItemId_list):            
                not_captured= [x for x in dataItemId_list if x not in set(Consolidated["dataItemId"])]
                Consolidated_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['parentFlag']=="Consolidated")&(historicalData_parsed['companyId']==companyid))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
                Consolidated = pd.concat([Consolidated,Consolidated_not_cap])
            extracted_tag_peo_count=extractedData_parsed['peo'].nunique()
            Consolidated_peo_count=Consolidated.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
            if (Consolidated_peo_count['peocount']<extracted_tag_peo_count).any():
                missed_peo_tag=Consolidated_peo_count[(Consolidated_peo_count['peocount']<extracted_tag_peo_count)]['dataItemId']
                collected_peo=Consolidated[Consolidated['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                Consolidated_missing_data=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Consolidated") & ~(historicalData_parsed['peo'].isin(collected_peo['peo'])))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
                Consolidated=pd.concat([Consolidated,Consolidated_missing_data],ignore_index=True)


            if len(parent)==0:
                parent = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['parentFlag']=="Parent")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
            if parent["dataItemId"].nunique()!=len(dataItemId_list):            
                not_captured= [x for x in dataItemId_list if x not in set(parent["dataItemId"])]
                parent_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['parentFlag']=="Parent")&(historicalData_parsed['companyId']==companyid))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']] 
                parent = pd.concat([parent,parent_not_cap])
            extracted_tag_peo_count=extractedData_parsed['peo'].nunique()
            parent_peo_count=parent.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
            if (parent_peo_count['peocount']<extracted_tag_peo_count).any():
                missed_peo_tag=parent_peo_count[(parent_peo_count['peocount']<extracted_tag_peo_count)]['dataItemId']
                collected_peo=parent[parent['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                parent_missing_data=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['parentFlag']=="Parent")& ~(historicalData_parsed['peo'].isin(collected_peo['peo'])))][['dataItemId','peo','value','parentFlag','value_scaled','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']]
                parent=pd.concat([parent,parent_missing_data],ignore_index=True)
        

            if len(Consolidated) > 0 and len(parent) > 0:
                base_currency = Consolidated.currency.mode()[0]
                
                if 'currency' in Consolidated.columns:
                    Consolidated.loc[Consolidated['currency'] != 'NA', "value_scaled"] = Consolidated.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']),axis=1)
                    Consolidated.loc[Consolidated['currency'] == 'NA', "value_scaled"] = Consolidated["value"]
                
                if 'currency' in parent.columns:
                    parent.loc[parent['currency'] != 'NA', "value_scaled"] = parent.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']),axis=1)
                    parent.loc[parent['currency'] == 'NA', "value_scaled"] = parent["value"]
            
            merged_df=pd.merge(Consolidated,parent,on=['dataItemId','peo','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
            
            # print(merged_df)

            
            tags=[]
            peos=[]
            accounting=[]
            tid=[]
            fyc=[]        
            diff=[]
            perc=[]
            
            for ind,row in merged_df.iterrows():
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    tags.append(row['dataItemId'])
                    peos.append(row['peo'])   
                    tid.append(row['tradingItemId'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    difference='NA'
                    diff.append((difference))
                    perc='NA'
            
            diff_df=pd.DataFrame({"dataItemId":tags,"peo":peos,"tradingItemId":tid,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','peo','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")
                                              &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                temp2 = historicalData_parsed[((historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")
                                               &(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','filingDate','versionId','companyId','feedFileId','peocomb']].drop_duplicates()

                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()
                
                # print(temp1_revised)
                # print(temp2_revised)
                
                for ind, row in temp1_revised.iterrows():
                        result = {"highlights": [], "error": "Consolidated data less than Parent data"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                        errors.append(result)
                for ind, row in temp2_revised.iterrows():
                        result = {"highlights": [], "error": "Consolidated data less than Parent data"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'], "filingDate": row['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                        errors.append(result) 
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors 

#Estimates Error Checks 
@add_method(Validation) 
def AG_333(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EPS GAAp and EPS Normalized
    try:
        primaryeps=filingMetadata['metadata']['primaryEpsFlag']
        
        # print(primaryeps)

        primaryflag=[]
        
        Normalized_to_check = ["22315", "22383", "22443"]
       
        if primaryeps in Normalized_to_check:

            primaryflag="Normalized"

        else:
            primaryflag="GAAP"
            
        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','description']]
        
        # print(temp)

        if (primaryflag == "Normalized" and temp["dataItemId"].nunique() <= 3 and (not temp['description'].str.contains(primaryflag, na=False).any())) or (primaryflag == "GAAP" and temp["dataItemId"].nunique() <= 3 and (not temp['description'].str.contains(primaryflag, na=False).any())):
            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list)) & (extractedData_parsed['value'] != "") & (extractedData_parsed['value'].notnull()))]
            
            # print(temp1)

            if temp1 is not None and not temp1.empty:
                for ind, row in temp1.iterrows():
                        result = {"highlights": [], "error": "Guidance captured in other than primary flavor"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": "NA", "percent": "NA"}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                        errors.append(result)                    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_326(historicalData, filingMetadata, extractedData, parameters):
    """
    This check should fire when Guidance Initiation - at Company level

    """ 

    errors = []

    if ((extractedData_parsed is not None) and 
        (len(extractedData_parsed) > 0) and 
        ({'value', 'peo', 'dataItemFlag', 'team'}.issubset(extractedData_parsed.columns)) and 
        (historicalData_parsed is not None) and 
        (len(historicalData_parsed) > 0) and 
        ({'value', 'peo', 'dataItemFlag', 'team'}.issubset(historicalData_parsed.columns))):
        
        companyid = filingMetadata['metadata']['companyId']
        temp0 = extractedData_parsed[
            (extractedData_parsed['dataItemFlag'] == "G") & 
            (extractedData_parsed['value'] != "") & 
            (extractedData_parsed['value'].notnull())
        ][['dataItemFlag', 'team', 'accountingStandardDesc', 'parentFlag', 'fiscalChainSeriesId','fiscalYear']].drop_duplicates()
        
        # print(temp0)

        if len(temp0) > 0:
            temp1 = historicalData_parsed[
                ((historicalData_parsed['dataItemFlag'] == "G") | (historicalData_parsed['dataItemFlag'] == "A")) & 
                (historicalData_parsed['companyId'] == companyid) & 
                (historicalData_parsed['value'] != "") & (historicalData_parsed['peo'] != "") &
                (historicalData_parsed['value'].notnull()) & (historicalData_parsed['peo'].notnull())
            ][['dataItemFlag', 'team', 'accountingStandardDesc', 'parentFlag', 'fiscalChainSeriesId','fiscalYear']].drop_duplicates()
            
            # print(temp1)
            
            # final = temp0[~temp0['dataItemFlag'].isin(temp1['dataItemFlag'])]['dataItemFlag'].unique()
            final = temp0[~(temp0['dataItemFlag'].isin(temp1['dataItemFlag']))].drop_duplicates()
            
            final = pd.DataFrame(final)
            
            # print(final)
            
            if len(final) > 0:                                                                                                                                                                                       
                peos = []
                diff = []
                perc = []
                dataitemflag = []
                
                for ind, row in final.iterrows():
                    peos.append(row['fiscalYear'])               
                    difference = 'NA'
                    diff.append(difference)
                    perc.append('NA')
                    dataitemflag.append(row['dataItemFlag'])
                
                diff_df = pd.DataFrame({"peo": peos,"dataItemFlag":dataitemflag ,"diff": diff, "perc": perc}).drop_duplicates()
                
                # print(diff_df)
                
                if len(diff_df) > 0:
                    
                    temp3 = final.copy()
                                        
                    for ind, row in temp3.iterrows():
                        result = {
                            "highlights": [], 
                            "error": "Guidance Initiation - Company level"
                        }
                        result["highlights"].append({
                            "row": {"name": "NA", "id": "NA"}, 
                            "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},
                            "section": row['team'],
                            "filingId": filingMetadata['metadata']['versionId'], 
                            "versionId": filingMetadata['metadata']['versionId'], 
                            "filingDate": filingMetadata['metadata']['filingDate']
                        })
                        result["checkGeneratedFor"] = {
                            "statement": "", "tag": "NA", "description": "NA", 
                            "versionId": filingMetadata['metadata']['versionId'],
                            "companyid": filingMetadata['metadata']['companyId'], 
                            "feedFileId": filingMetadata['metadata']['feedFileId'], 
                            "peo": "NA", "diff": "NA", "percent": "NA"
                        }
                        result["checkGeneratedForList"] = [{
                            "tag": "NA", "description": "NA", "tradingItemId": "NA",
                            "fiscalYear": row['fiscalYear'], "fiscalQuarter": "NA", "peo": "NA",
                            "value": "NA", "units": "NA", "currency": "NA",
                            "tradingItemName": "NA", "accountingStdDesc": row["accountingStandardDesc"],
                            "parentConsolidatedFlag": row["parentFlag"],
                            "fiscalChainSeries": row["fiscalChainSeriesId"], 
                            "diff": "NA", "percent": "NA"
                        }]
                        errors.append(result)
    
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors


#Estimates Error Checks 
# @add_method(Validation)     
# def AG_326(historicalData,filingMetadata,extractedData,parameters):
#     """
#     This check should fire when Guidance Initiation - at Company level

#     """ 

#     errors = []
    
#     if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'value','peo','dataItemFlag','team'}.issubset(extractedData_parsed.columns)) and (historicalData_parsed is not None) and (len(historicalData_parsed) > 0) and ({'value','peo','dataItemFlag','team'}.issubset(historicalData_parsed.columns))):
        
#         companyid=filingMetadata['metadata']['companyId']
#         temp0 = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemFlag','value','peo','team','accountingStandardDesc','parentFlag','fiscalChainSeriesId']].drop_duplicates()

#         if len(temp0)>0:
#             temp1=historicalData_parsed[((historicalData_parsed['dataItemFlag']=="G")&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemFlag','value','peo','team','accountingStandardDesc','parentFlag','fiscalChainSeriesId']].drop_duplicates()

#             final=temp0[~(temp0['dataItemFlag'].isin(temp1['dataItemFlag']))].drop_duplicates()
            
#             # final = temp0[~temp0['dataItemFlag'].isin(temp1['dataItemFlag'])]['dataItemFlag'].unique()
            
#             # final1=final['dataItemFlag'].unique()
            
#             final = pd.DataFrame(final)
            
#             # print(final)

              
#             if len(final)>0:                                                                                                                                                                                       
#                 peos=[]
#                 diff=[]
#                 perc=[]

#                 for ind,row in final.iterrows():
#                     peos.append(row['peo'])               
#                     difference='NA'
#                     diff.append(difference)
#                     perc='NA'
                
#                 diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc}).drop_duplicates()
                
#                 if len(diff_df)>0:
                
#                     for ind, row in final.iterrows():
#                         result = {"highlights": [], "error": "Guidance Initiation - Company level"}
#                         result["highlights"].append({"row": {"name": "NA", "id": "NA"}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
#                         result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA", "diff": "NA", "percent": "NA"}
#                         result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear": row["fiscalYear"], "fiscalQuarter":"NA", "peo": "NA","value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
#                         errors.append(result)
#     print(errors)
#     return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_341(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 20
    """
    This check should fire when Revenue actual and EBIT actual is same
    LHSdataItemIds-(100186)
    RHSdataItemIds-(100221)
    Operation-(==)

    """ 
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #REVENUE
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT
    operator = get_dataItemIds_list('Operation', parameters) #["=="]

    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        companyid=filingMetadata['metadata']['companyId']
    
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #REVENUE
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #CAPEX
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
                
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)


        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        
        # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #REVENUE
        
        # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()  #EBIT
        
        # print(lhs_df)
        # print(rhs_df)

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        # print(merged_df)

        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])               
                # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))  
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Revenue actual and EBIT actual is same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Revenue actual and EBIT actual is same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_359(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 20
    """
    This check should fire when EBITDA and EBT Normalised have same value and EBIT and EBT GAAP collected same values- excluding JAPAN, Korea(South and North)
    TAG1-(100193)
    TAG2-(100235)
    TAG3-(100221)
    TAG4-(100249)
    Operation-(==)
    COUNTRY_EXCLUDE-(JP,KP,KR))

    """ 
    errors = []
    tag1_list=get_dataItemIds_list('TAG1', parameters) #EBITDA
    tag2_list=get_dataItemIds_list('TAG2', parameters) #EBTN
    tag3_list=get_dataItemIds_list('TAG3', parameters) #EBIT
    tag4_list=get_dataItemIds_list('TAG4', parameters) #EBTG
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    countryCode=get_dataItemIds_list('COUNTRY_EXCLUDE',parameters)
    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        if filingMetadata['metadata']['country'] not in countryCode:
            
            companyid=filingMetadata['metadata']['companyId']
        
            tag1_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag1_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBITDA
            
            if len(tag1_df)==0:
                tag1_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            
            tag2_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag2_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBTN
            if len(tag2_df)==0:
                tag2_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(tag2_list)) & (historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                    
            if tag1_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
                tag1_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag1_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(tag1_df['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag1_df=pd.concat([tag1_df,tag1_df_missing_data],ignore_index=True)            
        
            if tag2_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
                tag2_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag2_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(tag2_df['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)            
                
            if (len(tag1_df)>0 & len(tag2_df)>0):
                base_currency=tag1_df.currency.mode()[0]
                tag1_df["value_scaled"] = tag1_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
                tag2_df["value_scaled"] = tag2_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)

            
            merged1_df=pd.merge(tag1_df,tag2_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

            
            tag3_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag3_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT
            if len(tag3_df)==0:
                tag3_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(tag3_list)) & (historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
       
            tag4_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag4_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBTG
            if len(tag4_df)==0:
                tag4_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(tag4_list)) & (historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
    
            if tag3_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
                tag3_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag3_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(tag3_df['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag3_df=pd.concat([tag3_df,tag3_df_missing_data],ignore_index=True)            
    
            
            if tag4_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
                tag4_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag4_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(tag4_df['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag4_df=pd.concat([tag4_df,tag4_df_missing_data],ignore_index=True)            
    

            if (len(tag3_df)>0 & len(tag2_df)>0):
                base_currency=tag3_df.currency.mode()[0]
                tag3_df["value_scaled"] = tag3_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
                tag4_df["value_scaled"] = tag4_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
   
            merged2_df=pd.merge(tag3_df,tag4_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
            
            # if (len(merged1_df)>0 & len(merged2_df)>0):
            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[] 
            diff=[]
            perc=[]
            
            for ind,row in merged1_df.iterrows():
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    for ind,row in merged2_df.iterrows():
                        if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                            peos.append(row['peo'])
                            tid.append(row['tradingItemId'])
                            parentflag.append(row['parentFlag'])
                            accounting.append(row['accountingStandardDesc']) 
                            fyc.append(row['fiscalChainSeriesId'])
                            diff='NA'
                            perc='NA'
            diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
                temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag1_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                        |((extractedData_parsed['dataItemId'].isin(tag2_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                        |((extractedData_parsed['dataItemId'].isin(tag3_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))    
                        |((extractedData_parsed['dataItemId'].isin(tag4_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                                                                                                                                                                                                                            
                temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(tag1_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))  
                        |((historicalData_parsed['dataItemId'].isin(tag2_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))
                        |((historicalData_parsed['dataItemId'].isin(tag3_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))
                        |((historicalData_parsed['dataItemId'].isin(tag4_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
                
                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()
                                                                                                                                                                                                                            
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "EBITDA and EBT Normalised have same value and EBIT and EBT GAAP collected same values- excluding JAPAN, Korea(South and North)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "EBITDA and EBT Normalised have same value and EBIT and EBT GAAP collected same values- excluding JAPAN, Korea(South and North)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_346(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 20
    """
    This check should fire when EBT GAAP and EBT Normalized less than NI GAAP, NI Normalized and ETR collected
    LHSdataItemIds-(100249)
    RHSdataItemIds-(100270)
    TAG1-(114165)
    TAG2-(100235)
    TAG3-(100256)
    Operation-(<)

    """
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBT_GAAP
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #NI_GAAP
    tag_list=get_dataItemIds_list('TAG1', parameters) #Taxrate
    tag2_list=get_dataItemIds_list('TAG2', parameters) #EBT_Norm
    tag3_list=get_dataItemIds_list('TAG3', parameters) #NI_Norm
    operator = get_dataItemIds_list('Operation', parameters) #["<"]
    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #NI_GAAP
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBT _GAAP
        ETR_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #Taxrate
        tag2_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag2_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #NI_Norm
        tag3_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag3_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBT_Norm

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
       
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
        if ETR_df["dataItemId"].nunique()!=len(tag_list):
            ETR_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        ETR_df_peo_count=ETR_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=ETR_df_peo_count[(ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=ETR_df[ETR_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            ETR_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            ETR_df=pd.concat([ETR_df,ETR_df_missing_data],ignore_index=True)            

        if tag2_df["dataItemId"].nunique()!=len(tag2_list):
            not_captured= [x for x in tag2_list if x not in set(tag2_df["dataItemId"])]
            tag2_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #DA
            tag2_df = pd.concat([tag2_df,tag2_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag2_df_peo_count=tag2_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (tag2_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=tag2_df_peo_count[(tag2_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=tag2_df[tag2_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            tag2_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)

        if tag3_df["dataItemId"].nunique()!=len(tag3_list):
            not_captured= [x for x in tag3_list if x not in set(tag3_df["dataItemId"])]
            tag3_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #DA
            tag3_df = pd.concat([tag3_df,tag3_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag3_df_peo_count=tag3_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (tag3_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=tag3_df_peo_count[(tag3_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=tag3_df[tag3_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            tag3_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag3_df=pd.concat([tag3_df,tag3_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0 & len(ETR_df)>0 & len(tag2_df)>0 & len(tag3_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)     
            tag2_df["value_scaled"] = tag2_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            tag3_df["value_scaled"] = tag3_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            

        # lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBTG

        # rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #NIG  

        # ETR_df=ETR_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #ETR

        # tag2_df=tag2_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBTN

        # tag3_df=tag3_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #NIN
        

        
        merged1_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
                
        merged2_df=pd.merge(tag2_df,tag3_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
                
        merged_df=pd.merge(merged1_df,merged2_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
                
        merged_ETR_df = pd.merge(ETR_df,merged_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

    
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        
        for ind,row in merged_ETR_df.iterrows():
            if (row['value_scaled']>0):
                if execute_operator(row['value_scaled_x_y'],row['value_scaled_y_y'],operator[0]) or execute_operator(row['value_scaled_x_x'],row['value_scaled_y_x'],operator[0]):
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
 
            temp1 = extractedData_parsed[((((extractedData_parsed['dataItemId'].isin(tag_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                                                                                                                                                                                                                        
            temp2 = historicalData_parsed[((((historicalData_parsed['dataItemId'].isin(tag_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))))][['dataItemId','peo','value','scale','currency','team','tradingItemId','accountingStandardDesc','parentFlag','fiscalChainSeriesId','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate']].drop_duplicates()


            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "EBT GAAP and EBT Normalized less than NI GAAP, NI Normalized and ETR collected"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "EBT GAAP and EBT Normalized less than NI GAAP, NI Normalized and ETR collected"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_90(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 20
    """
    This check should fire when Gross Margin/Effective tax rate captured within +/-1 range
    LHSdataItemIds-(105299)
    RHSdataItemIds-(114165)
    TAG1-(114230,114231,114232)
    TAG2-(114172,114173,114174)
    Operation-(<=)
    Max Threshold-(1)

    """
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # GM Actuals
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters) # ETR Actual
    tag1_list=get_dataItemIds_list('TAG1', parameters) # GM Guidance
    tag2_list=get_dataItemIds_list('TAG2', parameters) # ETR Guidance
    operator = get_dataItemIds_list('Operation', parameters) # "<="
    Max_Threshold=get_parameter_value(parameters,'Max Threshold') # 1
    # Min_Threshold=get_parameter_value(parameters,'Min_Threshold') # -1
    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        companyid=filingMetadata['metadata']['companyId']

        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        tag1_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        tag2_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        # Actuals
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured= [x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
      
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemId_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)


        #Guidance

        if tag1_df["dataItemId"].nunique()!=len(tag1_list):
            not_captured= [x for x in tag1_list if x not in set(tag1_df["dataItemId"])]
            tag1_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT DA
            tag1_df = pd.concat([tag1_df,tag1_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag1_df_peo_count=tag1_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (tag1_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=tag1_df_peo_count[(tag1_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=tag1_df[tag1_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                tag1_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag1_df=pd.concat([tag1_df,tag1_df_missing_data],ignore_index=True)
                
        if tag2_df["dataItemId"].nunique()!=len(tag2_list):
            not_captured= [x for x in tag2_list if x not in set(tag2_df["dataItemId"])]
            tag2_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT DA
            tag2_df = pd.concat([tag2_df,tag2_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag2_df_peo_count=tag2_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (tag2_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=tag2_df_peo_count[(tag2_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=tag2_df[tag2_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                tag2_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)
                
                        
        lhs_df=lhs_df.groupby(['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value'].sum().reset_index()
        rhs_df=rhs_df.groupby(['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value'].sum().reset_index()


        # print(lhs_df)
        # print(rhs_df)
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        merged_df1=pd.merge(tag1_df,tag2_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        # merged_df['variation']=abs((merged_df[['value_x', 'value_y']].max(axis=1) - merged_df[['value_x', 'value_y']].min(axis=1)))
        
        merged_df['variation'] = abs(((merged_df[['value_x','value_y']].max(axis=1).astype(float)) - (merged_df[['value_x','value_y']].min(axis=1).astype(float))))
        
        merged_df1['variation'] = abs(((merged_df1[['value_x','value_y']].max(axis=1).astype(float)) - (merged_df1[['value_x','value_y']].min(axis=1).astype(float))))
                
        # print(merged_df)

        dataItemIds_x=[]
        dataItemIds_y=[]
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        for ind, row in merged_df.iterrows():
            if row['variation']!=0.0:
                if ((execute_operator(row['variation'],float(Max_Threshold[0]),operator[0]))):                
                    dataItemIds_x.append(row['dataItemId_x'])
                    dataItemIds_y.append(row['dataItemId_y'])
                    peos.append(row['peo'])  
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(row['variation'])
                    perc = 'NA'

        for ind, row in merged_df1.iterrows():
            if row['variation']!=0.0:
                if ((execute_operator(row['variation'],float(Max_Threshold[0]),operator[0]))):                
                    dataItemIds_x.append(row['dataItemId_x'])
                    dataItemIds_y.append(row['dataItemId_y'])
                    peos.append(row['peo'])  
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(row['variation'])
                    perc = 'NA'

        diff_df = pd.DataFrame({"peo": peos,"dataItemId_x": dataItemIds_x,"dataItemId_y": dataItemIds_y,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc ,"diff": diff, "perc": perc})
        
       
        if len(diff_df)>0:

            diff_df['peocomb_x'] = diff_df[['dataItemId_x','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            diff_df['peocomb_y'] = diff_df[['dataItemId_y','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # extractedData_parsed['peocomb_y'] = extractedData_parsed[['dataItemId_y','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1) 
             

            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb_x']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))] [['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()   
            temp2 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb_y']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))] [['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates() 


            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():    
                result = {"highlights": [], "error": "Gross Margin/Effective tax rate captured within +/-1 range"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb_x']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb_x']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb_x']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb_x']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
                
            for ind, row in temp2_revised.iterrows():    
                result = {"highlights": [], "error": "Gross Margin/Effective tax rate captured within +/-1 range"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb_y']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb_y']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb_y']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb_y']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_35A(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 21
    errors = []
    operator = get_dataItemIds_list('Operation', parameters)
    
    if extractedData_parsed and historicalData_parsed is not None:
        documentdate=filingMetadata['metadata']['filingDate']
        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemFlag']=='A')|(extractedData_parsed['dataItemFlag']=='G'))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!= "")&(extractedData_parsed['peo'].notnull()))][['dataItemId','peo','fiscalChainSeriesId']].drop_duplicates()
        
        # temp['filingDate']=pd.to_datetime(filingMetadata['metadata']['filingDate'])
        
        temp['companyId']=filingMetadata['metadata']['companyId']
        
        
        previous = historicalData_parsed[(((historicalData_parsed['dataItemFlag']=='A')|(historicalData_parsed['dataItemFlag']=='G'))&(historicalData_parsed['companyId'].isin(temp['companyId']))&(historicalData_parsed['filingDate']<documentdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo']!="")&(historicalData_parsed['peo'].notnull()))][['dataItemId','description','peo','fiscalChainSeriesId','filingDate','companyId']].drop_duplicates()
        
        newfyc=temp[~temp['fiscalChainSeriesId'].isin(previous['fiscalChainSeriesId'].unique())]
        
        # print(newfyc)

        maxprevious=previous.groupby(['companyId'])['filingDate'].max().reset_index()

        previous=previous[previous['filingDate'].isin(maxprevious['filingDate'])]
        
  
        merged_df=pd.merge(temp,previous,on=['companyId'],how='inner')


        filingdate=[]
        diff=[]
        perc=[]
        series1=[]
        series2=[]
               
        for ind,row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'],row['fiscalChainSeriesId_y'],operator[0]):
                filingdate.append(row['filingDate'])             
                difference='NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc='NA'

        diff_df=pd.DataFrame({"diff":diff,"perc":perc,"filingDate":filingdate,"curseries":series1,"preseries":series2}).drop_duplicates()
       

        temp1 = extractedData_parsed[((extractedData_parsed['fiscalChainSeriesId'].isin(series1)))][['fiscalChainSeriesId','accountingStandardDesc','parentFlag','team']].drop_duplicates()

        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['fiscalChainSeriesId'].isin(series2)))][['fiscalChainSeriesId','accountingStandardDesc','parentFlag','team','versionId','feedFileId','filingDate','companyId']].drop_duplicates()
    
        if len(newfyc)>0:
            temp3 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(newfyc['fiscalChainSeriesId']))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
            temp3_revised=temp3.dropna()
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],"error": "New fiscal Year series collected for this company"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA","companyid": filingMetadata['metadata']['companyId']},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)           

        
        if len(temp1) > 0 and len(temp2) > 0:
            temp1_revised=temp1.dropna()          
            for ind, row in temp1_revised.iterrows():  
                result = {"highlights": [], "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA","companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear":"NA", "fiscalQuarter":"NA", "peo":"NA","value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                errors.append(result)
        
            temp2_revised=temp2.dropna()
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Variation in fiscal Year series compared to the previous document"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA","companyid": row['companyId']}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear": "NA", "fiscalQuarter":"NA", "peo": "NA","value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result)
        print(errors)
        return errors
    # except Exception as e:
    #     print(e)
    #     return errors 

#Estimates Error Checks 
@add_method(Validation) 
def AG_350(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 20
    """
    This check should fire when NI GAAP & NI Normalized Increased whereas EPS GAAP & EPS Normalized decreases and vice versa
    TAG1-(100270)
    TAG2-(100256)
    TAG3-(100284)
    TAG4-(100179)
    Operation-(>,<)

    """

    errors = []
    ni_gaap=get_dataItemIds_list('TAG1', parameters) #ni_gaap_df
    ni_nor=get_dataItemIds_list('TAG2', parameters) #ni_nor_df
    eps_gaap=get_dataItemIds_list('TAG3', parameters) #eps_gaap_df
    eps_nor=get_dataItemIds_list('TAG4', parameters) #eps_nor_df
    operator=get_dataItemIds_list('Operation', parameters) #['>','<']
    try:
    # if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'value', 'peo'}.issubset(extractedData_parsed.columns) or {'value', 'peo'}.issubset(historicalData_parsed.columns))):
        
        companyid=filingMetadata['metadata']['companyId']
        
        ni_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        ni_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                         
        eps_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        eps_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                          

        current_peo_count=extractedData_parsed['peo'].nunique()
        if len(ni_gaap_df['dataItemId'])==0:
            ni_gaap_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_gaap) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((ni_gaap_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            ni_gaap_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_gaap_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            ni_gaap_df=pd.concat([ni_gaap_df,ni_gaap_df_missing_data],ignore_index=True)
        
        if len(ni_nor_df['dataItemId'])==0:
            ni_nor_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_nor) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((ni_nor_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            ni_nor_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_nor_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            ni_nor_df=pd.concat([ni_nor_df,ni_nor_df_missing_data],ignore_index=True)
        
        if len(eps_gaap_df['dataItemId'])==0:
            eps_gaap_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_gaap) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((eps_gaap_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            eps_gaap_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_gaap_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            eps_gaap_df=pd.concat([eps_gaap_df,eps_gaap_df_missing_data],ignore_index=True)
        
        if len(eps_nor_df['dataItemId'])==0:
            eps_nor_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_nor) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((eps_nor_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            eps_nor_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_nor_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            eps_nor_df=pd.concat([eps_nor_df,eps_nor_df_missing_data],ignore_index=True)
       
        #print(ni_gaap_df['value_scaled'],ni_nor_df['value_scaled'],eps_gaap_df['value_scaled'],eps_nor_df['value_scaled'])
        

        ni_merged_df=pd.merge(ni_gaap_df,ni_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        eps_merged_df=pd.merge(eps_gaap_df,eps_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        merged_df=pd.merge(ni_merged_df,eps_merged_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        

        peos=[]
        diff=[]
        perc=[]
        tidx=[]
        tidy=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        

        for ind,row in merged_df.iterrows():
            if ((execute_operator(row['value_scaled_x_x'],row['value_scaled_y_x'],operator[0]) and
                 execute_operator(row['value_scaled_x_y'],row['value_scaled_y_y'],operator[1])) or 
                (execute_operator(row['value_scaled_y_x'],row['value_scaled_x_x'],operator[0]) and 
                 execute_operator(row['value_scaled_y_y'],row['value_scaled_x_y'],operator[1]))) :
                peos.append(row['peo'])
                diff='NA'
                perc='NA'
                tidx.append(row['tradingItemId_x'])
                tidy.append(row['tradingItemId_y'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
           
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId_x":tidx,"tradingItemId_y":tidy,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc}).drop_duplicates()
        

        if len(diff_df)>0:
            
            diff_df['peocomb']=diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)            
            extractedData_parsed['peocomb']=extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(ni_gaap)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))& (extractedData_parsed['tradingItemId'].isin(diff_df['tradingItemId_x']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          |((extractedData_parsed['dataItemId'].isin(ni_nor)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))& (extractedData_parsed['tradingItemId'].isin(diff_df['tradingItemId_x']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          |((extractedData_parsed['dataItemId'].isin(eps_gaap)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))& (extractedData_parsed['tradingItemId'].isin(diff_df['tradingItemId_y']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))    
                                          |((extractedData_parsed['dataItemId'].isin(eps_nor)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))& (extractedData_parsed['tradingItemId'].isin(diff_df['tradingItemId_y']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                                                                                                                                                                                                                    
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(ni_gaap)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))& (historicalData_parsed['tradingItemId'].isin(diff_df['tradingItemId_x']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                                           |((historicalData_parsed['dataItemId'].isin(ni_nor)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))& (historicalData_parsed['tradingItemId'].isin(diff_df['tradingItemId_x']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))
                                           |((historicalData_parsed['dataItemId'].isin(eps_gaap)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))& (historicalData_parsed['tradingItemId'].isin(diff_df['tradingItemId_y']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))
                                           |((historicalData_parsed['dataItemId'].isin(eps_nor)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))& (historicalData_parsed['tradingItemId'].isin(diff_df['tradingItemId_y']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','filingDate','feedFileId']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "NI GAAP & NI Normalized Increased whereas EPS GAAP & EPS Normalized decreases and vice versa"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row['peocomb']]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row['peocomb']]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row['peocomb']]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row['peocomb']]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "NI GAAP & NI Normalized Increased whereas EPS GAAP & EPS Normalized decreases and vice versa"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row['peocomb']]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row['peocomb']]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row['peocomb']]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row['peocomb']]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_54(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold')
    try:
        filingdate=filingMetadata['metadata']['filingDate']
        
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)  
        
        current=extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','dataItemFlag']]

        current['peocomb']=current.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        history=historicalData_parsed[((historicalData_parsed['peocomb'].isin(current['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate','dataItemFlag']]
       
        history['daysdiff']=abs((pd.to_datetime(filingdate)-pd.to_datetime(history['filingDate'])).dt.days)
 
        merged_df=pd.merge(current,history,on=['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','dataItemFlag'],how='inner')

        if len(merged_df)>0:
        
            dataItemIds=[]
            diff=[]
            perc=[]
            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            
            for ind,row in merged_df.iterrows():

                if row['dataItemFlag']=="G":
                    if execute_operator(row['daysdiff'],float(Threshold[0]),operator[1]):
                        if execute_operator(float(row['value_x']),float(row['value_y']),operator[0]):   
                            dataItemIds.append(row['dataItemId'])               
                            diff='NA'
                            perc='NA'
                            peos.append(row['peo'])
                            tid.append(row['tradingItemId']) 
                            parentflag.append(row['parentFlag']) 
                            accounting.append(row['accountingStandardDesc']) 
                            fyc.append(row['fiscalChainSeriesId'])
    
                if row['dataItemFlag']=="A":
                    if execute_operator(float(row['value_x']),float(row['value_y']),operator[0]):
                        dataItemIds.append(row['dataItemId'])               
                        diff='NA'
                        perc='NA'
                        peos.append(row['peo'])
                        tid.append(row['tradingItemId']) 
                        parentflag.append(row['parentFlag']) 
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc,'peo':peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

            if len(diff_df)>0:
                diff_df['peocomb']=diff_df[['peo','dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed[['peo','dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised=temp1.dropna()

                for ind, row in temp1_revised.iterrows():

                    result = {"highlights": [], "error": "Duplicate Actual/Guidance value"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
            
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors



#Estimates Error Checks 
@add_method(Validation) 
def AG_55(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actual 21
    #JSON 34
    """
    This check should fire when some Data Captured in Inactive trading item


    """ 

    errors = []
    # operator = get_dataItemIds_list('Operation', parameters)
    
    if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'value', 'dataItemId','tradingItemStatus','parentFlag'}.issubset(extractedData_parsed.columns))):
        
        temp = extractedData_parsed[((extractedData_parsed['tradingItemId']!=-1)&(extractedData_parsed['tradingItemStatus']=='Inactive')&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','description','tradingItemStatus','tradingItemId','tradingItemName','parentFlag']].drop_duplicates()

        temp['filingDate']=pd.to_datetime(filingMetadata['metadata']['filingDate']).date()
        
        print(temp)

        if len(temp)>0:
            dataItemIds=[]
            tradingitemid=[]
            tradingitem=[]
            parentflag=[]
    
            for ind,row in temp.iterrows():
                # if execute_operator(row['filingDate'],pd.to_datetime(row['lastTradedDate']).date(),operator[0]):
                dataItemIds.append(row['dataItemId'])               
                tradingitemid.append(row['tradingItemId'])
                tradingitem.append(row['tradingItemName'])
                parentflag.append(row['parentFlag'])
                          
            diff_df=pd.DataFrame({"tradingItemId":tradingitemid,"tradingItemName":tradingitem,"parentFlag":parentflag}).drop_duplicates()
            
            if len(diff_df)>0:
                diff_df['peocomb'] = diff_df[['tradingItemId','parentFlag']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['tradingItemId','parentFlag']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                temp1=extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['tradingItemStatus']=='Inactive')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['fiscalChainSeriesId','tradingItemId','accountingStandardDesc','parentFlag','team','tradingItemName']].drop_duplicates()
                
                temp1_revised=temp1.dropna()
                
                # print(temp1_revised)

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Data Captured in Inactive trading item"}
                    result["highlights"].append({"row": {"name": "NA", "id": "NA"}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                    result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": row["tradingItemId"],"fiscalYear":"NA", "fiscalQuarter":"NA", "peo": "NA","value": "NA","units": "NA","currency": "NA","tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)

    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_106(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list=get_dataItemIds_list('LHSdataItemIds', parameters) #Tags
    operator = get_dataItemIds_list('Operation', parameters) #['>,<]
    maxvariation=get_parameter_value(parameters,'Max Threshold') #100%
    minvariation=get_parameter_value(parameters,'Min_Threshold') #20%
    
    try:
        temp0 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value','scaleId','value_scaled','currency','currencyId','consValue','consScaleId','consCurrency','consCurrencyId','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if len(temp0)>0:
            temp0['consValue_scaled'] = temp0.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1) 
            
            if temp0.currency.mode()[0] != 'NA':
                base_currency=temp0.currency.mode()[0]
                temp0['value_scaled'] = temp0.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                temp0['consValue_scaled'] = temp0.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
            
            
            temp0['consensusvariation'] = abs(((temp0[['value_scaled', 'consValue_scaled']].max(axis=1).astype(float)) - (temp0[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) / (temp0[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) * 100

            temp0.replace([np.inf, -np.inf], np.nan, inplace=True)

            # print(temp0)
                        
            temp1=temp0.copy()
            temp1['currency'] = temp1['currency'].astype(str)
            temp1['consCurrency'] = temp1['consCurrency'].astype(str)
            
            temp1['currency_mismatch'] = temp1.apply(lambda x: x['currency'] != x['consCurrency'], axis=1)
                

            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            
            for ind,row in temp0.iterrows():
                if (execute_operator(float(row['consensusvariation']), float(minvariation[0]), operator[0]) and execute_operator(float(row['consensusvariation']), float(maxvariation[0]), operator[1])):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId']) 
                    diff.append(float(round(row['consensusvariation'])))
                    perc.append(float(round(row['consensusvariation'])))

            for ind,row in temp1.iterrows():
                if row['currency_mismatch']==True:
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(float(round(row['consensusvariation'])))
                    perc.append(float(round(row['consensusvariation'])))

            diff_df=pd.DataFrame({"peo":peos,"dataItemId":dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc}).drop_duplicates()
            
            if len(diff_df)>0:

                diff_df['peocomb']=diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)                   

                    
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','consCurrency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates() 
                
                for ind, row in temp1.iterrows():
                    result = {"highlights": [], "error": "Currency and percentage variation between Actual and Concensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency'],"conCurrency":row['consCurrency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"conCurrency":row['consCurrency'],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)                          
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors  


#Estimates Error Checks 
@add_method(Validation) 
def AG_103(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) # tags
    operator = get_dataItemIds_list('Operation', parameters) # "=="
    Threshold=get_parameter_value(parameters,'Max Threshold') # 0
    try:

        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")
                                     &(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','value']].drop_duplicates()


        dataItemIds=[]
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        for ind, row in temp.iterrows():
            if execute_operator(float(row['value']),float(Threshold[0]),operator[0]):                
                dataItemIds.append(row['dataItemId'])
                peos.append(row['peo'])  
                tid.append(row['tradingItemId']) 
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff = 'NA'
                perc = 'NA'

        diff_df = pd.DataFrame({"peo": peos,"dataItemId": dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc, "diff": diff, "perc": perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
   
            # diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            

            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()   

            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():    
                result = {"highlights": [], "error": "Data item value cannot be zero"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"conCurrency":row['consCurrency'],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_9(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actauls 21
    errors = []
    left_dataItemId_list=get_dataItemIds_list2('LHSdataItemIds', parameters) #Tag
    right_dataItemId_list=get_dataItemIds_list2('RHSdataItemIds', parameters) #related Tag
    
            
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        related_ids = {left: right for left, right in zip(left_dataItemId_list, right_dataItemId_list)} 
                 
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()

        # print(lhs_df)

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured= [x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates() #EBIT DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
                lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)

        lhs_df['related_dataItemId'] = lhs_df['dataItemId'].map(related_ids)
        
        # print(lhs_df)
        
        related_ids = {left: right for left, right in zip(right_dataItemId_list,left_dataItemId_list)}            
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()

        if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):
            not_captured= [x for x in right_dataItemId_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
                rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        rhs_df['related_dataItemId'] = rhs_df['dataItemId'].map(related_ids)
        

        lhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list)))][['dataItemId','description']].drop_duplicates()
        rhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list)))][['dataItemId','description']].drop_duplicates()


        lhs_df2 = pd.merge(lhs_df,rhs_df1,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))
        rhs_df2 = pd.merge(rhs_df,lhs_df1,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))
        
        
        if not lhs_df2.empty:
            lhs_df2['peocomb']=lhs_df2[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        elif lhs_df2.empty and not rhs_df2.empty:
            lhs_df2['peocomb']=rhs_df2[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
        if not rhs_df2.empty:
            rhs_df2['peocomb']=rhs_df2[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        elif rhs_df2.empty and not lhs_df2.empty:
            rhs_df2['peocomb']=lhs_df2[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
            
        # print(lhs_df) 
        # print(rhs_df)     

        lhs_df_revised = pd.DataFrame()
        if not lhs_df2.empty:
            if not rhs_df2.empty:
                lhs_df_revised = lhs_df2[~lhs_df2['peocomb'].isin(rhs_df2['peocomb'])]
            else:
                lhs_df_revised = lhs_df2.copy()

        rhs_df_revised = pd.DataFrame()
        if not rhs_df2.empty:
            if not lhs_df2.empty:
                rhs_df_revised = rhs_df2[~rhs_df2['peocomb'].isin(lhs_df2['peocomb'])]
            else:
                rhs_df_revised = rhs_df2.copy()

                 
        # print(lhs_df_revised)
        # print(rhs_df_revised)
                                            
        if len(lhs_df_revised)>0:
            lhs_df_revised1=lhs_df_revised.drop_duplicates()
            lhs_df_revised_new=lhs_df_revised1.dropna()

            for ind, row in lhs_df_revised_new.iterrows():
                result = {"highlights": [], "error": "Related data item not available (Related Tag)"}
                result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description_related"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description_related"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(rhs_df_revised)>0:
            rhs_df_revised1=rhs_df_revised.drop_duplicates()
            rhs_df_revised_new=rhs_df_revised1.dropna()

            for ind, row in rhs_df_revised_new.iterrows():
                result = {"highlights": [], "error": "Related data item not available (Related Tag)"}
                result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description_related"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description_related"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)
                                
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors  


# #Estimates Error Checks 
# @add_method(Validation) 
# def AG_9(historicalData,filingMetadata,extractedData,parameters):
#     #JSON Actauls 21
#     errors = []
#     left_dataItemId_list=get_dataItemIds_list2('LHSdataItemIds', parameters) #Tag
#     right_dataItemId_list=get_dataItemIds_list2('RHSdataItemIds', parameters) #related Tag
    
            
#     try:
#         companyid=filingMetadata['metadata']['companyId']
        
#         related_ids = {left: right for left, right in zip(left_dataItemId_list, right_dataItemId_list)} 
                 
#         lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()

#         # print(lhs_df)

#         if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
#             not_captured= [x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
#             lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates() #EBIT DA
#             lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
#         extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
#         lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
#         if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
#                 missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
#                 collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
#                 lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
#                 lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)

#         lhs_df['related_dataItemId'] = lhs_df['dataItemId'].map(related_ids)


        
#         # print(lhs_df)
        
#         related_ids = {left: right for left, right in zip(right_dataItemId_list,left_dataItemId_list)}            
#         rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()

#         if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):
#             not_captured= [x for x in right_dataItemId_list if x not in set(rhs_df["dataItemId"])]
#             rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
#             rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
#         extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
#         rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
#         if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
#                 missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
#                 collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
#                 rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
#                 rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

#         rhs_df['related_dataItemId'] = rhs_df['dataItemId'].map(related_ids)
        
        
#         # rhs_df['related_desc'] = lhs_df1['description'].copy()

#         lhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list)))][['dataItemId','description']].drop_duplicates()
#         rhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list)))][['dataItemId','description']].drop_duplicates()
#         # lhs_df['related_desc'] = rhs_df1['description'] if lhs_df['related_dataItemId'] == rhs_df1['dataItemId'] else 'NaN'

#         lhs_df2 = pd.merge(lhs_df,rhs_df1,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))
#         rhs_df2 = pd.merge(rhs_df,lhs_df1,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))
        
#         # print(lhs_df2)
#         # print(rhs_df2)
#         # print(lhs_df1)
#         # print(rhs_df1)
        
#         if not lhs_df.empty:
#             lhs_df['peocomb']=lhs_df[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
#         elif lhs_df.empty and not rhs_df.empty:
#             lhs_df['peocomb']=rhs_df[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
#         if not rhs_df.empty:
#             rhs_df['peocomb']=rhs_df[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
#         elif rhs_df.empty and not lhs_df.empty:
#             rhs_df['peocomb']=lhs_df[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
            
#         # print(lhs_df) 
#         # print(rhs_df)     

#         lhs_df_revised = pd.DataFrame()
#         if not lhs_df.empty:
#             if not rhs_df.empty:
#                 lhs_df_revised = lhs_df[~lhs_df['peocomb'].isin(rhs_df['peocomb'])]
#             else:
#                 lhs_df_revised = lhs_df.copy()

#         rhs_df_revised = pd.DataFrame()
#         if not rhs_df.empty:
#             if not lhs_df.empty:
#                 rhs_df_revised = rhs_df[~rhs_df['peocomb'].isin(lhs_df['peocomb'])]
#             else:
#                 rhs_df_revised = rhs_df.copy()

                 
#         # print(lhs_df_revised)
#         # print(rhs_df_revised)
                                            
#         if len(lhs_df_revised)>0:
#             lhs_df_revised1=lhs_df_revised.drop_duplicates()
#             lhs_df_revised_new=lhs_df_revised1.dropna()

#             for ind, row in lhs_df_revised_new.iterrows():
#                 result = {"highlights": [], "error": "Related data item not available (Related Tag)"}
#                 result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
#                 result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
#                 result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
#                 errors.append(result)

#         if len(rhs_df_revised)>0:
#             rhs_df_revised1=rhs_df_revised.drop_duplicates()
#             rhs_df_revised_new=rhs_df_revised1.dropna()

#             for ind, row in rhs_df_revised_new.iterrows():
#                 result = {"highlights": [], "error": "Related data item not available (Related Tag)"}
#                 result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
#                 result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
#                 result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
#                 errors.append(result)
                                
#         print(errors) 
#         return errors
#     except Exception as e:
#         print(e)
#         return errors    

#Estimates Error Checks 
@add_method(Validation) 
def AG_11(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)

    try:
        
        quarter = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
        fiscalyear = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear']].drop_duplicates()
        
        quarter['peocomb']=quarter[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        fiscalyear['peocomb']=fiscalyear[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

        quarter_revised=quarter[~(quarter['peocomb'].isin(fiscalyear['peocomb']))]
        
        if len(quarter_revised)>0:
            quarter_revised=quarter_revised.drop_duplicates()
            quarter_revised_new=quarter_revised.dropna()

            for ind, row in quarter_revised_new.iterrows():
                result = {"highlights": [], "error": "Value exists for Qs but not for FY"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_382(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actuals 21
    """
    This check should fire when EBITDA and EBIT surprise FQ is greater than 200 percent and FQ-1 is less than 10 percent
    LHSdataItemIds-(100221,100193)
    Operation-(<,>)
    Max Threshold-(200)
    Min_Threshold-(10)

    """
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBITDA,EBIT
    operator=get_dataItemIds_list('Operation', parameters) #[>,<]
    Threshold=get_parameter_value(parameters,'Max Threshold') #200
    Threshold1=get_parameter_value(parameters,'Min_Threshold') #10
    
    try:
        
        companyid = filingMetadata['metadata']['companyId']

        current = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemId_list)&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())
                                        &(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if len(current)>0:
            current['consValue_scaled'] = current.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)

        base_currency=current.currency.mode()[0]
        current['value_scaled'] = current.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        current['consValue_scaled'] = current.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
        
        # current['consensusvariation']=abs((current[['value_scaled','consValue_scaled']].max(axis=1)-current[['value_scaled','consValue_scaled']].min(axis=1))/current[['value_scaled','consValue_scaled']].min(axis=1))*100

        current['consensusvariation'] = abs(((current[['value_scaled', 'consValue_scaled']].max(axis=1).astype(float)) - (current[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) / (current[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) * 100

        current.replace([np.inf, -np.inf], np.nan, inplace=True)
        current.dropna(inplace=True)
        
        # print(current)
        
        
        comparableyear=current['fiscalYear']-1
        
        # print(comparableyear)

        
        comparable = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(current['dataItemId']))&(historicalData_parsed['fiscalYear'].isin(comparableyear))&(historicalData_parsed['periodTypeId']==2)
                                            &(historicalData_parsed['fiscalQuarter'].isin(current['fiscalQuarter']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                            &(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['consValue']!="")&(historicalData_parsed['consValue'].notnull()))][['dataItemId','peo','value','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if len(comparable)>0:
            comparable['consValue_scaled'] = comparable.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)

        base_currency=comparable.currency.mode()[0]
        comparable['value_scaled'] = comparable.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        comparable["consValue_scaled"] = comparable.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
        
        # comparable['consensusvariation']=abs((comparable[['value_scaled','consValue_scaled']].max(axis=1)-comparable[['value_scaled','consValue_scaled']].min(axis=1))/comparable[['value_scaled','consValue_scaled']].min(axis=1))*100

        comparable['consensusvariation'] = abs(((comparable[['value_scaled', 'consValue_scaled']].max(axis=1).astype(float)) - (comparable[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) / (comparable[['value_scaled', 'consValue_scaled']].min(axis=1).astype(float))) * 100

        comparable.replace([np.inf, -np.inf], np.nan, inplace=True)
        comparable.dropna(inplace=True)
        
        # print(comparable)

        combined =pd.merge(current,comparable,on=['dataItemId','periodTypeId','fiscalQuarter','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')

        # print(combined[['dataItemId','value_scaled_x','consValue_scaled_x','value_scaled_y','consValue_scaled_y']])

        combined['variation']=abs(((combined[['value_scaled_x','consValue_scaled_x']].max(axis=1).astype(float)) - (combined[['value_scaled_x','consValue_scaled_x']].min(axis=1).astype(float))) / (combined[['value_scaled_x','consValue_scaled_x']].min(axis=1).astype(float))) * 100
        
        combined['variation1']=abs(((combined[['value_scaled_y','consValue_scaled_y']].max(axis=1).astype(float)) - (combined[['value_scaled_y','consValue_scaled_y']].min(axis=1).astype(float))) / (combined[['value_scaled_y','consValue_scaled_y']].min(axis=1).astype(float))) * 100        

        combined.replace([np.inf, -np.inf], np.nan, inplace=True)
        combined.dropna(inplace=True)

        # print(combined)
        
        # print(combined[['dataItemId','consensusvariation_x','consensusvariation_y','variation']])
        
        
        dataItemIds=[]
        peos_x=[]
        peos_y=[]
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]
        diff=[]
        perc=[]
        
        for ind,row in combined.iterrows():
            if ((execute_operator(float(row['consensusvariation_x']),float(Threshold[0]),operator[0])) and (execute_operator(float(row['consensusvariation_y']),float(Threshold1[0]),operator[1]))):
                dataItemIds.append(row['dataItemId'])
                peos_x.append(row['peo_x'])
                peos_y.append(row['peo_y'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(round(row['variation'], 2))
                perc.append(round(row['variation'], 2))
                             
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo_x":peos_x,"peo_y":peos_y,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','peo_x','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            diff_df['peocomb1'] = diff_df[['dataItemId','peo_y','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())
                                        &(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter','team','description','tradingItemName','peocomb']].drop_duplicates()

            # temp2 = historicalData_parsed[(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['periodTypeId']==2)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['consValue']!="")&(historicalData_parsed['consValue'].notnull()))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter','team','description','tradingItemName','versionId','companyId','feedFileId','estimatePeriodId','filingDate','peocomb1]].drop_duplicates()
            
            temp1_revised=temp1.dropna()
            # temp2_revised=temp2.dropna()
    
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "EBITDA & EBIT surprise FQ is greater than 200 perc and FQ-1 is less than 10 perc"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
                
            # for ind, row in temp2_revised.iterrows():
            #     result = {"highlights": [], "error": "EBITDA & EBIT surprise FQ is greater than 200 perc and FQ-1 is less than 10 perc"}
            #     result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"filingdate": row['filingDate'], "versionId": row['versionId'], "filingDate": row['filingDate']})
            #     result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
            #     result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"],"diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
            #     errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors  

#Estimates Error Checks 
@add_method(Validation) 
def AG_99(historicalData,filingMetadata,extractedData,parameters):
    #JSON Actuals 20
    """
    This check should fire when Sum of three dataitems not equal to Fourth dataitem fails for O&G Industry

    LHSdataItemIds-(115403)
    TAG1-(115455)
    TAG2-(115429)
    RHSdataItemIds-(115591)
    Min_Threshold-(1)
    Operation-">="

    """ 

    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) # dt1 Total Production Oil
    tag1_list=get_dataItemIds_list('TAG1', parameters) # dt2 Total Production Gas
    tag2_list=get_dataItemIds_list('TAG2', parameters) # dt3 Total Production NGL
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) # dt4 Total Production Actual
    operator = get_dataItemIds_list('Operation', parameters) #["!="]
    Threshold=get_parameter_value(parameters,'Min_Threshold') #1

    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'dataItemId','value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!="")&(extractedData_parsed['peo'].notnull())][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt1
        tag1_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag1_list)&(extractedData_parsed['value']!="")& (extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!="")&(extractedData_parsed['peo'].notnull())][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt2
        tag2_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag2_list)&(extractedData_parsed['value']!="")& (extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!="")&(extractedData_parsed['peo'].notnull())][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt3

        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")& (extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!="")&(extractedData_parsed['peo'].notnull())][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt4
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt1, dt2, dt3
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)

        if tag1_df["dataItemId"].nunique()!=len(tag1_list):
            not_captured= [x for x in tag1_list if x not in set(tag1_df["dataItemId"])]
            tag1_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt1, dt2, dt3
            tag1_df = pd.concat([tag1_df,tag1_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag1_df_peo_count=tag1_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (tag1_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=tag1_df_peo_count[(tag1_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=tag1_df[tag1_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                tag1_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag1_df=pd.concat([tag1_df,tag1_df_missing_data],ignore_index=True)

        if tag2_df["dataItemId"].nunique()!=len(tag2_list):
            not_captured= [x for x in tag2_list if x not in set(tag2_df["dataItemId"])]
            tag2_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #dt1, dt2, dt3
            tag2_df = pd.concat([tag2_df,tag2_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        tag2_df_peo_count=tag2_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (tag1_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=tag2_df_peo_count[(tag2_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=tag2_df[tag2_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                tag2_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)
       
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','scale','scaleId','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)



        if (len(lhs_df) > 0) and (len(tag1_df) > 0) and (len(tag2_df) > 0) and (len(rhs_df) > 0):
           
               
            lhs_combined_df = pd.concat([lhs_df, tag1_df, tag2_df], ignore_index=True)
            
            
            lhs_combined_df = lhs_combined_df.groupby(['peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId', 'periodTypeId', 'fiscalYear', 'fiscalQuarter'])['value_scaled'].sum().reset_index()
            
                                
            rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #dt4


            merged_df=pd.merge(lhs_combined_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
            
            
            merged_df['variation']=((merged_df[['value_scaled_x', 'value_scaled_y']].max(axis=1) - merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) / merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) * 100
            
            
            concatenated_list = left_dataItemIds_list + tag1_list + tag2_list
            lhs_dataItemIds_list=pd.Series(concatenated_list).drop_duplicates()
            
            
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]

        
            for ind,row in merged_df.iterrows():
                if execute_operator(row['variation'],float(Threshold[0]),operator[0]):
                    peos.append(row['peo'])               
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(round(row['variation'],2))
                    perc.append(round(row['variation'],2))
            diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"parentFlag":parentflag,"accountingStandardDesc":accounting,"tradingItemId":tid,"fiscalChainSeriesId":fyc}).drop_duplicates()
            

            if len(diff_df)>0: 
                
                diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(lhs_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                        |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter','team','description','tradingItemName','peocomb']].drop_duplicates()

                temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(lhs_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))  
                        |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))][['dataItemId','peo','value','scale','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter','team','description','tradingItemName','versionId','companyId','feedFileId','estimatePeriodId','filingDate','peocomb']].drop_duplicates()

                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()
                
                
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Sum of three dataitems not equal to Fourth dataitem"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Sum of three dataitems not equal to Fourth dataitem"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'], "filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_81(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    operator = get_dataItemIds_list('Operation', parameters)
    
    try:
        actual = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="A")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','dataItemFlag']]
        Guidance = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','dataItemFlag']]

       
        if len(actual)>0 and len(Guidance)>0:
            merged_df=pd.merge(actual,Guidance,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','tradingItemId'],how='inner')

            peos=[]
            diff=[]
            perc=[]
            for ind,row in merged_df.iterrows():
    
                if execute_operator(row['dataItemFlag_x'],row['dataItemFlag_y'],operator[0]):
    
                    peos.append(row['peo'])
                    diff='NA'
                    perc='NA'
            diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})
        
        
            if len(diff_df)>0:
                temp1 = extractedData_parsed[(extractedData_parsed['peo'].isin(diff_df['peo']))][['peo','team','accountingStandardDesc','parentFlag','fiscalChainSeriesId']]
    
                                                                                                                                                                                                      
                temp=temp1.drop_duplicates()
        
                for ind, row in temp.iterrows():
        
                    result = {"highlights": [], "error": "Actual and Guidance collected for the same data item and the same PEO from the same document"}
                    result["highlights"].append({"row": {"name": "NA", "id": "NA"}, "cell": {"peo": row['peo'], "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                    result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"]}
                    result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear":"NA", "fiscalQuarter":"NA", "peo": row["peo"],"value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors


#Estimates Error Checks 
@add_method(Validation) 
def AG_89(historicalData,filingMetadata,extractedData,parameters):
    """
    This check should fire when NI/EPS normalized collected for Japanese company

    LHSdataItemIds-(100179,22383,22443,22315,100256,22397,22457,22329)
    COUNTRY_INCLUDE-"JP"

    """ 

    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    country=get_dataItemIds_list('COUNTRY_INCLUDE',parameters) #Japan
    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'dataItemId'}.issubset(extractedData_parsed.columns)):
        
        if filingMetadata['metadata']['country'] in country:
            temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value'] != "")&(extractedData_parsed['value'].notnull()))][['dataItemId', 'value', 'parentFlag', 'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']].drop_duplicates()

            if len(temp)>0:
                dataItemIds=[]
                tid=[]
                parentflag=[]
                accounting=[]
                fyc=[]
                diff=[]
                perc=[]

                for ind, row in temp.iterrows():
                    dataItemIds.append(row['dataItemId'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff = 'NA'
                    perc = 'NA'

                diff_df = pd.DataFrame({"dataItemId": dataItemIds, "tradingItemId": tid, "parentFlag": parentflag, "accountingStandardDesc": accounting, "fiscalChainSeriesId": fyc, "diff": diff, "perc": perc})
                
                if len(diff_df)>0:
                    
                    diff_df['peocomb'] = diff_df[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                    temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) & (extractedData_parsed['value']!="") & (extractedData_parsed['value'].notnull()))][['dataItemId',  'parentFlag', 'accountingStandardDesc','tradingItemId', 'team', 'description', 'tradingItemName','fiscalChainSeriesId']].drop_duplicates()

                    temp1_revised = temp1.dropna()
                    
                    for ind, row in temp1_revised.iterrows():
                        result = {"highlights": [], "error": "NI/EPS normalized collected for Japanese company"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']},"cell": {"peo": 'NA', "scale": 'NA', "value": 'NA',"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                        result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": 'NA', "units": 'NA', "currency": 'NA',"tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA',"percent": 'NA'}]
                        errors.append(result)

    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_65A(historicalData, filingMetadata, extractedData, parameters):
    # JSON Actuals 21
    errors = []
    left_dataItemId_list = get_dataItemIds_list('LHSdataItemIds', parameters)  # High end Guidance
    right_dataItemId_list = get_dataItemIds_list('RHSdataItemIds', parameters)  # Low end Guidance

    related_ids = {left: right for left, right in zip(left_dataItemId_list, right_dataItemId_list)}
    reverse_related_ids = {right: left for left, right in zip(left_dataItemId_list, right_dataItemId_list)}

    lhs_query = (extractedData['dataItemId'].isin(left_dataItemId_list)) & \
                (extractedData['value'] != "") & \
                extractedData['value'].notnull()
    lhs_columns = ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
                   'fiscalChainSeriesId', 'fiscalYear', 'team', 'description', 'tradingItemName']
    lhs_df = extractedData[lhs_query][lhs_columns].drop_duplicates()
    lhs_df['related_dataItemId'] = lhs_df['dataItemId'].map(related_ids)

    rhs_query = (extractedData['dataItemId'].isin(right_dataItemId_list)) & \
                (extractedData['value'] != "") & \
                extractedData['value'].notnull()
    rhs_df = extractedData[rhs_query][lhs_columns].drop_duplicates()
    rhs_df['related_dataItemId'] = rhs_df['dataItemId'].map(reverse_related_ids)

    for df in [lhs_df, rhs_df]:
        if not df.empty:
            df['peocomb'] = df[['dataItemId', 'peo', 'fiscalYear', 'parentFlag',
                                'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']].astype(str).apply(''.join, axis=1)

    lhs_df_revised = lhs_df[~lhs_df['peocomb'].isin(rhs_df['peocomb'])] if not lhs_df.empty and not rhs_df.empty else lhs_df.copy()
    rhs_df_revised = rhs_df[~rhs_df['peocomb'].isin(lhs_df['peocomb'])] if not rhs_df.empty and not lhs_df.empty else rhs_df.copy()

    if not lhs_df_revised.empty:
        for ind, row in lhs_df_revised.iterrows():
            result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'}, "section": row['team'], "filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'], "companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"], "fiscalYear": row["fiscalYear"], "fiscalQuarter": 'NA', "peo": row['peo'], "value": 'NA', "units": 'NA', "currency": 'NA', "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
            errors.append(result)

    if not rhs_df_revised.empty:
        for ind, row in rhs_df_revised.iterrows():
            result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'}, "section": row['team'], "filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
            result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'], "companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
            result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"], "fiscalYear": row["fiscalYear"], "fiscalQuarter": 'NA', "peo": row['peo'], "value": 'NA', "units": 'NA', "currency": 'NA', "tradingItemName": row["tradingItemName"], "accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
            errors.append(result)

    return errors


# #Estimates Error Checks 
# @add_method(Validation) 
# def AG_65(historicalData,filingMetadata,extractedData,parameters):
#     #JSON Actauls 21
#     errors = []
#     left_dataItemId_list = get_dataItemIds_list('LHSdataItemIds', parameters)  # High end Guidance
#     right_dataItemId_list = get_dataItemIds_list('RHSdataItemIds', parameters)  # Low end Guidance

#     try:

#         related_ids = {left: right for left, right in zip(left_dataItemId_list, right_dataItemId_list)}
#         reverse_related_ids = {right: left for left, right in zip(left_dataItemId_list, right_dataItemId_list)}


#         lhs_query = (extractedData_parsed['dataItemId'].isin(left_dataItemId_list)) & \
#                     (extractedData_parsed['value'] != "") & \
#                     extractedData_parsed['value'].notnull()
#         lhs_columns = ['dataItemId', 'peo', 'parentFlag', 'accountingStandardDesc', 'tradingItemId',
#                     'fiscalChainSeriesId', 'fiscalYear', 'team', 'description', 'tradingItemName']
#         lhs_df = extractedData_parsed[lhs_query][lhs_columns].drop_duplicates()
#         lhs_df['related_dataItemId'] = lhs_df['dataItemId'].map(related_ids)


#         rhs_query = (extractedData_parsed['dataItemId'].isin(right_dataItemId_list)) & \
#                     (extractedData_parsed['value'] != "") & \
#                     extractedData_parsed['value'].notnull()
#         rhs_df = extractedData_parsed[rhs_query][lhs_columns].drop_duplicates()
#         rhs_df['related_dataItemId'] = rhs_df['dataItemId'].map(reverse_related_ids)


#         for df in [lhs_df, rhs_df]:
#             if not df.empty:
#                 df['peocomb'] = df[['dataItemId', 'peo', 'fiscalYear', 'parentFlag',
#                                     'accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId']].astype(str).apply(''.join, axis=1)


#         lhs_df_revised = lhs_df[~lhs_df['peocomb'].isin(rhs_df['peocomb'])] if not lhs_df.empty and not rhs_df.empty else lhs_df.copy()
#         rhs_df_revised = rhs_df[~rhs_df['peocomb'].isin(lhs_df['peocomb'])] if not rhs_df.empty and not lhs_df.empty else rhs_df.copy()


#         if len(lhs_df_revised)>0:
#             lhs_df_revised1=lhs_df_revised.drop_duplicates()
#             lhs_df_revised_new=lhs_df_revised1.dropna()

#             for ind, row in lhs_df_revised_new.iterrows():
#                 result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
#                 result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
#                 result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
#                 result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
#                 errors.append(result)

#         if len(rhs_df_revised)>0:
#             rhs_df_revised1=rhs_df_revised.drop_duplicates()
#             rhs_df_revised_new=rhs_df_revised1.dropna()

#             for ind, row in rhs_df_revised_new.iterrows():
#                 result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
#                 result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
#                 result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
#                 result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
#                 errors.append(result)
                                
#         print(errors) 
#         return errors                                                                   
#     except Exception as e:
#         print(e)
#         return errors 
    
    #     for df, role in [(lhs_df_revised, "High End"), (rhs_df_revised, "Low End")]:
    #         for _, row in df.drop_duplicates().dropna().iterrows():
    #             result = {
    #                 "highlights": [{"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']}],
    #                 "error": f"{role} of Guidance captured but opposite end not captured",
    #                 "checkGeneratedFor": {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA', "percent": 'NA'},
    #                 "checkGeneratedForList": [{"tag": row['dataItemId'], "description": row["description"],"tradingItemId": row["tradingItemId"], "fiscalYear": row["fiscalYear"],"fiscalQuarter": 'NA', "peo": row['peo'], "value": 'NA', "units": 'NA',"currency": 'NA', "tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA', "percent": 'NA'}]
    #             }
    #             errors.append(result)

    #     print(errors)
    #     return errors

    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_65(historicalData, filingMetadata, extractedData, parameters):
    """
    This check should fire when High End of Guidance captured but Low End not captured and vice versa
    LHSdataItemIds-(High end Guidance)
    RHSdataItemIds-(Low end Guidance)

    """

    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #High end Guidance
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters) #Low end Guidance
    
    if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'dataItemId', 'value',  'peo'}.issubset(extractedData_parsed.columns))):
                
        related_ids = {left: right for left, right in zip(left_dataItemId_list, right_dataItemId_list)}            
        lhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
        lhs_df1['related_dataItemId'] = lhs_df1['dataItemId'].map(related_ids)

        # print(lhs_df)
        
        related_ids = {left: right for left, right in zip(right_dataItemId_list,left_dataItemId_list)}            
        rhs_df1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','team','description','tradingItemName']].drop_duplicates()
        rhs_df1['related_dataItemId'] = rhs_df1['dataItemId'].map(related_ids)


        lhs_df2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list)))][['dataItemId','description']].drop_duplicates()
        rhs_df2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list)))][['dataItemId','description']].drop_duplicates()


        lhs_df = pd.merge(lhs_df1,rhs_df2,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))
        rhs_df = pd.merge(rhs_df1,lhs_df2,left_on='related_dataItemId',right_on='dataItemId',how='left',suffixes=('','_related'))

        if not lhs_df.empty:
            lhs_df['peocomb']=lhs_df[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        elif lhs_df.empty and not rhs_df.empty:
            lhs_df['peocomb']=rhs_df[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
        if not rhs_df.empty:
            rhs_df['peocomb']=rhs_df[['dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        elif rhs_df.empty and not lhs_df.empty:
            rhs_df['peocomb']=lhs_df[['related_dataItemId','peo','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
            
        # print(lhs_df) 
        # print(rhs_df)     

        lhs_df_revised = pd.DataFrame()
        if not lhs_df.empty:
            if not rhs_df.empty:
                lhs_df_revised = lhs_df[~lhs_df['peocomb'].isin(rhs_df['peocomb'])]
            else:
                lhs_df_revised = lhs_df.copy()

        rhs_df_revised = pd.DataFrame()
        if not rhs_df.empty:
            if not lhs_df.empty:
                rhs_df_revised = rhs_df[~rhs_df['peocomb'].isin(lhs_df['peocomb'])]
            else:
                rhs_df_revised = rhs_df.copy()

                 
        # print(lhs_df_revised)
        # print(rhs_df_revised)
                                            
        if len(lhs_df_revised)>0:            
            lhs_df_revised1=lhs_df_revised.drop_duplicates()
            lhs_df_revised_new=lhs_df_revised1.dropna()

            for ind, row in lhs_df_revised_new.iterrows():
                result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
                result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description_related"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description_related"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(rhs_df_revised)>0:
            rhs_df_revised1=rhs_df_revised.drop_duplicates()
            rhs_df_revised_new=rhs_df_revised1.dropna()

            for ind, row in rhs_df_revised_new.iterrows():
                result = {"highlights": [], "error": "High End of Guidance captured but Low End not captured and vice versa"}
                result["highlights"].append({"row": {"name": row['related_dataItemId'], "id": row["related_dataItemId"]}, "cell": {"peo": row['peo'], "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['related_dataItemId'], "description": row["description_related"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'], "diff": 'NA', "percent": 'NA'}
                result["checkGeneratedForList"]=[{"tag": row['related_dataItemId'], "description": row["description_related"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row['peo'],"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                errors.append(result)
                                
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors 
    
#Estimates Error Checks 
@add_method(Validation) 
def AG_34(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    #operator=get_tags_list('Operation', parameters) #"==,!="
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        dataitempeo=extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemId_list))][['dataItemId','peo']]
        
        # print(dataitempeo)
        
        Actual = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','description','peo','value','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']]
        
        # print(Actual)
        
        Consensus = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemId_list)&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','description','peo','consValue','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']]                          
        
        # print(Consensus)
        
        actualpeo=Actual[['dataItemId','peo']].drop_duplicates()
        
        # print(actualpeo)


        if len(dataitempeo)>len(actualpeo):
            missedpeo=dataitempeo[~dataitempeo.isin(actualpeo)].dropna()
            
            # print(missedpeo)


            Actual_hist = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missedpeo['dataItemId']) &(historicalData_parsed['peo'].isin(missedpeo['peo']))
                                                    &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid))][['dataItemId','description','peo','value','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']]
 
            # print(Actual_hist)
            
            Actual1 = pd.concat([Actual,Actual_hist])

        merged_df=pd.merge(Actual1,Consensus,on=['peo','dataItemId','description','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId'],how='inner')
        # print(merged_df)

        #collected tags
        peos_x=[]
        tags_x=[]
        tid_x=[]
        parentflag_x=[]
        accounting_x=[]
        fyc_x=[]             
        for ind,row in merged_df.iterrows():

            if (row['value']!="" and row['consValue']!=""):

                peos_x.append(row['peo'])
                tags_x.append(row['dataItemId'])
                tid_x.append(row['tradingItemId']) 
                parentflag_x.append(row['parentFlag']) 
                accounting_x.append(row['accountingStandardDesc']) 
                fyc_x.append(row['fiscalChainSeriesId'])

        diff_df=pd.DataFrame({"dataItemId":tags_x,"peo":peos_x,"parentFlag":parentflag_x,"tradingItemId":tid_x,"accountingStandardDesc":accounting_x,"fiscalChainSeriesId":fyc_x})

        #not collected tags
        peos_y=[]
        tags_y=[]
        tid_y=[]
        parentflag_y=[]
        accounting_y=[]
        fyc_y=[]        
        for ind,row in Consensus.iterrows():

            if ~((row['dataItemId']==diff_df['dataItemId']) & ( row['peo']==diff_df['peo'])& ( row['tradingItemId']==diff_df['tradingItemId'])
                 &( row['parentFlag']==diff_df['parentFlag'])&( row['accountingStandardDesc']==diff_df['accountingStandardDesc'])&( row['fiscalChainSeriesId']==diff_df['fiscalChainSeriesId'])).any():
               
                peos_y.append(row['peo'])
                tags_y.append(row['dataItemId'])
                tid_y.append(row['tradingItemId']) 
                parentflag_y.append(row['parentFlag']) 
                accounting_y.append(row['accountingStandardDesc']) 
                fyc_y.append(row['fiscalChainSeriesId'])  
                
        diff_df1=pd.DataFrame({"dataItemId":tags_y,"peo":peos_y,"parentFlag":parentflag_y,"tradingItemId":tid_y,"accountingStandardDesc":accounting_y,"fiscalChainSeriesId":fyc_y})

        if len(diff_df1)>0:
            
            diff_df1['peocomb']=diff_df1[['dataItemId','peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            # diff_df1['compressed']=diff_df1.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # extractedData_parsed['compressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        

            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df1['peocomb'])))]
            

            for ind, row in temp1.iterrows():   
                result = {"highlights": [], "error": "Consensus available but the actual was not captured"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff":"NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                errors.append(result)
       
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_33(historicalData,filingMetadata,extractedData,parameters):
    #JSON 19
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #Any Dataitemid
    operator = get_dataItemIds_list('Operation', parameters) #[">="]

    try:
        FQ = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==2))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()  #Quarters data
        # print(FQ)
        HY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==10))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()  #semi anual data
        # print(HY)
        FY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list)) &(extractedData_parsed["value"]!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"]==1))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()  #Annual data
        # print(FY)
    
        
        if (((len(FY)>0) & (len(FQ)>0))):
            base_currency=FQ.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            

        
        elif ((len(HY)>0) & (len(FQ)>0)):    
            base_currency=FQ.currency.mode()[0]
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            HY["value_scaled"] = HY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)            
        
        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')    
        merged_df1=pd.merge(FQ,HY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        merged_df['variation']=((merged_df[['value_scaled_x', 'value_scaled_y']].max(axis=1) - merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) / merged_df[['value_scaled_x', 'value_scaled_y']].min(axis=1)) * 100
        
        merged_df1['variation']=((merged_df1[['value_scaled_x', 'value_scaled_y']].max(axis=1) - merged_df1[['value_scaled_x', 'value_scaled_y']].min(axis=1)) / merged_df1[['value_scaled_x', 'value_scaled_y']].min(axis=1)) * 100

        
        # print(merged_df)
        # print(merged_df1)
            
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        for ind,row in merged_df.iterrows():
            if  execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                diff.append(float(round(row['variation'])))
                peos.append(row['fiscalYear'])
                dataItemIds.append(row['dataItemId'])
                FQx.append(row['fiscalQuarter_x'])
                FQy.append(row['fiscalQuarter_y'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                perc.append(float(round(row['variation'])))

        for ind,row in merged_df1.iterrows():
            if  execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                diff.append(float(round(row['variation'])))
                peos.append(row['fiscalYear'])
                dataItemIds.append(row['dataItemId'])
                FQx.append(row['fiscalQuarter_x'])
                FQy.append(row['fiscalQuarter_y'])
                tid.append(row['tradingItemId']) 
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                perc.append(float(round(row['variation'])))  
                    
        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
            
        # print(diff_df)

        if len(diff_df)>0:
            diff_df['peocomb']=diff_df[['fiscalYear','dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['fiscalYear','dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp0 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed["value"] != "")&(extractedData_parsed['value'].notnull())&(extractedData_parsed["periodTypeId"] == 1))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','tradingItemName','peocomb']].drop_duplicates()
            
            temp0_revised=temp0.dropna()

            for ind, row in temp0_revised.iterrows():
                result = {"highlights": [], "error": "Quarters value should not be greater than or equal to Fiscal year or Semi Annual"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['description'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":'NA', "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)                           
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_01(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    list1 = get_dataItemIds_list('TAG1', parameters)  # scale applicable data items
    list2 = get_dataItemIds_list('TAG2', parameters)  # currency applicable data items
    list3 = get_dataItemIds_list('TAG3', parameters)  # trading item applicable data items
    list4 = get_dataItemIds_list('TAG4', parameters)  # Parent flag applicable data items
    list5 = get_dataItemIds_list('LHSdataItemIds', parameters)  # GAAP applicable data items

    try:

        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['scaleId'] == -1)|(extractedData_parsed['scaleId'] == "")|(~(extractedData_parsed['scaleId'].notnull()))))][['dataItemId','peo',  'value',  'team', 'description']].drop_duplicates()

        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list2)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['currencyId'] == -1)|(extractedData_parsed['currencyId'] == "")|(~(extractedData_parsed['currencyId'].notnull()))))][['dataItemId', 'peo', 'value',  'team', 'description']].drop_duplicates()

        temp3 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list3)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['tradingItemId'] == -1)|(extractedData_parsed['tradingItemId'] == "")|(~(extractedData_parsed['tradingItemId'].notnull()))))][['dataItemId','peo', 'value', 'team', 'description']].drop_duplicates()

        temp4 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list4)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['parentFlag'] == "")|(~(extractedData_parsed['parentFlag'].notnull()))))][['dataItemId','peo',  'value',   'team', 'description']].drop_duplicates()

        temp5 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list5)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['accountingStandardDesc'] == "")|(~(extractedData_parsed['accountingStandardDesc'].notnull()))))][['dataItemId','peo',  'value',   'team', 'description']].drop_duplicates()

        if len(temp1)>0:
            temp1_revised=temp1.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],
                          "error": "Units not specified for the Data Item,"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp2)>0:

            temp2_revised=temp2.dropna()

            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Currency not specified for the Data Item,"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp3)>0:
           
            temp3_revised=temp3.dropna()

            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],
                          "error": "Trading item not specified for the Data Item,"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp4)>0:

            temp4_revised=temp4.dropna()

            for ind, row in temp4_revised.iterrows():
                result = {"highlights": [],
                          "error": "Parent flag not specified for the Data Item"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp5)>0:

            temp5_revised=temp5.dropna()

            for ind, row in temp5_revised.iterrows():
                result = {"highlights": [],
                          "error": "GAAP not specified for the Data Item"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation) 
def AG_02(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    list1 = get_dataItemIds_list('TAG1', parameters)  # scale not applicable data items(periodic)
    list2 = get_dataItemIds_list('TAG2', parameters)  # currency not applicable data items(periodic)
    list3 = get_dataItemIds_list('TAG3', parameters)  # trading item not applicable data items(periodic)

    try:

        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['scaleId'] != -1))][['dataItemId','peo',  'value',  'team', 'description']].drop_duplicates()

        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list2)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['currencyId'] != -1))][['dataItemId', 'peo', 'value',  'team', 'description']].drop_duplicates()

        temp3 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list3)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['tradingItemId'] != -1))][['dataItemId','peo', 'value', 'team', 'description']].drop_duplicates()

        if len(temp1)>0:
            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],
                          "error": "Units collected for not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)


        if len(temp2)>0:

            temp2_revised=temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Currency collected not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp3)>0:
           
            temp3_revised=temp3.dropna()
            
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],
                          "error": "Trading collected for not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": row['peo'], "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row['peo'],"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": row['peo'],"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_03(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    list1 = get_dataItemIds_list('TAG1', parameters)  # CAGR Guidance

    try:

        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['scaleId'] != -1))][['dataItemId', 'value',  'team', 'description']].drop_duplicates()

        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['currencyId'] != -1))][['dataItemId', 'value',  'team', 'description']].drop_duplicates()

        temp3 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & (extractedData_parsed['tradingItemId'] != -1))][['dataItemId', 'value', 'team', 'description']].drop_duplicates()

        temp4 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['parentFlag'] == "")|(~(extractedData_parsed['parentFlag'].notnull()))))][['dataItemId',  'value',   'team', 'description']].drop_duplicates()

        temp5 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(list1)) & (extractedData_parsed['value'] != "") &
                                      (extractedData_parsed['value'].notnull()) & ((extractedData_parsed['accountingStandardDesc'] == "")|(~(extractedData_parsed['accountingStandardDesc'].notnull()))))][['dataItemId',  'value',   'team', 'description']].drop_duplicates()

        if len(temp1)>0:
            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [],
                          "error": "Units collected for not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)


        if len(temp2)>0:

            temp2_revised=temp2.dropna()
            
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [],
                          "error": "Currency collected not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp3)>0:
           
            temp3_revised=temp3.dropna()
            
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],
                          "error": "Trading collected for not applicable data items"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp4)>0:

            temp4_revised=temp4.dropna()

            for ind, row in temp4_revised.iterrows():
                result = {"highlights": [],"error": "Parent flag not specified for the Data Item"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        if len(temp5)>0:

            temp5_revised=temp5.dropna()

            for ind, row in temp5_revised.iterrows():
                result = {"highlights": [],"error": "GAAP not specified for the Data Item"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]},"cell": {"peo": 'NA', "scale": 'NA', "value": row['value'],"currency": 'NA'}, "section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                result["checkGeneratedFor"] = {"statement": "", "tag": row['dataItemId'], "description": row["description"],"versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA',"diff": 'NA',"percent": 'NA'}
                result["checkGeneratedForList"] = [{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": "NA","fiscalYear": 'NA', "fiscalQuarter": 'NA', "peo": 'NA',"value": row["value"], "units": 'NA', "currency": 'NA',"tradingItemName": 'NA', "accountingStdDesc": 'NA',"parentConsolidatedFlag": "NA", "fiscalChainSeries": 'NA', "diff": 'NA', "percent": 'NA'}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_87(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBITDA
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT
    tag_list=get_dataItemIds_list('TAG1', parameters) #DA
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBITDA
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT
        DA_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag_list)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #DA
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
        
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
        if DA_df["dataItemId"].nunique()!=len(tag_list):
            DA_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        DA_df_peo_count=DA_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (DA_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=DA_df_peo_count[(DA_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=DA_df[DA_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            DA_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            DA_df=pd.concat([DA_df,DA_df_missing_data],ignore_index=True)            

        if (len(lhs_df)>0 & len(rhs_df)>0 & len(DA_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            DA_df["value_scaled"] = DA_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
        
        # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        #     lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA
        # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        #     rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBIT  
        # if (DA_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        #     DA_df=DA_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #DA

        # lhs_df['value_scaled']=lhs_df['value_scaled'].replace('',np.NaN)
        # rhs_df['value_scaled']=rhs_df['value_scaled'].replace('',np.NaN)
        # DA_df['value_scaled']=DA_df['value_scaled'].replace('',np.NaN)
        
        # if (lhs_df['value_scaled'].notna().any()) and (rhs_df['value_scaled'].notna().any()):
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
            # if (DA_df['value_scaled'].notna().any()):
        merged_DA_df = pd.merge(DA_df,merged_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        # print(merged_df)
        # print(merged_DA_df)

        dataItemIds=[]
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
    
        for ind,row in merged_DA_df.iterrows():
            if (row['value_scaled']!=0):
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        
        if len(diff_df)>0:
            
            diff_df['peocomb']=diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            for ind, row in temp1_revised.iterrows():            
                result = {"highlights": [], "error": "D&A Collected when EBITDA and EBIT are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
                
            for ind, row in temp2_revised.iterrows():            
                result = {"highlights": [], "error": "D&A Collected when EBITDA and EBIT are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors    

#Estimates Error Checks 
@add_method(Validation)
def AG_349(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBIT
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBITDA
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    try:
        companyid=filingMetadata['metadata']['companyId']
        docuement_date=pd.to_datetime(filingMetadata['metadata']['filingDate'])        
        result_date=pd.to_datetime(filingMetadata['metadata']['resultsAnnouncementDate'])
       
        if pd.notnull(result_date):            
            if float(((docuement_date)-(result_date)).days)<8.0:
    
                lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT
                rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
                    lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT
                extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
                lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
                    
                if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                    missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                    collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                    lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                    lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)        
              
                if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
                    rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
                rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
                    
                if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                    missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                    collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                    rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                    rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
                if (len(lhs_df)>0 & len(rhs_df)>0):
                    base_currency=lhs_df.currency.mode()[0]
                    lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                    rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                
                # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
                #     lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA
                # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
                #     rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()        
                
                # lhs_df['value_scaled']=lhs_df['value_scaled'].replace('',np.NaN)
                # rhs_df['value_scaled']=rhs_df['value_scaled'].replace('',np.NaN)

                # if (lhs_df['value_scaled'].notna().any()) and (rhs_df['value_scaled'].notna().any()):
                merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

                merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
                merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
                merged_df.dropna(inplace=True) 
                # print(merged_df)
                
                dataItemIds=[]
                dataItemIdy=[]
                peos=[]
                diff=[]
                perc=[]
                tid=[]
                parentflag=[]
                accounting=[]
                fyc=[]
                    

                for ind,row in merged_df.iterrows():
                    if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                        peos.append(row['peo'])
                        dataItemIds.append(row['dataItemId_x'])
                        dataItemIdy.append(row['dataItemId_y'])               
                        # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                        tid.append(row['tradingItemId'])
                        parentflag.append(row['parentFlag'])
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        diff.append(float(round(row['variation'])))
                        perc.append(float(round(row['variation'])))
                diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
                if len(diff_df)>0:
                    
                    diff_df['peocomb']=diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                    temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                    temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate']].drop_duplicates()       

                # temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                #         &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))) 
                #         |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                #         &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc))))]
        
                # temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (historicalData_parsed['peo'].isin(peos))&(historicalData_parsed['tradingItemId'].isin(tid))
                #         &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(accounting))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc)))  
                #         |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peo'].isin(peos))&(historicalData_parsed['tradingItemId'].isin(tid))
                #         &(historicalData_parsed['parentFlag'].isin(parentflag))&(historicalData_parsed['accountingStandardDesc'].isin(accounting))&(historicalData_parsed['fiscalChainSeriesId'].isin(fyc))))]

                    temp1_revised=temp1.dropna()
                    temp2_revised=temp2.dropna()
                    
                    for ind, row in temp1_revised.iterrows():            
                        result = {"highlights": [], "error": "EBIT is greater than EBITDA"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                        errors.append(result)
                        
                    for ind, row in temp2_revised.iterrows():            
                        result = {"highlights": [], "error": "EBIT is greater than EBITDA"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                        errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_351(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #REVENUE
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBIT

    try:
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)   
             
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #REVENUE
        
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        # print(lhs_df)

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            lhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['periodTypeId']==2)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #CAPEX
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['periodTypeId']==2)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
            
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['periodTypeId']==2)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['periodTypeId']==2)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if ((len(lhs_df)>0) & (len(rhs_df)>0)):

            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        
        
            peos=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            diff=[]
            perc=[]
    
            for ind,row in merged_df.iterrows():
                    if row['value_scaled_x']==0:
                        if row['value_scaled_y']>0:
                            peos.append(row['peo'])
                            tid.append(row['tradingItemId'])
                            parentflag.append(row['parentFlag'])
                            accounting.append(row['accountingStandardDesc']) 
                            fyc.append(row['fiscalChainSeriesId'])
                            diff='NA'
                            perc='NA'
        
            diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
            
            if len(diff_df)>0:
                
                diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                            
                temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                        |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
                

                temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)   

                temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                        |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()


                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()
                
                if len(temp1_revised)>0:
                    for ind, row in temp1_revised.iterrows():
                        result = {"highlights": [], "error": "Revenue Zero But EBIT Positive"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff":"NA", "percent": "NA"}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                        errors.append(result)
                    for ind, row in temp2_revised.iterrows():
                        result = {"highlights": [], "error": "Revenue Zero But EBIT Positive"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"],"diff":"NA", "percent": "NA"}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff":"NA", "percent": "NA"}]
                        errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_360(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #["!="]
    Threshold=get_parameter_value(parameters,'Min_Threshold')
    try:
        FQ = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())& (extractedData_parsed["periodTypeId"] == 2)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        
        FQ_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(FQ['dataItemId'])&(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())&(historicalData_parsed["periodTypeId"] == 2) &(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))] [['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        FY = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())&(extractedData_parsed["periodTypeId"] == 1)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        
        if len(FY)==0:
            FY = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) &(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())&(historicalData_parsed["periodTypeId"] ==1) & (historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        if ((len(FY)>0) & (len(FQ)>0|len(FQ_not_cap)>0)): 
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ_not_cap["value_scaled"] = FQ_not_cap.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
    

        FQ = pd.concat([FQ,FQ_not_cap])
        FQ_count = FQ.groupby(['dataItemId','fiscalYear','periodTypeId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']).agg(PEO_Count=('peo','count'),PEO_Sum=('value_scaled','sum')).reset_index()
        

        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        merged_df1=pd.merge(merged_df,FQ_count,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]    

        for ind,row in merged_df1.iterrows():
            if row['PEO_Count']==4:
                if row['fiscalQuarter_y']==4:
                    #if (row['PEO_Sum']-row['value_scaled_x'])>0:
                    if  execute_operator ((row['PEO_Sum']-row['value_scaled_x']),float(Threshold[0]),operator[0]):
                        if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[1]):
                            peos.append(row['fiscalYear'])
                            dataItemIds.append(row['dataItemId'])
                            FQx.append(row['fiscalQuarter_x'])
                            FQy.append(row['fiscalQuarter_y'])
                            tid.append(row['tradingItemId']) 
                            parentflag.append(row['parentFlag']) 
                            accounting.append(row['accountingStandardDesc']) 
                            fyc.append(row['fiscalChainSeriesId'])
                            diff='NA'
                            perc='NA'
           
        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb']=diff_df[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['periodTypeId']==1)&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['periodTypeId']==1)&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            # temp1 = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(dataItemIds) & extractedData_parsed['fiscalYear'].isin(peos)&(extractedData_parsed['periodTypeId']==1)&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()
            # temp2 = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(dataItemIds) & historicalData_parsed['fiscalYear'].isin(peos)&(historicalData_parsed['periodTypeId']==1)&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','filingDate','companyId','feedFileId']].drop_duplicates()
            
            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "DPS Q4 is Equal to FY, But Sum of Q1, Q2 & Q3 Greater than 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "DPS Q4 is Equal to FY, But Sum of Q1, Q2 & Q3 Greater than 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_373(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    operator = get_dataItemIds_list('Operation', parameters) 
    try:
        if filingMetadata['metadata']['latestPeriodType']=="FY":
            fy1year=int(filingMetadata['metadata']['latestPeriodYear'])+1
        else:
            fy1year=int(filingMetadata['metadata']['latestPeriodYear'])
            

        fy1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['fiscalYear']==fy1year)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
        

        temp=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(fy1['dataItemId']))&(historicalData_parsed['fiscalYear']==fy1year-1)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency','filingDate']].drop_duplicates()

        # print(fy1)
        # print(temp)
        
        max_fy0=temp.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear'])['filingDate'].max().reset_index()

        fy0=temp[(temp['filingDate'].isin(max_fy0['filingDate']))]
        
        
        if (len(fy1)>0 and len(fy0)>0):
            base_currency=fy0.currency.mode()[0]
            fy0["value_scaled"] = fy0.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            fy1["value_scaled"] = fy1.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
            
            merged_df=pd.merge(fy0,fy1,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')

            merged_df['multipled_value']=merged_df['value_scaled_y']*float(factor[0])

            merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
            merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
            merged_df.dropna(inplace=True)            
            # print(merged_df)

            dataItemIds=[]
            peox=[]
            peoy=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            diff=[]
            perc=[]
    
            for ind,row in merged_df.iterrows():
                if execute_operator(row['multipled_value'],row['value_scaled_x'],operator[0]):
                    dataItemIds.append(row['dataItemId'])   
                    peox.append(row['peo_x'])
                    peoy.append(row['peo_y'])
                    # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    # diff.append(difference)
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])                        
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
            

            diff_df=pd.DataFrame({"dataItemId":dataItemIds,'peo_x':peox,'peo_y':peoy,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

            if len(diff_df)>0:
                
                diff_df['peocomb1'] = diff_df[['dataItemId','peo_y','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                diff_df['peocomb2'] = diff_df[['dataItemId','peo_x','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb1'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb2'] = historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)


                temp1 = extractedData_parsed[((extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
                temp2 = historicalData_parsed[((historicalData_parsed['peocomb2'].isin(diff_df['peocomb2']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb2']].drop_duplicates()
                
                temp1_revised = temp1.dropna()
                temp2_revised = temp2.dropna()
                                
                for ind,row in temp1_revised.iterrows():                    
                    result = {"highlights": [], "error": "Revenue guidance High FY+1*10 is less than guidance of FY"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
                                    
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Revenue guidance High FY+1*10 is less than guidance of FY"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb2']==row["peocomb2"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb2']==row["peocomb2"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb2']==row["peocomb2"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb2']==row["peocomb2"]]['perc'].iloc[0]}]
                    errors.append(result)                
            
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors
    # errors = []
    # dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    # factor=get_parameter_value(parameters,'MultiplicationFactor')
    # operator = get_dataItemIds_list('Operation', parameters) #["!="] 
    # try:
    #     currentyear=filingMetadata['metadata']['latestPeriodYear']
    #     latestperiod=filingMetadata['metadata']['latestPeriodType']
        
    #     # print(latestperiod)

    #     FY_1 = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed['fiscalYear']==int(currentyear)+1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['periodTypeId']==1)][['dataItemId','peo','scaleId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()


    #     FY_0 = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(FY_1['dataItemId']))&(historicalData_parsed['fiscalYear']==int(currentyear))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['periodTypeId']==1)][['dataItemId','peo','scaleId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()

    #     if len(FY_1)>0 and len(FY_0)>0:
    #         base_currency=FY_0.currency.mode()[0]
    #         FY_0["value_scaled"] = FY_0.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
    #         FY_1["value_scaled"] = FY_1.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            

    #     merged_df=pd.merge(FY_0,FY_1,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
    #     merged_df['multipled_value']=merged_df['value_scaled_y']*float(factor[0])
        
    #     print(merged_df)

        
    #     dataItemIds=[]
    #     peos_x=[]
    #     peos_y=[]
    #     parentflag=[]
    #     accounting=[]
    #     tid=[]
    #     fyc=[]
    #     diff=[]
    #     perc=[]
        
        
    #     for ind,row in merged_df.iterrows():
    #         if execute_operator(row['multipled_value'],row['value_scaled_x'],operator[0]):
    #             dataItemIds.append(row['dataItemId'])
    #             peos_x.append(row['peo_x'])
    #             peos_y.append(row['peo_y'])
    #             tid.append(row['tradingItemId'])
    #             parentflag.append(row['parentFlag'])
    #             accounting.append(row['accountingStandardDesc'])
    #             fyc.append(row['fiscalChainSeriesId'])
    #             difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
    #             diff.append(difference)
    #             perc.append((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100)
    #     diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo_x":peos_x,"peo_y":peos_y,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

    #     if len(diff_df)>0:
            
    #         diff_df['peocomb1']=diff_df[['dataItemId','peo_y','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
    #         diff_df['peocomb2']=diff_df[['dataItemId','peo_x','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
    #         extractedData_parsed['peocomb1']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
    #         historicalData_parsed['peocomb2']=historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

    #         temp1=extractedData_parsed[(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
    #         temp2=historicalData_parsed[(historicalData_parsed['peocomb2'].isin(diff_df['peocomb2'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb2','versionId','filingDate','companyId','feedFileId']].drop_duplicates()
          
    #         temp1_revised=temp1.dropna()
    #         temp2_revised=temp2.dropna()

    #         for ind,row in temp1_revised.iterrows():
    #             result = {"highlights": [], "error": "Revenue guidance High FY+1*10 is less than guidance of FY"}
    #             result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
    #             result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
    #             result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
    #             errors.append(result)

    #         for ind, row in temp2_revised.iterrows():
    #             result = {"highlights": [], "error": "Revenue guidance High FY+1*10 is less than guidance of FY"}
    #             result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
    #             result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
    #             result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
    #             errors.append(result)               
           
    #     print(errors) 
    #     return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_34(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    #operator=get_tags_list('Operation', parameters) #"==,!="
    try:
        companyid=filingMetadata['metadata']['companyId']
        dataitempeo=extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemId_list))][['dataItemId','peo']].drop_duplicates()
        Actual = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','description','peo','value','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        Consensus = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemId_list)&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','description','peo','consValue','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()                         
        actualpeo=Actual[['dataItemId','peo']].drop_duplicates()


        if len(dataitempeo)>len(actualpeo):
            missedpeo=dataitempeo[~dataitempeo.isin(actualpeo)].dropna()


            Actual_hist = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(missedpeo['dataItemId']) &(historicalData_parsed['peo'].isin(missedpeo['peo']))
                                                    &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid))][['dataItemId','description','peo','value','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']]
 
            Actual1 = pd.concat([Actual,Actual_hist])

        merged_df=pd.merge(Actual1,Consensus,on=['peo','dataItemId','description','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId'],how='inner')

        #collected tags
        peos_x=[]
        dataItemIds_x=[]
        tid_x=[]
        parentflag_x=[]
        accounting_x=[]
        fyc_x=[]
                  
        for ind,row in merged_df.iterrows():
            if (row['value']!="" and row['consValue']!=""):
                peos_x.append(row['peo'])
                dataItemIds_x.append(row['dataItemId'])
                tid_x.append(row['tradingItemId']) 
                parentflag_x.append(row['parentFlag']) 
                accounting_x.append(row['accountingStandardDesc']) 
                fyc_x.append(row['fiscalChainSeriesId'])

        diff_df=pd.DataFrame({"dataItemId":dataItemIds_x,"peo":peos_x,"parentFlag":parentflag_x,'tradingItemId':tid_x,"accountingStandardDesc":accounting_x,"fiscalChainSeriesId":fyc_x})

        #not collected tags
        peos_y=[]
        dataItemIds_y=[]
        tid_y=[]
        parentflag_y=[]
        accounting_y=[]
        fyc_y=[]
        
        for ind,row in Consensus.iterrows():
            if ~((row['dataItemId']==diff_df['dataItemId']) & ( row['peo']==diff_df['peo']) & ( row['tradingItemId']==diff_df['tradingItemId'])
                 &( row['parentFlag']==diff_df['parentFlag'])&( row['accountingStandardDesc']==diff_df['accountingStandardDesc'])&( row['fiscalChainSeriesId']==diff_df['fiscalChainSeriesId'])).any():               
                peos_y.append(row['peo'])
                dataItemIds_y.append(row['dataItemId'])
                tid_y.append(row['tradingItemId']) 
                parentflag_y.append(row['parentFlag'])
                accounting_y.append(row['accountingStandardDesc'])
                fyc_y.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                
        diff_df1=pd.DataFrame({"dataItemId":dataItemIds_y,"peo":peos_y,"parentFlag":parentflag_y,'tradingItemId':tid_y,"accountingStandardDesc":accounting_y,"fiscalChainSeriesId":fyc_y,"diff":diff,"perc":perc})

        if len(diff_df1)>0:
            
            diff_df1['peocomb']=diff_df1[['dataItemId','peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        

            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df1['peocomb'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            

            for ind, row in temp1.iterrows():   
                result = {"highlights": [], "error": "Consensus available but the actual was not captured"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df1[diff_df1['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df1[diff_df1['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df1[diff_df1['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
       
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_32(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #['>]
    variation=get_parameter_value(parameters,'Min_Threshold') #100%
    
    try:
        yesscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']!=-1)&(extractedData_parsed['currencyId']!=-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        volume = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']!=-1)&((extractedData_parsed['currencyId']==-1)|(extractedData_parsed['currencyId']==0)))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        noscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']==-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if len(yesscale)>0:
            yesscale['consValue_scaled'] = yesscale.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
            
            base_currency=yesscale.consCurrency.mode()[0]
            yesscale["value_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            yesscale["consValue_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
        
        if len(volume)>0:
            volume['consValue_scaled'] = volume.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
    
        if len(noscale)>0: 
            noscale['value_scaled'] = pd.to_numeric(noscale['value_scaled'],errors = 'coerce')
            noscale['consValue_scaled'] = pd.to_numeric(noscale['consValue'],errors = 'coerce')

        if (len(yesscale)>0 or len(noscale)>0 or len(volume)>0):
            frames = [yesscale, volume,noscale]
            temp = pd.concat(frames)[['dataItemId','peo','value_scaled','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId','consValue_scaled']]
            
            temp['consensusvariation']=((abs(((temp['value_scaled']).astype(float))-((temp['consValue_scaled']).astype(float))))/(abs(temp[['value_scaled','consValue_scaled']])).min(axis=1))*100
            
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)


            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
                    
            for ind,row in temp.iterrows():                
                if execute_operator(row['consensusvariation'],float(variation[0]),operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(float(round(row['consensusvariation'])))
                    perc.append(float(round(row['consensusvariation'])))
                    
            diff_df=pd.DataFrame({"peo":peos,"dataItemId":dataItemIds,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

            # print(diff_df)
                        
            if len(diff_df)>0:
                diff_df['peocomb']=diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                # diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                # extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                temp1=extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()    

                # temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) & (extractedData_parsed['tradingItemId'].isin(tid))
                #         &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))] 
                
                temp1_revised=temp1.dropna()

                for ind, row in temp1_revised.iterrows():        
                    result = {"highlights": [], "error": "Guidance not in line with consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]                              
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors
    
#Estimates Error Checks 
@add_method(Validation)
def AG_324(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #Low,Mid
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters) #High
    operator=get_dataItemIds_list('Operation', parameters)
    
    # print(left_dataItemId_list)
    
    try:
        datacomb=list(zip(left_dataItemId_list,right_dataItemId_list))
        comparable=pd.DataFrame(datacomb,columns=['dataitem1','dataitem2'])
        
        # print(comparable)
        
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1) 
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','estimatePeriodId']].drop_duplicates()
        rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&
                                        (historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))
                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','estimatePeriodId']].drop_duplicates()
        
        # print(lhs_df)
        # print(rhs_df)
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','fiscalYear','periodTypeId','tradingItemId'])


        comparable['compressed']=comparable.apply(lambda x:'%s%s' % (x['dataitem1'],x['dataitem2']),axis=1)
        merged_df['compressed']=merged_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['dataItemId_y']),axis=1)

        key_cols = ['compressed']
        
        dataitem_combination=merged_df.merge(comparable.loc[:, comparable.columns.isin(key_cols)])
        
        
        peos=[]
        combineddataitem=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]
        
        for ind,row in dataitem_combination.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                peos.append(row['peo'])
                combineddataitem.append(row['compressed'])
        diff_df=pd.DataFrame({"diff":diff,"perc":perc,'peo':peos,"parentFlag":parentflag,"tradingItemId":tid,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

        if len(diff_df)>0:
            
            diff_df['peocomb1']=diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2']=diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1']=extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1']=historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            # diff_df['peocomb1']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['peo']),axis=1)
            
            # extractedData_parsed['peocomb1']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        
            temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName',"peocomb1"]].drop_duplicates()
            

            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName',"peocomb1",'versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised=temp1.dropna()            
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Previous guidance collected in High but the same value collected in Low/Mid in the current document"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
            for ind, row in temp2_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Previous guidance collected in High but the same value collected in Low/Mid in the current document"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)                            
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_328(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    
    try:
        datacomb=list(zip(left_dataItemId_list,right_dataItemId_list))
        comparable=pd.DataFrame(datacomb,columns=['dataitem1','dataitem2'])
                
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','estimatePeriodId']].drop_duplicates()
        rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                        &(historicalData_parsed['estimatePeriodId'].isin(lhs_df['estimatePeriodId']))&(historicalData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','estimatePeriodId']].drop_duplicates()
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['parentFlag','peo','accountingStandardDesc','fiscalChainSeriesId','fiscalYear','periodTypeId','tradingItemId'])

        comparable['compressed']=comparable.apply(lambda x:'%s%s' % (x['dataitem1'],x['dataitem2']),axis=1)        

        merged_df['compressed']=merged_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['dataItemId_y']),axis=1)


        peos=[]
        dataItemIds_x=[]
        dataItemIds_y=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]
        
        for ind,row in merged_df.iterrows():
            if (row['compressed']==comparable['compressed']).any():
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    if execute_operator(row['dataItemId_x'],row['dataItemId_y'],operator[1]):
                        dataItemIds_x.append(row['dataItemId_x'])
                        dataItemIds_y.append(row['dataItemId_y'])
                        tid.append(row['tradingItemId']) 
                        parentflag.append(row['parentFlag']) 
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        diff='NA'
                        perc='NA'
                        peos.append(row['peo'])
        diff_df=pd.DataFrame({"dataItemId_x":dataItemIds_x,'dataItemId_y':dataItemIds_y,"diff":diff,"perc":perc,'peo':peos,"parentFlag":parentflag,"tradingItemId":tid,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

        if len(diff_df)>0:
            
            diff_df['peocomb1']=diff_df[['dataItemId_x','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            diff_df['peocomb2']=diff_df[['dataItemId_y','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                        
            # diff_df['peocomb1']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['peo']),axis=1)
            # diff_df['peocomb2']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId_y'],x['peo']),axis=1)
            
            # extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            # historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
        
        
            temp1 = extractedData_parsed[(((extractedData_parsed['peocomb'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          | ((extractedData_parsed['peocomb'].isin(diff_df['peocomb2']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                                                                                                                                                                                                                                    
            temp2 = historicalData_parsed[(((historicalData_parsed['peocomb'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())) 
                                           | ((historicalData_parsed['peocomb'].isin(diff_df['peocomb2']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','feedFileId','companyId']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Same values captured in opposite flavors (Current Vs Previous documents)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)

            for ind, row in temp2_revised.iterrows():
                if row['value']!=0:
                    result = {"highlights": [], "error": "Same values captured in opposite flavors (Current Vs Previous documents)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                    errors.append(result)                    
                                
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_337(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    operator=get_dataItemIds_list('Operation', parameters) #"=="
    
    try:

        currentperiod = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="A")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','scaleId','value_scaled','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()


        comparableperiod = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(currentperiod['dataItemId']))                                                                
                                                    &(historicalData_parsed['fiscalYear'].isin(currentperiod['fiscalYear']-1))&(historicalData_parsed['fiscalQuarter'].isin(currentperiod['fiscalQuarter']))&(historicalData_parsed['periodTypeId'].isin(currentperiod['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','scaleId','value_scaled','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        
        merged_df=pd.merge(currentperiod,comparableperiod,on=['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])
        
        
        dataItemIds=[]
        peos_x=[]
        peos_y=[]
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]
        diff=[]
        perc=[]

        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'], row['value_scaled_y'], operator[0]):                    
                dataItemIds.append(row['dataItemId'])
                peos_x.append(row['peo_x'])
                peos_y.append(row['peo_y'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff="NA"
                perc="NA"

        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo_x":peos_x,"peo_y":peos_y,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})            

        if len(diff_df)>0:
            diff_df['peocomb']=diff_df[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)        
            extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['peo'].isin(peos_x))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp1_revised=temp1.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Current PEO and comparable PEO have same values"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)                                
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_329(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag1_list=get_dataItemIds_list('TAG1', parameters) # NI GAAP
    tag2_list=get_dataItemIds_list('TAG2', parameters) # NI Normalized
    tag3_list=get_dataItemIds_list('TAG3', parameters) # EPS GAAP
    tag4_list=get_dataItemIds_list('TAG4', parameters) # EPS Normalized
    operator = get_dataItemIds_list('Operation', parameters) #["=="],["!="]
    try:
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)

        tag1_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() # NI GAAP
        if len(tag1_df)==0:
            tag1_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        tag2_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI Normalized
        if len(tag2_df)==0:
            tag2_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                
        if tag1_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag1_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(tag1_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag1_df=pd.concat([tag1_df,tag1_df_missing_data],ignore_index=True)


        if tag2_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag2_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(tag2_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)
       
        
        merged1_df=pd.merge(tag1_df,tag2_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged1_df['variation']=((merged1_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged1_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged1_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged1_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged1_df.dropna(inplace=True)
        
        tag3_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag3_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI GAAP
        if len(tag3_df)==0:
            tag3_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="") & (historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        tag4_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag4_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI Normalized
        if len(tag3_list)==0:
            tag3_list=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="") &(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                
        if tag3_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag3_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="") & ~((historicalData_parsed['peo'].isin(tag3_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            tag3_df=pd.concat([tag3_df,tag3_df_missing_data],ignore_index=True)


        if tag4_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
           tag4_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="") & ~((historicalData_parsed['peo'].isin(tag4_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
           tag4_df=pd.concat([tag4_df,tag4_df_missing_data],ignore_index=True)           


        
        merged2_df=pd.merge(tag3_df,tag4_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged2_df['variation']=((merged2_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged2_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged2_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged2_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged2_df.dropna(inplace=True)
      
                            
        peos=[]
        diff=[]
        perc=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        for ind,row in merged1_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                for ind,row in merged2_df.iterrows():
                    if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1]):
                        peos.append(row['peo'])
                        parentflag.append(row['parentFlag']) 
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])                              
                        diff.append(float(round(row['variation'])))
                        perc.append(float(round(row['variation'])))

        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1']=diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)   
            # diff_df['peocomb2']=diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)       
            extractedData_parsed['peocomb1']=extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1']=historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          |((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                                          |((extractedData_parsed['dataItemId'].isin(tag3_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                                          |((extractedData_parsed['dataItemId'].isin(tag4_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())) 
                                          |((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))
                                          |((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))
                                          |((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
            

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Value duplicated for Net Income but not for EBT "}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)

            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Value duplicated for Net Income but not for EBT "}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)           
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors    

#Estimates Error Checks 
@add_method(Validation)
def AG_330(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag1_list=get_dataItemIds_list('TAG1', parameters) # NI GAAP
    tag2_list=get_dataItemIds_list('TAG2', parameters) # NI Normalized
    tag3_list=get_dataItemIds_list('TAG3', parameters) # EPS GAAP
    tag4_list=get_dataItemIds_list('TAG4', parameters) # EPS Normalized
    operator = get_dataItemIds_list('Operation', parameters) #["=="],["!="]
    try:
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)

        tag1_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() # NI GAAP
        if len(tag1_df)==0:
            tag1_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        tag2_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI Normalized
        if len(tag2_df)==0:
            tag2_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                
        if tag1_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag1_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(tag1_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag1_df=pd.concat([tag1_df,tag1_df_missing_data],ignore_index=True)


        if tag2_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag2_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(tag2_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
            tag2_df=pd.concat([tag2_df,tag2_df_missing_data],ignore_index=True)
       
        
        merged1_df=pd.merge(tag1_df,tag2_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged1_df['variation']=((merged1_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged1_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged1_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged1_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged1_df.dropna(inplace=True)         
        
        tag3_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag3_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI GAAP
        if len(tag3_df)==0:
            tag3_df=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="") & (historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        tag4_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag4_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] # NI Normalized
        if len(tag3_list)==0:
            tag3_list=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="") &(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                
        if tag3_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            tag3_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="") & ~((historicalData_parsed['peo'].isin(tag3_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            tag3_df=pd.concat([tag3_df,tag3_df_missing_data],ignore_index=True)


        if tag4_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
           tag4_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="") & ~((historicalData_parsed['peo'].isin(tag4_df['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
           tag4_df=pd.concat([tag4_df,tag4_df_missing_data],ignore_index=True)           


        
        merged2_df=pd.merge(tag3_df,tag4_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged2_df['variation']=((merged2_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged2_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged2_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged2_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged2_df.dropna(inplace=True)        
        # print(merged1_df)
        # print(merged1_df)
                            
        peos=[]
        diff=[]
        perc=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        for ind,row in merged1_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                for ind,row in merged2_df.iterrows():
                    if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1]):
                        peos.append(row['peo'])
                        parentflag.append(row['parentFlag']) 
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])                              
                        diff.append(float(round(row['variation'])))
                        perc.append(float(round(row['variation'])))

        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1']=diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)        
            extractedData_parsed['peocomb1']=extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1']=historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(tag1_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          |((extractedData_parsed['dataItemId'].isin(tag2_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                                          |((extractedData_parsed['dataItemId'].isin(tag3_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                                          |((extractedData_parsed['dataItemId'].isin(tag4_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
            
            temp2 = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(tag1_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))) 
                                          |((historicalData_parsed['dataItemId'].isin(tag2_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))
                                          |((historicalData_parsed['dataItemId'].isin(tag3_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))
                                          |((historicalData_parsed['dataItemId'].isin(tag4_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
            

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Value duplicated for EPS but not for Net Income"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result) 

            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Value duplicated for EPS but not for Net Income"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['description'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)            
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors  

#Estimates Error Checks 
@add_method(Validation)
def AG_335(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters) # ">"
    Threshold=get_parameter_value(parameters,'Max Threshold') # 1
    
    try:
        
        DataItem = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")&(extractedData_parsed['currencyId']!=-1))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
        
        DataItem1 = DataItem.groupby(['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])['currency'].nunique().reset_index(name='currencycount') 
       
        
        peos = []
        parentflag=[]
        accounting=[]
        tid=[]
        fyc=[]
        diff = []
        perc = []

        for ind, row in DataItem1.iterrows():
            if execute_operator(abs(row['currencycount']),float(Threshold[0]),operator[0]):                
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff = 'NA'
                perc = 'NA'

        diff_df = pd.DataFrame({"peo": peos,"diff": diff, "perc": perc,"parentFlag":parentflag,"tradingItemId":tid,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

        if len(diff_df)>0:        
            base_currency=DataItem.currency.mode()[0]
            diff_df['peocomb']=diff_df[['peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb']=extractedData_parsed[['peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            
            temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) &(extractedData_parsed['value'] != "") &(extractedData_parsed['currencyId'] != -1) &(extractedData_parsed['value'].notnull()) &(extractedData_parsed['currency'] != base_currency))][['dataItemId', 'peo', 'estimatePeriodId', 'value', 'parentFlag','accountingStandardDesc', 'tradingItemId', 'fiscalChainSeriesId','scale', 'currency', 'team', 'description', 'fiscalYear','fiscalQuarter', 'tradingItemName', 'peocomb']].drop_duplicates()

            temp1_revised = temp1.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "The different currencies are collected for different data Items in the same document and for the same PEO"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff":"NA", "percent": "NA" }]
                errors.append(result)
          
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors  

#Estimates Error Checks 
@add_method(Validation)
def AG_334(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold')
    try:
        current = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].drop_duplicates()
        
        current['filingDate'] = filingMetadata['metadata']['filingDate']
        
        history = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(current['dataItemId']))
                                        &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','filingDate']].drop_duplicates()
        
        merged_df=pd.merge(current,history,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')

        merged_df['date_diff']=(pd.to_datetime(merged_df['filingDate_x'])-pd.to_datetime(merged_df['filingDate_y'])).dt.days

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)         

        dataItemIds=[]
        diff=[]
        perc=[]
        peosx=[]
        peosy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        prefilingdate=[]
        
        for ind,row in merged_df.iterrows():
            if execute_operator(abs(row['date_diff']),float(Threshold[0]),operator[0]):
                if (execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1]) and execute_operator(row['peo_x'],row['peo_y'],operator[2])):
                    dataItemIds.append(row['dataItemId'])               
                    # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
                    peosx.append(row['peo_x'])
                    peosy.append(row['peo_y'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId']) 
                    prefilingdate.append(row['filingDate_y'])
        
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc,"peo_x":peosx,"peo_y":peosy,"filingDate_y":prefilingdate,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

        if len(diff_df)>0:
            diff_df['currentpeo'] = diff_df[['dataitemId','peo_x','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            diff_df['prevpeo'] = diff_df[['dataitemId','peo_y','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            diff_df['prefiling'] = diff_df[['dataItemId','filingDate_y']].astype(str).apply(lambda x: ''.join(x),axis=1)
            

            extractedData_parsed['currentpeo'] = extractedData_parsed[['dataitemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['prevpeo'] = historicalData_parsed[['dataitemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['prefiling'] = historicalData_parsed[['dataItemId','filingDate']].astype(str).apply(lambda x: ''.join(x),axis=1)
    
            temp1 = extractedData_parsed[((extractedData_parsed['currentpeo'].isin(diff_df['currentpeo']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','currentpeo']].drop_duplicates()
            
            
            temp2 = historicalData_parsed[((historicalData_parsed['prevpeo'].isin(diff_df['prevpeo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['prefiling'].isin(diff_df['prefiling']))&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','prevpeo']].drop_duplicates()
    
            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():                 
                result = {"highlights": [], "error": "Guidance values are matching with different periods for same filing date documents"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['currentpeo']==row["currentpeo"]]['diff'].iloc[0], "percent": diff_df[diff_df['currentpeo']==row["currentpeo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['currentpeo']==row["currentpeo"]]['diff'].iloc[0], "percent": diff_df[diff_df['currentpeo']==row["currentpeo"]]['perc'].iloc[0]}]
                errors.append(result)
    
            for ind, row in temp2_revised.iterrows():                 
                result = {"highlights": [], "error": "Guidance values are matching with different periods for same filing date documents"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['prevpeo']==row["prevpeo"]]['diff'].iloc[0], "percent": diff_df[diff_df['prevpeo']==row["prevpeo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['prevpeo']==row["prevpeo"]]['diff'].iloc[0], "percent": diff_df[diff_df['prevpeo']==row["prevpeo"]]['perc'].iloc[0]}]
                errors.append(result)
            
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_339(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    ni_gaap=get_dataItemIds_list('TAG1', parameters) #ni_gaap_df
    ni_nor=get_dataItemIds_list('TAG2', parameters) #ni_nor_df
    eps_gaap=get_dataItemIds_list('TAG3', parameters) #eps_gaap_df
    eps_nor=get_dataItemIds_list('TAG4', parameters) #eps_nor_df
    operator=get_dataItemIds_list('Operation', parameters) #['>','<']
    try:
        companyid=filingMetadata['metadata']['companyId']
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        ni_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        ni_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(ni_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                         
        eps_gaap_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_gaap)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()
        eps_nor_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(eps_nor)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']].drop_duplicates()                       
        
      
        current_peo_count=extractedData_parsed['peo'].nunique()
        if len(ni_gaap_df['dataItemId'])==0:
            ni_gaap_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_gaap) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((ni_gaap_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            ni_gaap_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_gaap_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&~((historicalData_parsed['peo'].isin(ni_gaap_df['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            ni_gaap_df=pd.concat([ni_gaap_df,ni_gaap_df_missing_data],ignore_index=True)
            
        if len(ni_nor_df['dataItemId'])==0:
            ni_nor_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_nor) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((ni_nor_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            ni_nor_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(ni_nor_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)&~((historicalData_parsed['peo'].isin(ni_nor_df['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            ni_nor_df=pd.concat([ni_nor_df,ni_nor_df_missing_data],ignore_index=True)
            
        if len(eps_gaap_df['dataItemId'])==0:
            eps_gaap_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_gaap) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((eps_gaap_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            eps_gaap_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_gaap_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&~((historicalData_parsed['peo'].isin(eps_gaap_df['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            eps_gaap_df=pd.concat([eps_gaap_df,eps_gaap_df_missing_data],ignore_index=True)
            
        if len(eps_nor_df['dataItemId'])==0:
            eps_nor_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_nor) 
                                               &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))
                                               &(historicalData_parsed['periodTypeId'].isin(extractedData_parsed['periodTypeId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        elif ((eps_nor_df.groupby(['dataItemId'])['peo'].nunique())<current_peo_count).any():
            eps_nor_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(eps_nor_df['dataItemId'])&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)&~((historicalData_parsed['peo'].isin(eps_nor_df['peo'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear']]
            eps_nor_df=pd.concat([eps_nor_df,eps_nor_df_missing_data],ignore_index=True)
        

        ni_merged_df=pd.merge(ni_gaap_df,ni_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        eps_merged_df=pd.merge(eps_gaap_df,eps_nor_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')
        

        
        merged_df=pd.merge(ni_merged_df,eps_merged_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear'],how='inner')


        peos=[]
        diff=[]
        perc=[]
        parentflag=[]
        accounting=[]
        fyc=[]
       
        for ind,row in merged_df.iterrows():
            if ((execute_operator(row['value_scaled_x_x'],row['value_scaled_y_x'],operator[0]) & execute_operator(row['value_scaled_x_y'],row['value_scaled_y_y'],operator[1])) | (execute_operator(row['value_scaled_x_x'],row['value_scaled_y_x'],operator[1]) & execute_operator(row['value_scaled_x_y'],row['value_scaled_y_y'],operator[0]))):                
                peos.append(row['peo'])
                diff='NA'
                perc='NA'
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:

            diff_df['peocomb1'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(ni_gaap))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(ni_nor)) & (extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(eps_gaap)) & (extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['tradingItemId'].isin(eps_merged_df['tradingItemId']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))    
                    |((extractedData_parsed['dataItemId'].isin(eps_nor)) & (extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['tradingItemId'].isin(eps_merged_df['tradingItemId']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
            
            temp1_revised=temp1.dropna()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)   
                                                                                                                                                                                                                
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(ni_gaap)) & (historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['dataItemId'].isin(ni_nor)) & (historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))
                    |((historicalData_parsed['dataItemId'].isin(eps_gaap)) & (historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['tradingItemId'].isin(eps_merged_df['tradingItemId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))
                    |((historicalData_parsed['dataItemId'].isin(eps_nor)) & (historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['tradingItemId'].isin(eps_merged_df['tradingItemId']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
            

            temp2_revised=temp2.dropna()

            
            if len(temp1_revised)>0:

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "NI GAAP > NI Normalized but EPS GAAP < EPS Normalized or vice versa"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "NI GAAP > NI Normalized but EPS GAAP < EPS Normalized or vice versa"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)            
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_344(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #FCF Guidance
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #CFO Guidance
    operator=get_dataItemIds_list('Operation', parameters)  #['>']
    
    try:
        currentyear=filingMetadata['metadata']['latestPeriodYear']
        companyid=filingMetadata['metadata']['companyId']
        
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['fiscalYear']==int(float(currentyear))+1)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','currency','fiscalYear','fiscalQuarter']].drop_duplicates()
        if len(lhs_df)==0:
            lhs_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['fiscalYear']==int(float(currentyear))+1)&(historicalData_parsed['periodTypeId']==1) & (historicalData_parsed['peo'].isin(extractedData_parsed['peo']))& (historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['dataItemFlag']=="G")&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        rhs_df=extractedData_parsed[(extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['fiscalYear']==int(float(currentyear))+1)&(extractedData_parsed['periodTypeId']==1)][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        if len(rhs_df)==0:
            rhs_df=historicalData_parsed[(historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['fiscalYear']==int(float(currentyear))+1)&(extractedData_parsed['periodTypeId']==1)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['dataItemFlag']=="G")&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]
  

        if lhs_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['fiscalYear']==int(float(currentyear))+1)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(lhs_df['peo'])))&(historicalData_parsed['dataItemFlag']=="G")&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)

    
        if rhs_df['peo'].nunique()<extractedData_parsed['peo'].nunique():
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['fiscalYear']==int(float(currentyear))+1)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(rhs_df['peo'])))&(historicalData_parsed['dataItemFlag']=="G")&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True) 

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[] 


        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
    
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
        
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)  

            temp2 = historicalData_parsed[(((historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "FCF Guidance is greater than CFO Guidance"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "FCF Guidance is greater than CFO Guidance"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_345(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EPSNormalized
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #CashEPS
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    try:
        companyid=filingMetadata['metadata']['companyId']
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
    
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EPSNormalized
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EPSNormalized
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
              
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        # lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EPSNormalized            
        # rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()  #CashEPS
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)
        
        # print(lhs_df)
        # print(rhs_df)
        # print(merged_df)
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
     
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        # print(diff_df)
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)            
  
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()



            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)   


            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
    
            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            if len(temp1_revised)>0:
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "EPS Normalized is greater than Cash EPS"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result) 
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "EPS Normalized is greater than Cash EPS"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result) 
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_349(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBIT
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBITDA
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    try:
        companyid=filingMetadata['metadata']['companyId']
        docuement_date=pd.to_datetime(filingMetadata['metadata']['filingDate'])        
        result_date=pd.to_datetime(filingMetadata['metadata']['resultsAnnouncementDate'])
        
       
        if pd.notnull(result_date):
            if float(((docuement_date)-(result_date)).days)<8.0:
                
                extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
                historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
                
                
                lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT
                rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
                
                if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
                    lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT
                extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
                lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
                    
                if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                    missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                    collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                    lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                    lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)        
              
                if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
                    rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
                rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
                    
                if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                    missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                    collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                    rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                    rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
                if (len(lhs_df)>0 & len(rhs_df)>0):
                    base_currency=lhs_df.currency.mode()[0]
                    lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                    rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
                

                merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

                merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
                merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
                merged_df.dropna(inplace=True)

                # print(merged_df)
                
                peos=[]
                diff=[]
                perc=[]
                tid=[]
                parentflag=[]
                accounting=[]
                fyc=[]
                    

                for ind,row in merged_df.iterrows():
                    if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                        peos.append(row['peo'])
                        tid.append(row['tradingItemId'])
                        parentflag.append(row['parentFlag'])
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        diff.append(float(round(row['variation'])))
                        perc.append(float(round(row['variation'])))
                diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
                
                # print(diff_df)
                if len(diff_df)>0:
                    
                    diff_df['peocomb1']=diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    # diff_df['peocomb2']=diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    extractedData_parsed['peocomb1']=extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    historicalData_parsed['peocomb1']=historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                    temp1 = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&extractedData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                                                 |(extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&extractedData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
 
                    temp1['peocompressed'] = extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                    historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1) 
 
                    temp2 = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~(historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))
                                                  |(historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~(historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()       

                    temp1_revised=temp1.dropna()
                    temp2_revised=temp2.dropna()
                    
                    # print(temp1_revised)
                    
                    for ind, row in temp1_revised.iterrows():            
                        result = {"highlights": [], "error": "EBIT is greater than EBITDA"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                        errors.append(result)
                        
                    for ind, row in temp2_revised.iterrows():            
                        result = {"highlights": [], "error": "EBIT is greater than EBITDA"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                        errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_353(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #NAVPS
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #BVPS
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    try:

        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #NAVPS
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value'].notnull())&(extractedData_parsed['value']!="")][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBITDA
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
      
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
            
        
        # print(lhs_df)
        # print(rhs_df)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        # lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #NAVPS
        # rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100

        # print(merged_df)
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]


        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        # print(diff_df)
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
   
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed'] = extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1) 

            temp2 = historicalData_parsed[((((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~(historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~(historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb1']].drop_duplicates()
        
            if len(temp1)>0:
                
                temp1_revised=temp1.dropna()
                
                for ind, row in temp1_revised.iterrows():                    
                    result = {"highlights": [], "error": "NAVPS is greater than BVPS"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
        
                temp2_revised=temp2.dropna()   
                 
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "NAVPS is greater than BVPS"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_376(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    operator = get_dataItemIds_list('Operation', parameters) 
    try:
        if filingMetadata['metadata']['latestPeriodType']=="FY":
            fy1year=int(filingMetadata['metadata']['latestPeriodYear'])+1
        else:
            fy1year=int(filingMetadata['metadata']['latestPeriodYear'])
            

        fy1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['fiscalYear']==fy1year)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency']].drop_duplicates()
        

        temp=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(fy1['dataItemId']))&(historicalData_parsed['fiscalYear']==fy1year-1)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','scale','value','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','currency','filingDate']].drop_duplicates()

        # print(fy1)
        # print(temp)
        
        max_fy0=temp.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear'])['filingDate'].max().reset_index()

        fy0=temp[(temp['filingDate'].isin(max_fy0['filingDate']))]
        
        
        if (len(fy1)>0 and len(fy0)>0):
            base_currency=fy0.currency.mode()[0]
            fy0["value_scaled"] = fy0.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
            fy1["value_scaled"] = fy1.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
            
            merged_df=pd.merge(fy0,fy1,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')


            merged_df['multipled_value']=merged_df['value_scaled_x']*float(factor[0])

            merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
            merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
            merged_df.dropna(inplace=True)

            dataItemIds=[]
            peox=[]
            peoy=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            diff=[]
            perc=[]
    
            for ind,row in merged_df.iterrows():
                if execute_operator(row['value_scaled_y'],row['multipled_value'],operator[0]):
                    dataItemIds.append(row['dataItemId'])   
                    peox.append(row['peo_x'])
                    peoy.append(row['peo_y'])
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])                        
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
            

            diff_df=pd.DataFrame({"dataItemId":dataItemIds,'peo_x':peox,'peo_y':peoy,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

            if len(diff_df)>0:
                
                diff_df['peocomb1'] = diff_df[['dataItemId','peo_y','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                diff_df['peocomb2'] = diff_df[['dataItemId','peo_x','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb1'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb2'] = historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)


                temp1 = extractedData_parsed[((extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
                temp2 = historicalData_parsed[((historicalData_parsed['peocomb2'].isin(diff_df['peocomb2']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb2']].drop_duplicates()
                
                temp1_revised = temp1.dropna()
                temp2_revised = temp2.dropna()
                                
                for ind,row in temp1_revised.iterrows():                    
                    result = {"highlights": [], "error": "Revenue High guidance for FY+1 10 times greater than FY+0 guidance"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
                                    
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Revenue High guidance for FY+1 10 times greater than FY+0 guidance"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb2']==row["peocomb2"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb2']==row["peocomb2"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb2']==row["peocomb2"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb2']==row["peocomb2"]]['perc'].iloc[0]}]
                    errors.append(result)                
            
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_352(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBT_GAAP, DA
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBITDA
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBT_GAAP, DA
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBT_GAAP, DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
     
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
 
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)
        # print(merged_df)

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        # print(diff_df)
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed'] = extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1) 

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb1']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "EBT(GAAP) + D&A =  EBITDA where EBT(GAAP) and EBIT are not same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "EBT(GAAP) + D&A =  EBITDA where EBT(GAAP) and EBIT are not same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_360(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #["!="]
    Threshold=get_parameter_value(parameters,'Min_Threshold')
    try:
        FQ = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list)&(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())& (extractedData_parsed["periodTypeId"] == 2)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        
        FQ_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(FQ['dataItemId'])&(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())&(historicalData_parsed["periodTypeId"] == 2) &(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))] [['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        FY = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())&(extractedData_parsed["periodTypeId"] == 1)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        
        if len(FY)==0:
            FY = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) &(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())&(historicalData_parsed["periodTypeId"] ==1) & (historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        if ((len(FY)>0) & (len(FQ)>0|len(FQ_not_cap)>0)): 
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ_not_cap["value_scaled"] = FQ_not_cap.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
    

        FQ = pd.concat([FQ,FQ_not_cap])
        FQ_count = FQ.groupby(['dataItemId','fiscalYear','periodTypeId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']).agg(PEO_Count=('peo','count'),PEO_Sum=('value_scaled','sum')).reset_index()
        

        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        merged_df1=pd.merge(merged_df,FQ_count,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]    

        for ind,row in merged_df1.iterrows():
            if row['PEO_Count']==4:
                if row['fiscalQuarter_y']==4:
                    #if (row['PEO_Sum']-row['value_scaled_x'])>0:
                    if  execute_operator ((row['PEO_Sum']-row['value_scaled_x']),float(Threshold[0]),operator[0]):
                        if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[1]):
                            peos.append(row['fiscalYear'])
                            dataItemIds.append(row['dataItemId'])
                            FQx.append(row['fiscalQuarter_x'])
                            FQy.append(row['fiscalQuarter_y'])
                            tid.append(row['tradingItemId']) 
                            parentflag.append(row['parentFlag']) 
                            accounting.append(row['accountingStandardDesc']) 
                            fyc.append(row['fiscalChainSeriesId'])
                            diff='NA'
                            perc='NA'
           
        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','fiscalYear','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb'])&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['periodTypeId']==1)&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            # temp1 = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(dataItemIds) & extractedData_parsed['fiscalYear'].isin(peos)&(extractedData_parsed['periodTypeId']==1)&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()
            # temp2 = historicalData_parsed[(historicalData_parsed['dataItemId'].isin(dataItemIds) & historicalData_parsed['fiscalYear'].isin(peos)&(historicalData_parsed['periodTypeId']==1)&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','filingDate','companyId','feedFileId']].drop_duplicates()
            
            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "DPS Q4 is Equal to FY, But Sum of Q1, Q2 & Q3 Greater than 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "DPS Q4 is Equal to FY, But Sum of Q1, Q2 & Q3 Greater than 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId":row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_361(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #[=="]

    try:
        FQ = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) & (extractedData_parsed["periodTypeId"] == 2)&(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        FQ_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(FQ['dataItemId']) & (historicalData_parsed["periodTypeId"] == 2)&(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull()) &(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))] [['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        FY = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) & (extractedData_parsed["periodTypeId"] == 1)&(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()


        if len(FY)==0:
            FY = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) & (historicalData_parsed["periodTypeId"] ==1)&(historicalData_parsed["value"] !="")&(historicalData_parsed["value"].notnull()) & (historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']] 

        if ((len(FY)>0) & (len(FQ)>0|len(FQ_not_cap)>0)): 
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ_not_cap["value_scaled"] = FQ_not_cap.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)


        FQ = pd.concat([FQ,FQ_not_cap])
 

        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[] 
        
        for ind,row in merged_df.iterrows():
            if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['fiscalYear'])
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                FQx.append(row['fiscalQuarter_x'])
                FQy.append(row['fiscalQuarter_y'])
                diff='NA'
                perc='NA'
           
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"fiscalYear":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb']) &(extractedData_parsed['periodTypeId']==1)& extractedData_parsed['fiscalYear'].isin(peos)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb']) &(historicalData_parsed['periodTypeId']==1)& historicalData_parsed['fiscalYear'].isin(peos)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Interest expense actual same for Fiscal Quarter and FY"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)                    
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Interest expense actual same for Fiscal Quarter and FY"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)  
                
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_362(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #["!="]

    try:
        Q4 = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())& (extractedData_parsed["periodTypeId"] == 2)&(extractedData_parsed["fiscalQuarter"] == 4)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        if len(Q4)==0:
            Q4 = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) & (historicalData_parsed["periodTypeId"] == 2)&(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull()) &(historicalData_parsed["fiscalQuarter"] == 4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))] [['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']]

        FY = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull()) & (extractedData_parsed["periodTypeId"] == 1)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
    
        if len(FY)==0:
            FY = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) &(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())& (historicalData_parsed["periodTypeId"] ==1) & (historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']] 

        if ((len(FY)>0) and (len(Q4)>0)): 
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            Q4["value_scaled"] = Q4.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)


        merged_df=pd.merge(Q4,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')
        
        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[] 
        
        for ind,row in merged_df.iterrows():
            if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['fiscalYear'])
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                FQx.append(row['fiscalQuarter_x'])
                FQy.append(row['fiscalQuarter_y'])
                diff='NA'
                perc='NA'

        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy})

        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb']) &(extractedData_parsed['periodTypeId']==1) &extractedData_parsed['fiscalYear'].isin(peos)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb']) &(historicalData_parsed['periodTypeId']==1)& historicalData_parsed['fiscalYear'].isin(peos)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Revenue Q4 actual > FY actual"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)                    
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Revenue Q4 actual > FY actual"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_363(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #[=="]

    try:
        FQ = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull())& (extractedData_parsed["periodTypeId"] == 2)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        FQ_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) &(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())& (historicalData_parsed["periodTypeId"] == 2) &(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))] [['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        

        FY = extractedData_parsed[extractedData_parsed['dataItemId'].isin(dataItemId_list) &(extractedData_parsed["value"] != "")&(extractedData_parsed["value"].notnull()) &(extractedData_parsed["periodTypeId"] == 1)][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()
        
        if len(FY)==0:
            FY = historicalData_parsed[historicalData_parsed['dataItemId'].isin(dataItemId_list) &(historicalData_parsed["value"] != "")&(historicalData_parsed["value"].notnull())& (historicalData_parsed["periodTypeId"] ==1) & (historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))][['dataItemId','peo','scale','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter','currency']].drop_duplicates()

        if ((len(FY)>0) & (len(FQ)>0|len(FQ_not_cap)>0)): 
            # print('yesss')
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ["value_scaled"] = FQ.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            FQ_not_cap["value_scaled"] = FQ_not_cap.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            

        FQ = pd.concat([FQ,FQ_not_cap])
 

        merged_df=pd.merge(FQ,FY,on=['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId'],how='inner')

        peos=[]
        diff=[]
        perc=[]
        dataItemIds=[]
        FQx=[]
        FQy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[] 
        
        for ind,row in merged_df.iterrows():
            if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['fiscalYear'])
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                FQx.append(row['fiscalQuarter_x'])
                FQy.append(row['fiscalQuarter_y'])
                diff='NA'
                perc='NA'
           
        diff_df=pd.DataFrame({"fiscalYear":peos,"diff":diff,"perc":perc,"dataItemId":dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,'fiscalQuarter_x':FQx,'fiscalQuarter_y':FQy})

        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['dataItemId','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])& extractedData_parsed['fiscalYear'].isin(peos) &(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())& (extractedData_parsed['periodTypeId']==1)&((extractedData_parsed['fiscalQuarter'].isin(FQx))|(extractedData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
            temp2 = historicalData_parsed[(historicalData_parsed['peocomb'].isin(diff_df['peocomb']) & historicalData_parsed['fiscalYear'].isin(peos)&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&((historicalData_parsed['fiscalQuarter'].isin(FQx))|(historicalData_parsed['fiscalQuarter'].isin(FQy))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "EBITDA FQ actual is equals to EBITDA FY"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)                    
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "EBITDA FQ actual is equals to EBITDA FY"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['diff'].iloc[0], "percent": diff_df[diff_df['fiscalYear']==row["fiscalYear"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_370(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    try:
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        # print(lhs_df)
        # print(rhs_df)
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured)) &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                    &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
            
        extracted_dataItemId_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')   
        

        if (lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            missed_peo_dataItemId=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
            
                            
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):

            rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                            &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
    
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')  

        if (rhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(rhs_df['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        lhs_df["valuesign"]=np.sign((lhs_df['value']).astype(float))

        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        
        rhs_df["valuesign"]=np.sign((rhs_df['value']).astype(float))

        
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'],how='inner')
        
        # print(merged_df)

        peos=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]

                
        for ind,row in merged_df.iterrows():
            if ((row['valuesign_x']!=0)&(row['valuesign_y']!=0)):
                if execute_operator(np.sign(row['valuesign_x']),np.sign(row['valuesign_y']),operator[0]):
                    peos.append(row['peo'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc}) 

        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                        | ((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)             
                                                                                                                                                                                                                            
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                                            | ((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Sign difference between ROE and EPS (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
            
            for ind, row in temp2_revised.iterrows():
                if row['value']!=0:
                    result = {"highlights": [], "error": "Sign difference between ROE and EPS (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result) 
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_371(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    try:
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured)) &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                    &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
            
        extracted_dataItemId_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')   
        

        if (lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():

            missed_peo_dataItemId=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
            
                            
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):

            rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                            &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
    
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')  

        if (rhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(rhs_df['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        lhs_df["valuesign"]=np.sign((lhs_df['value']).astype(float))

        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        
        rhs_df["valuesign"]=np.sign((rhs_df['value']).astype(float))

        
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'],how='inner')

        peos=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]

                
        for ind,row in merged_df.iterrows():
            if ((row['valuesign_x']!=0)&(row['valuesign_y']!=0)):
                if execute_operator(np.sign(row['valuesign_x']),np.sign(row['valuesign_y']),operator[0]):
                    peos.append(row['peo'])
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc}) 

        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                        | ((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)             
                                                                                                                                                                                                                            
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                                            | ((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Sign difference between ROE and Net Income (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
            
            for ind, row in temp2_revised.iterrows():
                if row['value']!=0:
                    result = {"highlights": [], "error": "Sign difference between ROE and Net Income (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result) 
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_372(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemId_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    try:
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured)) &(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                    &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
            
        extracted_dataItemId_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')   
        

        if (lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():

            missed_peo_dataItemId=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
            
                            
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemId_list):

            rhs_df = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                            &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId'])))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
    
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')  

        if (rhs_df_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                        &(historicalData_parsed['estimatePeriodId'].isin(extractedData_parsed['estimatePeriodId']))& ~((historicalData_parsed['peo'].isin(rhs_df['peo']))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','fiscalChainSeriesId']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True) 

        lhs_df["valuesign"]=np.sign((lhs_df['value']).astype(float))

        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        
        rhs_df["valuesign"]=np.sign((rhs_df['value']).astype(float))

        
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'])["valuesign"].sum().reset_index()
        

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'],how='inner')

        peos=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]
                
        for ind,row in merged_df.iterrows():
            if ((row['valuesign_x']!=0)&(row['valuesign_y']!=0)):
                if execute_operator(np.sign(row['valuesign_x']),np.sign(row['valuesign_y']),operator[0]):
                    peos.append(row['peo'])
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc}) 

        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                        | ((extractedData_parsed['dataItemId'].isin(right_dataItemId_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)            
                                                                                                                                                                                                                            
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                                            | ((historicalData_parsed['dataItemId'].isin(right_dataItemId_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','filingDate','companyId','feedFileId']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                if row['value']!=0:                    
                    result = {"highlights": [], "error": "Sign difference between ROA and Net Income (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
            
            for ind, row in temp2_revised.iterrows():
                if row['value']!=0:
                    result = {"highlights": [], "error": "Sign difference between ROA and Net Income (both flavors)"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result) 
        print(errors) 
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_383(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    try:
        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value','consValue','consScale','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
         
        # print(temp)                
        if len(temp)>0:
            temp['multipled_value']=((temp['value']).astype(float))*float(factor[0])
            # temp['multipled_value'] = temp['value'].astype(float) * factor[0]

            temp['variation']=((temp[['value','consValue']].max(axis=1)-temp[['value','consValue']].min(axis=1))/temp[['value','consValue']].min(axis=1))*100
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)             
            # print(temp)

                      
            dataItemIds=[]
            peos=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]

            for ind,row in temp.iterrows():
                if execute_operator(row['multipled_value'],float(row['consValue']),operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])               
                    # difference=float(row[['value','consValue']].max())-float(row[['value','consValue']].min())
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation']))) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
            
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised = temp1.dropna()

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "ROE Actual*5 <  ROE Consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_384(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    try:
        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value','consValue','consScale','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
             
        if len(temp)>0:
            temp['multipled_value']=((temp['value']).astype(float))*float(factor[0])
            # temp['multipled_value'] = temp['value'].astype(float) * factor[0]
            temp['variation']=((temp[['value','consValue']].max(axis=1)-temp[['value','consValue']].min(axis=1))/temp[['value','consValue']].min(axis=1))*100
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True) 
                      
            dataItemIds=[]
            peos=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]

            for ind,row in temp.iterrows():
                if execute_operator(row['multipled_value'],float(row['consValue']),operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])               
                    # difference=float(row[['value','consValue']].max())-float(row[['value','consValue']].min())
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation']))) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
            
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised = temp1.dropna()

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "ROA Actual*5 <  ROA Consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_354(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBIT, DA
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBITDA
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&((extractedData_parsed['periodTypeId']==2)|(extractedData_parsed['periodTypeId']==10))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT, DA
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&((extractedData_parsed['periodTypeId']==2)|(extractedData_parsed['periodTypeId']==10))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        # print(lhs_df)
        # print(rhs_df)

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&((historicalData_parsed['periodTypeId']==2)|(historicalData_parsed['periodTypeId']==10))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&((historicalData_parsed['periodTypeId']==2)|(historicalData_parsed['periodTypeId']==10))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
       
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&((historicalData_parsed['periodTypeId']==2)|(historicalData_parsed['periodTypeId']==10))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&((historicalData_parsed['periodTypeId']==2)|(historicalData_parsed['periodTypeId']==10))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        lhs_df['value_scaled'] = lhs_df.apply(lambda x: x['value_scaled']*(-1) if x['dataItemId']== left_dataItemIds_list[1] else x['value_scaled'], axis=1)
        
        # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==2).all()):
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA-EBIT
        # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()        
            
                    
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True) 
        # print(merged_df)
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
                
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
    
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()



            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)   

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb1']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            if len(temp1_revised)>0:
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "EBIT actual less DA actual equals to EBITDA Actual for latest quarter"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "EBIT actual less DA actual equals to EBITDA Actual for latest quarter"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_355(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBT_GAAP 
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBT_Norm
    tag_list=get_dataItemIds_list('TAG1', parameters) #Taxrate
    #operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #NI 
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBT
        ETR_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag_list)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #Taxrate

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
       
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
        if ETR_df["dataItemId"].nunique()!=len(tag_list):
            ETR_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        ETR_df_peo_count=ETR_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=ETR_df_peo_count[(ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=ETR_df[ETR_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            ETR_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            ETR_df=pd.concat([ETR_df,ETR_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0 & len(ETR_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            ETR_df["value_scaled"] = ETR_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        

        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBIT  
        ETR_df=ETR_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #DA

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_ETR_df = pd.merge(ETR_df,merged_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

     
        for ind,row in merged_ETR_df.iterrows():
            if (row['value_scaled']!=0):
                if row['value_scaled_x']<0 or row['value_scaled_y']<0:
                    # if row['value_scaled_y']<0:
                    peos.append(row['peo'])
                    # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
  
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb']].drop_duplicates()

            temp1_revised = temp1.dropna()
            
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Effective Tax Rate Collected When EBT(GAAP) or EBT Normalized < 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Effective Tax Rate Collected When EBT(GAAP) or EBT Normalized < 0"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_357(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBITDA, EBIT
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #DA*5
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    factor=get_parameter_value(parameters,'MultiplicationFactor') 
    try:
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)        
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT, EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT, EBITDA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))& (historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
        
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if ((lhs_df['dataItemId'].nunique()>1) & (len(rhs_df)>0)):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)

        lhs_df['value_scaled'] = lhs_df.apply(lambda x: x['value_scaled']*(-1) if x['dataItemId']== left_dataItemIds_list[1] else x['value_scaled'], axis=1)
    
    
        # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==2).all()):
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA-EBIT
        # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()        


        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_df['multipled_value']=merged_df['value_scaled_y']*float(factor[0])

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True) 
        
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        
        for ind,row in merged_df.iterrows():
            if (row['value_scaled_x']!=0) & (row['value_scaled_y']!=0):
                if execute_operator(abs(row['value_scaled_x']),row['multipled_value'],operator[0]):
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)            
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)            
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
    
    
            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
    
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb1']].drop_duplicates()
    

            
            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Difference of EBITDA and EBIT is 5 times greater than D&A  "}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Difference of EBITDA and EBIT is 5 times greater than D&A  "}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors 

#Estimates Error Checks 
@add_method(Validation)
def AG_358(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #CAPEX_Guidance_High
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #Maintenance_Capex_Guidance_High
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1) 
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['dataItemFlag']=='G')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #CAPEX_Guidance_High
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['dataItemFlag']=='G')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            lhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['dataItemFlag']=='G')&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CAPEX_Guidance_High
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['dataItemFlag']=='G')&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
      
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['dataItemFlag']=='G')&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['dataItemFlag']=='G')&(historicalData_parsed['value']!="")&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)

        # lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA-EBIT
        # rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index()        
    
        
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        
     
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)             
  
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb1']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Capex guidance High = Maintenance Capex guidance High"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Capex guidance High = Maintenance Capex guidance High"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_347(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #NI 
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #EBT
    tag_list=get_dataItemIds_list('TAG1', parameters) #Taxrate
    operator = get_dataItemIds_list('Operation', parameters) #["=="]
    try:
        companyid=filingMetadata['metadata']['companyId']
        
        lhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #NI 
        rhs_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBT
        ETR_df = extractedData_parsed[extractedData_parsed['dataItemId'].isin(tag_list)&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #Taxrate

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #DA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
        
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[historicalData_parsed['dataItemId'].isin(not_captured)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)
        
        if ETR_df["dataItemId"].nunique()!=len(tag_list):
            ETR_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(tag_list)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo']))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        ETR_df_peo_count=ETR_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')    
            
        if (ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=ETR_df_peo_count[(ETR_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=ETR_df[ETR_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            ETR_df_missing_data=historicalData_parsed[historicalData_parsed['dataItemId'].isin(missed_peo_tag)&(historicalData_parsed['peo'].isin(extractedData_parsed['peo'])) & ~((historicalData_parsed['peo'].isin(collected_peo['peo'])))&(historicalData_parsed['periodTypeId']==1)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid)][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            ETR_df=pd.concat([ETR_df,ETR_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0 & len(ETR_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            # ETR_df["value_scaled"] = ETR_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
        
        # if (lhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        lhs_df=lhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBITDA
        # if (rhs_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        rhs_df=rhs_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #EBIT  
        # if (ETR_df.groupby('peo').apply(lambda group: group['dataItemId'].nunique()==1).all()):
        ETR_df=ETR_df.groupby(['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'])['value_scaled'].sum().reset_index() #DA

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_ETR_df = pd.merge(ETR_df,merged_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]  
        
        for ind,row in merged_ETR_df.iterrows():
            if (row['value_scaled']!=0):
                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    peos.append(row['peo'])
                    # difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
   
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed']))))  
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) & (historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb']].drop_duplicates()

            temp1_revised = temp1.dropna()
            temp2_revised = temp2.dropna()

            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Effective Tax Rate collected when EBT and Net Income are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "Effective Tax Rate collected when EBT and Net Income are same"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_343(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters) #EBIT
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters) #Revenue*3
    operator = get_dataItemIds_list('Operation', parameters) #[">"]
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    try:
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT, EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT, EBITDA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
      
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)

        if (len(lhs_df)>0 & len(rhs_df)>0):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        
        # if ((len(lhs_df)>0) & (len(rhs_df)>0)):
        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_df['multipled_value']=merged_df['value_scaled_y']*float(factor[0])

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)
            
        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]

        
        for ind,row in merged_df.iterrows():
            if execute_operator(row['value_scaled_x'],row['multipled_value'],operator[0]):
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff.append(float(round(row['variation'])))
                perc.append(float(round(row['variation'])))
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        
        if len(diff_df)>0:
            
            diff_df['peocomb1'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            # diff_df['peocomb2'] = diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['peocomb1'] = extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb1'] = historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                    |((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()


            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)   

            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list)) &(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                    |((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)) &(historicalData_parsed['peocomb1'].isin(diff_df['peocomb1']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1','versionId','companyId','feedFileId','filingDate']].drop_duplicates()

            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()

            if len(temp1_revised)>0:
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Revenue actual 3 times less than EBIT actual"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Revenue actual 3 times less than EBIT actual"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb1']==row["peocomb1"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb1']==row["peocomb1"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_375(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    
    try:
        
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT, EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT, EBITDA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            lhs_df=pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)
        
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            # rhs_df = historicalData_parsed[historicalData_parsed['dataItemId'].isin(right_dataItemIds_list)&historicalData_parsed['peo'].isin(extractedData_parsed['peo'])][["dataItemId","peo","value_scaled"]]
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap])   
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
            missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
            collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
            rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df=pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True)


        if ((len(lhs_df)>0) & (len(rhs_df)>0)):
            base_currency=lhs_df.currency.mode()[0]
            lhs_df["value_scaled"] = lhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)        
            rhs_df["value_scaled"] = rhs_df.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        
        # if ((len(lhs_df)>0) & (len(rhs_df)>0)):

        merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter'],how='inner')
        merged_df['multipled_value']=merged_df['value_scaled_y']*float(factor[0])

        merged_df['variation']=((merged_df[['value_scaled_x','value_scaled_y']].max(axis=1)-merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))/merged_df[['value_scaled_x','value_scaled_y']].min(axis=1))*100
        merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        merged_df.dropna(inplace=True)

        peos=[]
        diff=[]
        perc=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]  
            
        for ind,row in merged_df.iterrows():                
            if ((row['value_scaled_x']<0)&(row['multipled_value']<0)):                
                if execute_operator(row['value_scaled_x'],row['multipled_value'],operator[0]):
                    peos.append(row['peo']) 
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])                        
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))

            else:
                if execute_operator(row['value_scaled_x'],row['multipled_value'],operator[1]):
                    peos.append(row['peo']) 
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])                        
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
        
        diff_df=pd.DataFrame({"peo":peos,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
        

        if len(diff_df)>0:
            diff_df['peocomb']=diff_df[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            extractedData_parsed['peocomb']=extractedData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['peocomb']=historicalData_parsed[['peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            
            temp1 = extractedData_parsed[(((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list)) &(extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) &(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())) 
                                          | ((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

            temp1['peocompressed']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
            historicalData_parsed['peocompressed']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                                                                                                                                                                                                                                            
            temp2 = historicalData_parsed[(((historicalData_parsed['dataItemId'].isin(left_dataItemIds_list))&(historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))) 
                                           | ((historicalData_parsed['dataItemId'].isin(right_dataItemIds_list))&(historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peocompressed'].isin(temp1['peocompressed'])))))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb']].drop_duplicates()


            temp1_revised=temp1.dropna()
            temp2_revised=temp2.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "EBT GAAP actual 100 times greater than EBT Normalized actual"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
            for ind, row in temp2_revised.iterrows():
                result = {"highlights": [], "error": "EBT GAAP actual 100 times greater than EBT Normalized actual"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": row['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'], "versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_379(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')

    try:
        temp = extractedData_parsed[(extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull())][['dataItemId','peo','consHigh','value_scaled','currency','consValue','consScaleId','consCurrency','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()

        # print(temp)
        if len(temp)>0:
            temp['consValue_scaled'] = temp.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)

            # base_currency=temp.currency.mode()[0]
            # temp["consValue_scaled"] = temp.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
            temp['multipled_value']=temp['consValue_scaled']*float(factor[0])

            temp['variation']=((temp[['value_scaled','consValue_scaled']].max(axis=1)-temp[['value_scaled','consValue_scaled']].min(axis=1))/temp[['value_scaled','consValue_scaled']].min(axis=1))*100
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)    

            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
    
            for ind,row in temp.iterrows():
                if execute_operator(row['value_scaled'],row['multipled_value'],operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    # difference=row[['value_scaled','consValue_scaled']].max()-row[['value_scaled','consValue_scaled']].min()
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
    
            
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised = temp1.dropna()

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "DPS Guidance High > DPS Consensus*5"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"]}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_380(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    try:
        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value_scaled','currency','consValue','consCurrency','consScaleId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()

        
        if len(temp)>0:
            temp['consValue_scaled'] = temp.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
            # base_currency=temp.currency.mode()[0]
            # temp["consValue_scaled"] = temp.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)

            temp['multipled_value']=((temp['value_scaled']).astype(float))*float(factor[0])

            temp['variation']=((temp[['value_scaled','consValue_scaled']].max(axis=1)-temp[['value_scaled','consValue_scaled']].min(axis=1))/temp[['value_scaled','consValue_scaled']].min(axis=1))*100
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)  
            # print(temp)

            dataItemIds=[]
            peos=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]

            for ind,row in temp.iterrows():
                if execute_operator(row['multipled_value'],float(row['consValue_scaled']),operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])               
                    # difference=float(row[['value_scaled','consValue_scaled']].max())-float(row[['value_scaled','consValue_scaled']].min())
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])        
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised = temp1.dropna()

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "D&A Actual 10 times less than D&A consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'], "versionId": filingMetadata['metadata']['versionId'],"filingId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_381(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters)
    factor=get_parameter_value(parameters,'MultiplicationFactor')
    try:
        temp = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','value_scaled','currency','consValue','consScaleId','consCurrency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','estimatePeriodId']].drop_duplicates()
        
        if len(temp)>0:
            temp['consValue_scaled'] = temp.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)

            # base_currency=temp.currency.mode()[0]
            # temp["consValue_scaled"] = temp.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
            
            temp['multipled_value']=temp['consValue_scaled']*float(factor[0])

            temp['variation']=((temp[['value_scaled','consValue_scaled']].max(axis=1)-temp[['value_scaled','consValue_scaled']].min(axis=1))/temp[['value_scaled','consValue_scaled']].min(axis=1))*100
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)              

            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
            
            for ind,row in temp.iterrows():
                if execute_operator(row['value_scaled'],row['multipled_value'],operator[0]):
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])   
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId']) 
                    # difference=row[['value_scaled','consValue_scaled']].max()-row[['value_scaled','consValue_scaled']].min()
                    diff.append(float(round(row['variation'])))
                    perc.append(float(round(row['variation'])))            
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"peo":peos,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
            
            # print(diff_df)
            
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()

                temp1_revised = temp1.dropna()
    
                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Cash EPS actual > 10x of Cash EPS Consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)

        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_391(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #['>]
    variation=get_parameter_value(parameters,'Max Threshold') #100%
    
    try:

        yesscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull())&(extractedData_parsed['scaleId']!=-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        noscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull())&(extractedData_parsed['scaleId']==-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        

        if len(yesscale)>0:
            yesscale['consValue_scaled'] = yesscale.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)

            base_currency=yesscale.currency.mode()[0]
            yesscale["consValue_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
            

        if len(noscale)>0: 
            #noscale['consValue_scaled']=noscale.consValue.astype(float)
            noscale['consValue_scaled'] = pd.to_numeric(noscale['consValue'],errors = 'coerce')
        if (len(yesscale)>0 or len(noscale)>0):

            temp=pd.concat([yesscale,noscale]) 


            temp['consensusvariation']=((abs(((temp['value_scaled']).astype(float))-((temp['consValue_scaled']).astype(float))))/(abs(temp[['value_scaled','consValue_scaled']])).min(axis=1))*100
    
   
            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
                
            for ind,row in temp.iterrows():
                if execute_operator(row['consensusvariation'],float(variation[0]),operator[0]):    
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])  
                    tid.append(row['tradingItemId']) 
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId']) 
                    diff.append(round(row['consensusvariation']))
                    perc.append(round(row['consensusvariation']))                        
            diff_df=pd.DataFrame({"peo":peos,"dataItemId":dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})
                            
            if len(diff_df)>0:
                diff_df['peocomb'] = diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                    
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['consValue']!="")&(extractedData_parsed['consValue'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                
                temp1_revised = temp1.dropna()
                        
                for ind, row in temp1_revised.iterrows():        
                    result = {"highlights": [], "error": "Guidance not in line with consensus_ >900% variation"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors   

#Estimates Error Checks 
@add_method(Validation)
def AG_406(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    tag_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator = get_dataItemIds_list('Operation', parameters) #['>]
    variation=get_parameter_value(parameters,'Min_Threshold') #100%
    
    try:
        yesscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']!=-1)&(extractedData_parsed['currencyId']!=-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        volume = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']!=-1)&((extractedData_parsed['currencyId']==-1)|(extractedData_parsed['currencyId']==0)))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        noscale = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(tag_list))&(extractedData_parsed['dataItemFlag']=="G")&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['scaleId']==-1))][['dataItemId','peo','scaleId','value_scaled','currency','consValue','consScaleId','consCurrency','periodTypeId','fiscalYear','fiscalQuarter','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].drop_duplicates()
        
        if len(yesscale)>0:
            yesscale['consValue_scaled'] = yesscale.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
            
            base_currency=yesscale.consCurrency.mode()[0]
            yesscale["value_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            yesscale["consValue_scaled"] = yesscale.apply(lambda x: currency_converter(currency_from=x['consCurrency'], currency_to=base_currency, value=x['consValue_scaled']), axis=1)
        
        if len(volume)>0:
            volume['consValue_scaled'] = volume.apply(lambda row: get_scaled_value(row['consValue'], row['consScaleId']), axis=1)
    
        if len(noscale)>0: 
            noscale['value_scaled'] = pd.to_numeric(noscale['value_scaled'],errors = 'coerce')
            noscale['consValue_scaled'] = pd.to_numeric(noscale['consValue'],errors = 'coerce')
        if (len(yesscale)>0 or len(noscale)>0 or len(volume)>0):

            frames = [yesscale, volume,noscale]
            temp = pd.concat(frames)[['dataItemId','peo','value_scaled','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId','consValue_scaled']]
            
            temp['consensusvariation']=((abs(((temp['value_scaled']).astype(float))-((temp['consValue_scaled']).astype(float))))/(abs(temp[['value_scaled','consValue_scaled']])).min(axis=1))*100
            
            temp.replace([np.inf, -np.inf], np.nan, inplace=True)
            temp.dropna(inplace=True)

            dataItemIds=[]
            peos=[]
            diff=[]
            perc=[]
            tid=[]
            parentflag=[]
            accounting=[]
            fyc=[]
                
            for ind,row in temp.iterrows():                    
                if execute_operator(row['consensusvariation'],float(variation[0]),operator[0]):    
                    dataItemIds.append(row['dataItemId'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag'])
                    accounting.append(row['accountingStandardDesc'])
                    fyc.append(row['fiscalChainSeriesId'])
                    diff.append(float(round(row['consensusvariation'])))
                    perc.append(float(round(row['consensusvariation'])))
                        
            diff_df=pd.DataFrame({"peo":peos,"dataItemId":dataItemIds,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,"diff":diff,"perc":perc})

            # print(diff_df)
                        
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

                # diff_df['peocomb']=diff_df.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                # extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemId'],x['peo']),axis=1)
                    
                temp1 = extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb']].drop_duplicates()
                
                temp1_revised = temp1.dropna()

                for ind, row in temp1_revised.iterrows():        
                    result = {"highlights": [], "error": "Guidance not in line with consensus"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]                              
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def EST_25(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    try:
        filingdate = filingMetadata['metadata']['filingDate']
        contributor = filingMetadata['metadata']['researchContributorId']

        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=='E')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        previous = historicalData_parsed[((historicalData_parsed['dataItemFlag']=='E')&(historicalData_parsed['value']!="")
                                          &(historicalData_parsed['value'].notnull())&(historicalData_parsed['researchContributorId']==contributor))][['parentFlag','accountingStandardDesc','tradingItemId','filingDate','team']].drop_duplicates()

        maxprevious=previous.groupby(['parentFlag','accountingStandardDesc','tradingItemId','team'])['filingDate'].max().reset_index()

        parent=maxprevious[((maxprevious['team']!='Non - Periodic Estimates')&(~(maxprevious['parentFlag'].isin(current['parentFlag']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()
        
        accounting=maxprevious[((maxprevious['accountingStandardDesc']!='')&(maxprevious['accountingStandardDesc'].notnull())&(~(maxprevious['accountingStandardDesc'].isin(current['accountingStandardDesc']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        tid=maxprevious[((maxprevious['tradingItemId']!=-1)&(~(maxprevious['tradingItemId'].isin(current['tradingItemId']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        if len(parent)>0:

            parent_df=historicalData_parsed[((historicalData_parsed['parentFlag'].isin(parent['parentFlag']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                               (historicalData_parsed['value'].notnull()) & (historicalData_parsed['researchContributorId'] == contributor))][['parentFlag','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            parent_df_revised=parent_df.dropna()

            for ind, row in parent_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(parentFlag) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": 'NA',"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": 'NA',"accountingStdDesc": 'NA',"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

        if len(accounting)>0:

            accounting_df=historicalData_parsed[((historicalData_parsed['accountingStandardDesc'].isin(accounting['accountingStandardDesc']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                               (historicalData_parsed['value'].notnull()) & (historicalData_parsed['researchContributorId'] == contributor))][['accountingStandardDesc','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            accounting_df_revised=accounting_df.dropna()

            for ind, row in accounting_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(accountingStandardDesc) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": 'NA',"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": 'NA',"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": 'NA',"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

        if len(tid)>0:

            tid_df=historicalData_parsed[((historicalData_parsed['tradingItemId'].isin(tid['tradingItemId']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                               (historicalData_parsed['value'].notnull()) & (historicalData_parsed['researchContributorId'] == contributor))][['tradingItemId','tradingItemName','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            tid_df_revised=tid_df.dropna()

            for ind, row in tid_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(tradingItemId) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row['tradingItemId'],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": 'NA',"parentConsolidatedFlag": 'NA',"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_20(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters) #Data items
    operator = get_dataItemIds_list('Operation', parameters) #["!="]
    try:
    # conditions for flavour check
        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['periodEndDate']),axis=1)

        FY = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==1)&(extractedData_parsed['fiscalQuarter']==4)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #REVENUE
        Q4 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==2)&(extractedData_parsed['fiscalQuarter']==4)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        H2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemId_list))&(extractedData_parsed['periodTypeId']==10)&(extractedData_parsed['fiscalQuarter']==4)&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
        
        
        if FY["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(FY["dataItemId"])]
            FY_not_captured = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['periodTypeId']==1)
                                        &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                        &(historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CAPEX
        
            FY = pd.concat([FY,FY_not_captured])
        

        extracted_dataItemId_peo_count=extractedData_parsed['fiscalYear'].nunique()
        FY_peo_count=FY.groupby(['dataItemId'])['fiscalYear'].nunique().reset_index(name='peocount')    

        if (FY_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            missed_peo_dataItemId=FY_peo_count[(FY_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']

            collected_peo=FY[FY['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','fiscalYear']]

            FY_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['periodTypeId']==1) &(historicalData_parsed['value'].notnull())
                                                  &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")
                                                  & (historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))&~(historicalData_parsed['fiscalYear'].isin(collected_peo['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

            FY = pd.concat([FY,FY_missing_data])

        
        if Q4["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(Q4["dataItemId"])]
            Q4_not_captured = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['periodTypeId']==2)
                                        &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                        &(historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CAPEX
            
            Q4 = pd.concat([Q4,Q4_not_captured])

        Q4_peo_count=Q4.groupby(['dataItemId'])['fiscalYear'].nunique().reset_index(name='peocount')    
        

        if (Q4_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            missed_peo_dataItemId=Q4_peo_count[(Q4_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']

            collected_peo=Q4[Q4['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','fiscalYear']]

            Q4_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['periodTypeId']==2) 
                                                  &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                  & (historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))&~(historicalData_parsed['fiscalYear'].isin(collected_peo['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

            Q4 = pd.concat([Q4,Q4_missing_data])
            

        if H2["dataItemId"].nunique()!=len(left_dataItemId_list):
            not_captured=[x for x in left_dataItemId_list if x not in set(H2["dataItemId"])]
            H2_not_captured = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['periodTypeId']==10)
                                        &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                        &(historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #CAPEX
            H2 = pd.concat([H2,H2_not_captured])
            
        H2_peo_count=H2.groupby(['dataItemId'])['fiscalYear'].nunique().reset_index(name='peocount')    
        

        if (H2_peo_count['peocount']<extracted_dataItemId_peo_count).any():
            missed_peo_dataItemId=H2_peo_count[(H2_peo_count['peocount']<extracted_dataItemId_peo_count)]['dataItemId']

            collected_peo=H2[H2['dataItemId'].isin(missed_peo_dataItemId)][['dataItemId','fiscalYear']]

            H2_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_dataItemId))&(historicalData_parsed['periodTypeId']==10) 
                                                  &(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())
                                                  & (historicalData_parsed['fiscalQuarter']==4)&(historicalData_parsed['fiscalYear'].isin(extractedData_parsed['fiscalYear']))&~(historicalData_parsed['fiscalYear'].isin(collected_peo['fiscalYear'])))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]

            H2 = pd.concat([H2,H2_missing_data])            
   

        if (len(FY)>0):
            base_currency=FY.currency.mode()[0]
            FY["value_scaled"] = FY.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            if (len(Q4)>0): 
                Q4["value_scaled"] = Q4.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
            
            if (len(H2)>0):           
                H2["value_scaled"] = H2.apply(lambda x: currency_converter(currency_from=x['currency'], currency_to=base_currency, value=x['value_scaled']), axis=1)
        

        merged_df1=pd.merge(FY,Q4,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter'],how='inner')
        merged_df2=pd.merge(FY,H2,on=['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','fiscalYear','fiscalQuarter'],how='inner')


        dataItemIds=[]
        FYs=[]
        peo_q4=[]
        peo_fy=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]        
        diff=[]
        perc=[]
        for ind,row in merged_df1.iterrows():

            if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                dataItemIds.append(row['dataItemId'])
                FYs.append(int(row['fiscalYear']))
                peo_q4.append(row['peo_y'])
                peo_fy.append(row['peo_x'])
                tid.append(row['tradingItemId']) 
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                diff.append(round((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
                perc.append(round((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
        
      
        for ind,row in merged_df2.iterrows():
            if execute_operator (row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                dataItemIds.append(row['dataItemId'])
                FYs.append(int(row['fiscalYear']))
                peo_q4.append(row['peo_y'])
                peo_fy.append(row['peo_x'])
                tid.append(row['tradingItemId']) 
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                difference=row[['value_scaled_x','value_scaled_y']].max()-row[['value_scaled_x','value_scaled_y']].min()
                diff.append(round((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
                perc.append(round((difference/(row[['value_scaled_x','value_scaled_y']].min()))*100))
        
        
        diff_df=pd.DataFrame({"fiscalYear":FYs,"diff":diff,"perc":perc,'dataItemId':dataItemIds,"peo_y":peo_q4,"peo_x":peo_fy,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})


        if len(diff_df)>0:
            diff_df['compressed']=diff_df[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','fiscalChainSeriesId','tradingItemId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            extractedData_parsed['compressed']=extractedData_parsed[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','fiscalChainSeriesId','tradingItemId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            historicalData_parsed['compressed']=historicalData_parsed[['dataItemId','fiscalYear','parentFlag','accountingStandardDesc','fiscalChainSeriesId','tradingItemId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            temp1 = extractedData_parsed[((extractedData_parsed['compressed'].isin(diff_df['compressed'])) &((extractedData_parsed['peo'].isin(peo_q4))|(extractedData_parsed['peo'].isin(peo_fy)))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','compressed']].drop_duplicates()

            temp1_revised=temp1.dropna()
            
            
            temp2 = historicalData_parsed[((historicalData_parsed['compressed'].isin(diff_df['compressed'])) &((historicalData_parsed['peo'].isin(peo_q4))|(historicalData_parsed['peo'].isin(peo_fy)))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb','versionId','companyId','feedFileId','filingDate','compressed']].drop_duplicates()
            temp2_revised=temp2.dropna()
            
            if len(temp1_revised)>0:

                for ind, row in temp1_revised.iterrows():
                    result = {"highlights": [], "error": "Value mismatch for Balance sheet data items for same period end date"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"],"diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"diff": 'NA', "percent": 'NA'}]
                    errors.append(result)

                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Value mismatch for Balance sheet data items for same period end date"}
                    result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors


#Estimates Error Checks 
@add_method(Validation)
def AG_25(historicalData,filingMetadata,extractedData,parameters):

    """
    This check should fire when some pre-defined arithmetic relation between current parentFlag,accountingStandardDesc,tradingItemId and previous parentFlag,accountingStandardDesc,tradingItemId

    """ 
    errors = []

    if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'value', 'peo'}.issubset(extractedData_parsed.columns))):
        
        filingdate = filingMetadata['metadata']['filingDate']
        companyid = filingMetadata['metadata']['companyId']
    
        current = extractedData_parsed[(((extractedData_parsed['dataItemFlag']=='A')|(extractedData_parsed['dataItemFlag']=='G'))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        previous = historicalData_parsed[(((historicalData_parsed['dataItemFlag']=='A')|(historicalData_parsed['dataItemFlag']=='G'))&(historicalData_parsed['value']!="")
                                            &(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid))][['parentFlag','accountingStandardDesc','tradingItemId','filingDate','team']].drop_duplicates()

        maxprevious=previous.groupby(['parentFlag','accountingStandardDesc','tradingItemId','team'])['filingDate'].max().reset_index()
        
        # print(maxprevious)
        

        parent=maxprevious[((maxprevious['team']!='Non - Periodic Guidance')&(~(maxprevious['parentFlag'].isin(current['parentFlag']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        # print(parent)
        
        accounting=maxprevious[((maxprevious['accountingStandardDesc']!='')&(maxprevious['accountingStandardDesc'].notnull())&(~(maxprevious['accountingStandardDesc'].isin(current['accountingStandardDesc']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

        tid=maxprevious[((maxprevious['tradingItemId']!=-1)&(~(maxprevious['tradingItemId'].isin(current['tradingItemId']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates() 
        # print(tid)

        if len(parent)>0:

            parent_df=historicalData_parsed[((historicalData_parsed['parentFlag'].isin(parent['parentFlag']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                                (historicalData_parsed['value'].notnull()) & (historicalData_parsed['companyId'] == companyid))][['parentFlag','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            parent_df_revised=parent_df.dropna()
            
            # print(parent_df)

            for ind, row in parent_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(parentFlag) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": 'NA',"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": 'NA',"accountingStdDesc": 'NA',"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

        if len(accounting)>0:

            accounting_df=historicalData_parsed[((historicalData_parsed['accountingStandardDesc'].isin(accounting['accountingStandardDesc']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                                (historicalData_parsed['value'].notnull()) & (historicalData_parsed['companyId'] == companyid))][['accountingStandardDesc','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            accounting_df_revised=accounting_df.dropna()

            for ind, row in accounting_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(accountingStandardDesc) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": 'NA',"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": 'NA',"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": 'NA',"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

        if len(tid)>0:

            tid_df=historicalData_parsed[((historicalData_parsed['tradingItemId'].isin(tid['tradingItemId']))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate']))&(historicalData_parsed['value'] != "") & 
                                                (historicalData_parsed['value'].notnull()) & (historicalData_parsed['companyId'] == companyid))][['tradingItemId','tradingItemName','versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates()

            tid_df_revised=tid_df.dropna()
            
            # print(tid_df_revised)

            for ind, row in tid_df_revised.iterrows():
                result = {"highlights": [], "error": "Flavor(tradingItemId) not collected in current document compared to previous document"}
                result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": 'NA',"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
                result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row['tradingItemId'],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": 'NA',"parentConsolidatedFlag": 'NA',"fiscalChainSeries": 'NA',"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                errors.append(result) 

    print(errors)
    return errors
    # except Exception as e:
    #     print(e) 
    #     return errors

    # errors = []
    # try:
    #     company_id = filingMetadata['metadata']['companyId']

    #     current_flags = ['G', 'A']
    #     current = extractedData_parsed[(extractedData_parsed['dataItemFlag'].isin(current_flags)) & (extractedData_parsed['value'] != "") & extractedData_parsed['value'].notnull()]
    #     current = current[['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'team']].drop_duplicates()

    #     previous = historicalData_parsed[(historicalData_parsed['dataItemFlag'].isin(current_flags)) & (historicalData_parsed['value'] != "") & historicalData_parsed['value'].notnull() & (historicalData_parsed['companyId'] == company_id)]
    #     previous = previous[['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'filingDate', 'team']].drop_duplicates()

    #     max_previous = previous.groupby(['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'team'])['filingDate'].max().reset_index()

    #     discrepancies = []
    #     for field in ['parentFlag', 'accountingStandardDesc', 'tradingItemId']:
    #         filter_condition = ~max_previous[field].isin(current[field]) if field != 'tradingItemId' else ((max_previous['tradingItemId'] != -1) & ~max_previous['tradingItemId'].isin(current['tradingItemId']))
    #         discrepancies.append(max_previous[filter_condition][['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'team']].drop_duplicates())

    #     if any(len(df) != 0 for df in discrepancies):
    #         filtered = pd.concat(discrepancies).drop_duplicates()
    #         temp1 = historicalData_parsed[(historicalData_parsed[['parentFlag', 'accountingStandardDesc', 'tradingItemId']].isin(filtered).any(axis=1)) & historicalData_parsed['filingDate'].isin(max_previous['filingDate'])]
    #         temp1 = temp1[['parentFlag', 'accountingStandardDesc', 'tradingItemId', 'team', 'tradingItemName', 'versionId', 'companyId', 'feedFileId', 'filingDate']].drop_duplicates().dropna()

    #         for ind, row in temp1.iterrows():
    #             result = {"highlights": [], "error": "Flavor not collected in current document compared to previous document"}
    #             result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": row['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
    #             result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
    #             result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA', "diff":"NA", "percent": "NA"}]
    #             errors.append(result)         
    #     print(errors)
    #     return errors
    # except Exception as e:
    #     print(e) 
    #     return errors
    
    
    # errors = []
    # try:
    #     companyid=filingMetadata['metadata']['companyId']

    #     current = extractedData_parsed[(((extractedData_parsed['dataItemFlag']=='G')|(extractedData_parsed['dataItemFlag']=='A'))&(extractedData_parsed['value']!="")
    #                                     &(extractedData_parsed['value'].notnull()))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()


    #     previous = historicalData_parsed[(((historicalData_parsed['dataItemFlag']=='G')|(historicalData_parsed['dataItemFlag']=='A'))&(historicalData_parsed['value']!="")
    #                                       &(historicalData_parsed['value'].notnull())&(historicalData_parsed['companyId']==companyid))][['parentFlag','accountingStandardDesc','tradingItemId','filingDate','team']].drop_duplicates()

    #     maxprevious=previous.groupby(['parentFlag','accountingStandardDesc','tradingItemId','team'])['filingDate'].max().reset_index()
                                                                                                             

    #     parent=maxprevious[(~(maxprevious['parentFlag'].isin(current['parentFlag'])))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

    #     accounting=maxprevious[(~(maxprevious['accountingStandardDesc'].isin(current['accountingStandardDesc'])))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()

    #     tid=maxprevious[((maxprevious['tradingItemId']!=-1)&(~(maxprevious['tradingItemId'].isin(current['tradingItemId']))))][['parentFlag','accountingStandardDesc','tradingItemId','team']].drop_duplicates()
                          

    #     if ((len(parent)!=0) | (len(accounting)!=0) | (len(tid)!=0)):
       
    #         temp1=historicalData_parsed[(((historicalData_parsed['parentFlag'].isin(parent['parentFlag']))|(historicalData_parsed['accountingStandardDesc'].isin(accounting['accountingStandardDesc']))
    #                                      |(historicalData_parsed['tradingItemId'].isin(tid['tradingItemId'])))&(historicalData_parsed['filingDate'].isin(maxprevious['filingDate'])))][['parentFlag','accountingStandardDesc','tradingItemId','team','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
            
    #         temp1_revised=temp1.dropna()

    #         for ind, row in temp1_revised.iterrows():
    #             result = {"highlights": [], "error": "Flavor not collected in current document compared to previous document"}
    #             result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
    #             result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": "NA", "percent": "NA"}
    #             result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": 'NA', "diff":"NA", "percent": "NA"}]
    #             errors.append(result)         
    #     print(errors)
    #     return errors
    # except Exception as e:
    #     print(e) 
    #     return errors


#Estimates Error Checks 
@add_method(Validation)
def AG_35(historicalData,filingMetadata,extractedData,parameters):
    """
    This check should fire when Variation in fiscal Year series compared to the previous document

    Operation-"!="

    """

    errors = []
    operator = get_dataItemIds_list('Operation', parameters)
    
    if (extractedData_parsed is not None and len(extractedData_parsed) > 0 and {'value', 'peo'}.issubset(extractedData_parsed.columns)):
        
        documentdate=filingMetadata['metadata']['filingDate']
        
        temp = extractedData_parsed[(((extractedData_parsed['dataItemFlag']=='A')|(extractedData_parsed['dataItemFlag']=='G'))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())&(extractedData_parsed['peo']!= "")&(extractedData_parsed['peo'].notnull()))][['dataItemId','peo','fiscalChainSeriesId']].drop_duplicates()
        
        # temp['filingDate']=pd.to_datetime(filingMetadata['metadata']['filingDate'])
        
        temp['companyId']=filingMetadata['metadata']['companyId']
        
        
        previous = historicalData_parsed[(((historicalData_parsed['dataItemFlag']=='A')|(historicalData_parsed['dataItemFlag']=='G'))&(historicalData_parsed['companyId'].isin(temp['companyId']))&(historicalData_parsed['filingDate']<documentdate)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&(historicalData_parsed['peo']!="")&(historicalData_parsed['peo'].notnull()))][['dataItemId','description','peo','fiscalChainSeriesId','filingDate','companyId']].drop_duplicates()
        
        newfyc=temp[~((temp['fiscalChainSeriesId']).isin(previous['fiscalChainSeriesId']))]
        
        # print(newfyc)

        maxprevious=previous.groupby(['companyId'])['filingDate'].max().reset_index()

        previous=previous[previous['filingDate'].isin(maxprevious['filingDate'])]
        
  
        merged_df=pd.merge(temp,previous,on=['companyId'],how='inner')


        filingdate=[]
        diff=[]
        perc=[]
        series1=[]
        series2=[]
               
        for ind,row in merged_df.iterrows():
            if execute_operator(row['fiscalChainSeriesId_x'],row['fiscalChainSeriesId_y'],operator[0]):
                filingdate.append(row['filingDate'])             
                difference='NA'
                series1.append(row['fiscalChainSeriesId_x'])
                series2.append(row['fiscalChainSeriesId_y'])
                diff.append(difference)
                perc='NA'

        diff_df=pd.DataFrame({"diff":diff,"perc":perc,"filingDate":filingdate,"curseries":series1,"preseries":series2}).drop_duplicates()
       
       

        temp1 = extractedData_parsed[((extractedData_parsed['fiscalChainSeriesId'].isin(series1)))][['fiscalChainSeriesId','accountingStandardDesc','parentFlag','team']].drop_duplicates()

        temp2 = historicalData_parsed[((historicalData_parsed['filingDate'].isin(diff_df['filingDate']))&(historicalData_parsed['fiscalChainSeriesId'].isin(series2)))][['fiscalChainSeriesId','accountingStandardDesc','parentFlag','team','versionId','feedFileId','filingDate','companyId']].drop_duplicates()
        
        temp1_revised=temp1.dropna()
        temp2_revised=temp2.dropna()
    
        if len(newfyc)>0:
            temp3 = extractedData_parsed[(extractedData_parsed['fiscalChainSeriesId'].isin(newfyc['fiscalChainSeriesId']))][['fiscalChainSeriesId', 'accountingStandardDesc', 'parentFlag', 'team']].drop_duplicates()
            temp3_revised=temp3.dropna()
            for ind, row in temp3_revised.iterrows():
                result = {"highlights": [],"error": "New fiscal Year series collected for this company"}
                result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"},"cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"] = {"statement": "", "tag": "NA", "description": "NA","versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'],"feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                result["checkGeneratedForList"] = [{"tag": "NA", "description": "NA", "tradingItemId": "NA", "fiscalYear": "NA", "fiscalQuarter": "NA","peo": "NA", "value": "NA", "units": "NA", "currency": "NA", "tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"], "parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": "NA", "percent": "NA"}]
                errors.append(result)           

        else:
            if len(temp1_revised) > 0 and len(temp2_revised) > 0:     
                for ind, row in temp1_revised.iterrows():  
                    result = {"highlights": [], "error": "Variation in fiscal Year series compared to the previous document"}
                    result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": "NA"}
                    result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear":"NA", "fiscalQuarter":"NA", "peo":"NA","value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)

         
                for ind, row in temp2_revised.iterrows():
                    result = {"highlights": [], "error": "Variation in fiscal Year series compared to the previous document"}
                    result["highlights"].append({"row": {"fiscalChainSeriesId": row['fiscalChainSeriesId'], "id": "NA"}, "cell": {"peo": "NA", "scale": "NA", "value": "NA", "currency": "NA"},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": "NA", "description": "NA", "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": "NA"}
                    result["checkGeneratedForList"]=[{"tag": "NA", "description": "NA", "tradingItemId": "NA","fiscalYear": "NA", "fiscalQuarter":"NA", "peo": "NA","value": "NA","units": "NA","currency": "NA","tradingItemName": "NA","accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff":"NA", "percent": "NA"}]
                    errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors 

#Estimates Error Checks 
@add_method(Validation)
def AG_320(historicalData,filingMetadata,extractedData,parameters):
    """
    This check should fire when Guidance data collected for Actualized PEO

    Operation-">"

    """ 
    errors = []
    operator = get_dataItemIds_list('Operation', parameters) #['>']
    
    if ((extractedData_parsed is not None) and (len(extractedData_parsed) > 0) and ({'dataItemId','value', 'peo','actualizedDate'}.issubset(extractedData_parsed.columns))):
        
        temp = extractedData_parsed[((extractedData_parsed['dataItemFlag']=='G')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','actualizedDate']].drop_duplicates()
        temp['filingDate']=pd.to_datetime(filingMetadata['metadata']['filingDate'])
        temp['actualizedDate']=pd.to_datetime(temp['actualizedDate'])
        
        # print(temp)
       
        peos=[]
        diff=[]
        perc=[]
        
        for ind,row in temp.iterrows():
            if execute_operator(row['filingDate'],row['actualizedDate'],operator[0]):
                peos.append(row['peo'])             
                difference='NA'
                diff.append(difference)
                perc='NA'
        
        diff_df=pd.DataFrame({"peo":peos,"diff":diff,"perc":perc})

        if len(diff_df)>0:
            temp1=extractedData_parsed[(extractedData_parsed['dataItemFlag']=='G')&(extractedData_parsed['peo'].isin(diff_df['peo']))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull())][['dataItemId','peo','value','scale','currency','team','description','tradingItemId','fiscalYear','fiscalQuarter','tradingItemName','accountingStandardDesc','parentFlag','fiscalChainSeriesId']].drop_duplicates()

            temp1_revised=temp1.dropna()
            
            for ind, row in temp1_revised.iterrows():
                result = {"highlights": [], "error": "Guidance data collected for Actualized PEO"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                errors.append(result)
    print(errors)
    return errors
    # except Exception as e:
    #     print(e)
    #     return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_3(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    right_dataItemIds_list=get_dataItemIds_list('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
               
    try:
        
        companyid = filingMetadata['metadata']['companyId']

        datacomb=list(zip(left_dataItemIds_list,right_dataItemIds_list))
        comparable=pd.DataFrame(datacomb,columns=['dataitem1','dataitem2'])

        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        
        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates() #EBIT, EBITDA
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT, EBITDA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])

        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')       
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['companyId']==companyid) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                # lhs_df=lhs_df._append(lhs_df_missing_data,ignore_index=True)
                lhs_df = pd.concat([lhs_df,lhs_df_missing_data],ignore_index=True)

        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap]) 

        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['companyId']==companyid)&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                # rhs_df=rhs_df._append(rhs_df_missing_data,ignore_index=True)
                rhs_df = pd.concat([rhs_df,rhs_df_missing_data],ignore_index=True) 

        lhs_df["valuesign"]=np.sign(lhs_df['value_scaled'])
        rhs_df["valuesign"]=np.sign(rhs_df['value_scaled'])

        if not lhs_df.empty and not rhs_df.empty:
            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId'],how='inner')
            comparable['compressed']=comparable.apply(lambda x:'%s%s' % (x['dataitem1'],x['dataitem2']),axis=1)
            merged_df['compressed']=merged_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['dataItemId_y']),axis=1)
            
            key_cols = ['compressed']
            dataitem_combination=merged_df.merge(comparable.loc[:, comparable.columns.isin(key_cols)])
            
            

            dataItemIds_x=[]
            dataItemIds_y=[]
            peos=[]
            parentflag=[]
            accounting=[]
            fyc=[]        
            diff=[]
            perc=[]
            
            for ind,row in dataitem_combination.iterrows():
                if ((row['valuesign_x']!=0)&(row['valuesign_y']!=0)):
                    if execute_operator(np.sign(row['valuesign_x']),np.sign(row['valuesign_y']),operator[0]):
                        dataItemIds_x.append(row['dataItemId_x'])
                        dataItemIds_y.append(row['dataItemId_y'])
                        peos.append(row['peo'])
                        parentflag.append(row['parentFlag']) 
                        accounting.append(row['accountingStandardDesc']) 
                        fyc.append(row['fiscalChainSeriesId'])
                        diff='NA'
                        perc='NA'
            diff_df=pd.DataFrame({"dataItemId_x":dataItemIds_x,"dataItemId_y":dataItemIds_y,"peo":peos,"diff":diff,"perc":perc,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

            if len(diff_df)>0:
                diff_df['peocomb1']=diff_df[['dataItemId_x','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                diff_df['peocomb2']=diff_df[['dataItemId_y','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb1']=extractedData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb2']=historicalData_parsed[['dataItemId','peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                            
                
                temp1 = extractedData_parsed[(((extractedData_parsed['peocomb1'].isin(diff_df['peocomb1'])) | (extractedData_parsed['peocomb1'].isin(diff_df['peocomb2'])))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','peocomb1']].drop_duplicates()
                                                                                                                                                                                                                                            
                temp2 = historicalData_parsed[(((historicalData_parsed['peocomb2'].isin(diff_df['peocomb1'])) | (historicalData_parsed['peocomb2'].isin(diff_df['peocomb2'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb2']].drop_duplicates()
    
                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()


                if len(temp1_revised)>0:
                    for ind, row in temp1_revised.iterrows():        
                        if row['value']!=0:
                            result = {"highlights": [], "error": "Sign difference between two related data items"}
                            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[(diff_df['peocomb1']==row["peocomb1"]) | (diff_df['peocomb2']==row["peocomb1"])]['diff'].iloc[0], "percent": diff_df[(diff_df['peocomb1']==row["peocomb1"]) | (diff_df['peocomb2']==row["peocomb1"])]['perc'].iloc[0]}
                            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[(diff_df['peocomb1']==row["peocomb1"]) | (diff_df['peocomb2']==row["peocomb1"])]['diff'].iloc[0], "percent": diff_df[(diff_df['peocomb1']==row["peocomb1"]) | (diff_df['peocomb2']==row["peocomb1"])]['perc'].iloc[0]}]
                            errors.append(result)
        
                    for ind, row in temp2_revised.iterrows():
                        if row['value']!=0:
                            result = {"highlights": [], "error": "Sign difference between two related data items"}
                            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": diff_df[(diff_df['peocomb1']==row["peocomb2"]) | (diff_df['peocomb2']==row["peocomb2"])]['diff'].iloc[0], "percent": diff_df[(diff_df['peocomb1']==row["peocomb2"]) | (diff_df['peocomb2']==row["peocomb2"])]['perc'].iloc[0]}
                            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": diff_df[(diff_df['peocomb1']==row["peocomb2"]) | (diff_df['peocomb2']==row["peocomb2"])]['diff'].iloc[0], "percent": diff_df[(diff_df['peocomb1']==row["peocomb2"]) | (diff_df['peocomb2']==row["peocomb2"])]['perc'].iloc[0]}]
                            errors.append(result)
                                   
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_6(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    operator=get_dataItemIds_list('Operation', parameters)
    Threshold=get_parameter_value(parameters,'Max Threshold') 
    left_dataItemId_list=get_dataItemIds_list2('LHSdataItemIds', parameters)
    right_dataItemId_list=get_dataItemIds_list2('RHSdataItemIds', parameters)
    
    try:
        datacomb=list(zip(left_dataItemId_list,right_dataItemId_list))
        comparable=pd.DataFrame(datacomb,columns=['dataitem1','dataitem2']) 
        

        filingdate=filingMetadata['metadata']['filingDate']

        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=='G')&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        

        previous = historicalData_parsed[((historicalData_parsed['dataItemFlag']=='G')&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear','filingDate']]
        

        previous['daysdiff']=abs((pd.to_datetime(filingdate)-pd.to_datetime(previous['filingDate'])).dt.days)
        
        currenthistory=previous[(previous['daysdiff']<8)][['dataItemId','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId','periodTypeId','fiscalYear']]
        
        current=pd.concat([current,currenthistory])

        relateddi=comparable[comparable['dataitem1'].isin(current['dataItemId'])][['dataitem1','dataitem2']]
        
        relateddataitem=pd.merge(relateddi,current[['dataItemId','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']],left_on='dataitem1',right_on='dataItemId',how='left')
      
        maxprevious=previous.groupby(['dataItemId','parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId'])['filingDate'].max().reset_index()
        
        maxprevious['days']=abs((pd.to_datetime(filingdate)-pd.to_datetime(maxprevious['filingDate'])).dt.days)
        

        temp=maxprevious[~((maxprevious['dataItemId'].isin(relateddataitem['dataitem2']))&(maxprevious['parentFlag'].isin(relateddataitem['parentFlag']))
                         &(maxprevious['accountingStandardDesc'].isin(relateddataitem['accountingStandardDesc']))&(maxprevious['fiscalChainSeriesId'].isin(relateddataitem['fiscalChainSeriesId']))
                         &(maxprevious['tradingItemId'].isin(relateddataitem['tradingItemId'])))]
        
        dataItemIds=[]
        previousdate=[]
        parentflag=[]
        tid=[]
        accounting=[]
        fyc=[]
        diff=[]
        perc=[]  
      
        for ind, row in temp.iterrows():
            if execute_operator(row['days'],float(Threshold[0]),operator[0]):
                dataItemIds.append(row['dataItemId'])
                previousdate.append(row['filingDate'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag'])
                accounting.append(row['accountingStandardDesc'])
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'  
                
        diff_df=pd.DataFrame({"dataItemId":dataItemIds,"filingDate":previousdate,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc})

        if len(diff_df)>0:
            diff_df['peocomb']=diff_df[['dataItemId','filingDate','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)

            historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','filingDate','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
            
            temp1 = historicalData_parsed[((historicalData_parsed['peocomb'].isin(diff_df['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))] [['dataItemId','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','team','description','tradingItemName','versionId','companyId','feedFileId','filingDate','peocomb']].drop_duplicates()
            
            temp1_revised=temp1.dropna()   

            for ind, row in temp1_revised.iterrows():
    
                result = {"highlights": [], "error": "Tag not in current document (Comparable)"}
                result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale": 'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}
                result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"], "diff": diff_df[diff_df['peocomb']==row["peocomb"]]['diff'].iloc[0], "percent": diff_df[diff_df['peocomb']==row["peocomb"]]['perc'].iloc[0]}]
                errors.append(result)
        print(errors)
        return errors
    except Exception as e:
        print(e) 
        return errors


#Estimates Error Checks 
@add_method(Validation)
def AG_60(historicalData,filingMetadata,extractedData,parameters):
    errors = []

    try:
       
        current = extractedData_parsed[((extractedData_parsed['dataItemFlag']=="A")&(extractedData_parsed['value']!="")&(extractedData_parsed['value']!="")&(~(extractedData_parsed['parentFlag'].isin(historicalData_parsed['parentFlag']))
                                        |~(extractedData_parsed['tradingItemId'].isin(historicalData_parsed['tradingItemId']))
                                        |~(extractedData_parsed['accountingStandardDesc'].isin(historicalData_parsed['accountingStandardDesc']))
                                        |~(extractedData_parsed['fiscalChainSeriesId'].isin(historicalData_parsed['fiscalChainSeriesId']))
                                        ))][['dataItemId','peo','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']].drop_duplicates()


        if len(current)>0:
            dataItemIds=[]
            parentflag=[]
            accounting=[]
            # peos=[]
            tid=[]
            fyc=[]        
            diff=[]
            perc=[]
            
            for ind,row in current.iterrows():
                dataItemIds.append(row['dataItemId'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                # peos.append(row['peo'])
                diff='NA'
                perc='NA'
            diff_df=pd.DataFrame({"dataItemId":dataItemIds,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc}).drop_duplicates()
        
            if len(diff_df)>0:
                
                diff_df['peocomb'] = diff_df[['parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb'] = extractedData_parsed[['parentFlag','tradingItemId','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                temp= extractedData_parsed[((extractedData_parsed['peocomb'].isin(diff_df['peocomb'])) &(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','team','tradingItemName']].drop_duplicates()
                
                temp_revised = temp.dropna()
        
                for ind, row in temp_revised.iterrows():
                    result = {"highlights": [], "error": "New Flavor Captured For The Company"}
                    result["highlights"].append({"row": {"name": 'NA', "id": 'NA',"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": 'NA', "scale":'NA', "value": 'NA', "currency": 'NA'},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                    result["checkGeneratedFor"]={"statement": "", "tag": 'NA', "description": 'NA', "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": 'NA', "diff": 'NA', "percent": 'NA'}
                    result["checkGeneratedForList"]=[{"tag": 'NA', "description": 'NA', "tradingItemId": row["tradingItemId"],"fiscalYear": 'NA', "fiscalQuarter":'NA', "peo": 'NA',"value": 'NA',"units": 'NA',"currency": 'NA',"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": 'NA', "percent": 'NA'}]
                    errors.append(result)                    
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_10(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    left_dataItemIds_list=get_dataItemIds_list2('LHSdataItemIds', parameters)
    right_dataItemIds_list=get_dataItemIds_list2('RHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
            
    try:

        datacomb=list(zip(left_dataItemIds_list,right_dataItemIds_list))

        comparable=pd.DataFrame(datacomb,columns=['dataitem1','dataitem2']).drop_duplicates(inplace=False)

        extractedData_parsed['peocomb']=extractedData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)
        historicalData_parsed['peocomb']=historicalData_parsed.apply(lambda x:'%s%s' % (x['dataItemFlag'],x['peo']),axis=1)


        lhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(left_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']]
        rhs_df = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(right_dataItemIds_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value_scaled','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId']]                          
        
        
        if lhs_df["dataItemId"].nunique()!=len(left_dataItemIds_list):
            not_captured= [x for x in left_dataItemIds_list if x not in set(lhs_df["dataItemId"])]
            lhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']] #EBIT, EBITDA
            lhs_df = pd.concat([lhs_df,lhs_df_not_cap])
        extracted_dataItemIds_peo_count=extractedData_parsed['peo'].nunique()
        lhs_df_peo_count=lhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')   
    
        if (lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=lhs_df_peo_count[(lhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=lhs_df[lhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                lhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb'])) &(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())&~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                #lhs_df=lhs_df._append(lhs_df_missing_data,ignore_index=True)
                lhs_df = pd.concat([lhs_df,lhs_df_missing_data])
        if rhs_df["dataItemId"].nunique()!=len(right_dataItemIds_list):
            not_captured= [x for x in right_dataItemIds_list if x not in set(rhs_df["dataItemId"])]
            rhs_df_not_cap = historicalData_parsed[((historicalData_parsed['dataItemId'].isin(not_captured))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
            rhs_df = pd.concat([rhs_df,rhs_df_not_cap]) 

        rhs_df_peo_count=rhs_df.groupby(['dataItemId'])['peo'].nunique().reset_index(name='peocount')        
        if (rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count).any():
                missed_peo_tag=rhs_df_peo_count[(rhs_df_peo_count['peocount']<extracted_dataItemIds_peo_count)]['dataItemId']
                collected_peo=rhs_df[rhs_df['dataItemId'].isin(missed_peo_tag)][['dataItemId','peo']]
                rhs_df_missing_data=historicalData_parsed[((historicalData_parsed['dataItemId'].isin(missed_peo_tag))&(historicalData_parsed['peocomb'].isin(extractedData_parsed['peocomb']))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull())& ~((historicalData_parsed['peo'].isin(collected_peo['peo']))))][['dataItemId','peo','value_scaled','currency','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']]
                #rhs_df=rhs_df._append(rhs_df_missing_data,ignore_index=True)            
                rhs_df = pd.concat([rhs_df,rhs_df_missing_data])
        if len(lhs_df)>0 and len(rhs_df)>0:
            merged_df=pd.merge(lhs_df,rhs_df,on=['peo','parentFlag','accountingStandardDesc','fiscalChainSeriesId','tradingItemId'],how='inner')
            comparable['compressed']=comparable.apply(lambda x:'%s%s' % (x['dataitem1'],x['dataitem2']),axis=1)
            merged_df['compressed']=merged_df.apply(lambda x:'%s%s' % (x['dataItemId_x'],x['dataItemId_y']),axis=1)
            
            key_cols = ['compressed']
            dataitem_combination=merged_df.merge(comparable.loc[:, comparable.columns.isin(key_cols)])

            
            dataItemIds_x=[]
            dataItemIds_y=[]
            peos=[]
            parentflag=[]
            accounting=[]
            tid=[]
            fyc=[]        
            diff=[]
            perc=[]
            for ind,row in dataitem_combination.iterrows():

                if execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[0]):
                    dataItemIds_x.append(row['dataItemId_x'])
                    dataItemIds_y.append(row['dataItemId_y'])
                    peos.append(row['peo'])
                    tid.append(row['tradingItemId'])
                    parentflag.append(row['parentFlag']) 
                    accounting.append(row['accountingStandardDesc']) 
                    fyc.append(row['fiscalChainSeriesId'])
                    diff='NA'
                    perc='NA'
            diff_df=pd.DataFrame({"dataItemId_x":dataItemIds_x,"dataItemId_y":dataItemIds_y,"peo":peos,"diff":diff,"perc":perc,"tradingItemId":tid,"parentFlag":parentflag,"accountingStandardDesc":accounting,"fiscalChainSeriesId":fyc,})

            
            if len(diff_df)>0:
                diff_df['peocomb1']=diff_df[['dataItemId_x','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                diff_df['peocomb2']=diff_df[['dataItemId_y','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                extractedData_parsed['peocomb']=extractedData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                historicalData_parsed['peocomb']=historicalData_parsed[['dataItemId','peo','tradingItemId','parentFlag','accountingStandardDesc','fiscalChainSeriesId']].astype(str).apply(lambda x: ''.join(x),axis=1)
                
                
                temp1 = extractedData_parsed[(((extractedData_parsed['peocomb'].isin(diff_df['peocomb1'])) | (extractedData_parsed['peocomb'].isin(diff_df['peocomb2'])))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName']].drop_duplicates()
                                                                                                                                                                                                                                            
                temp2 = historicalData_parsed[(((historicalData_parsed['peocomb'].isin(diff_df['peocomb1'])) | (historicalData_parsed['peocomb'].isin(diff_df['peocomb2'])))&(historicalData_parsed['value']!="")&(historicalData_parsed['value'].notnull()))][['dataItemId','peo','estimatePeriodId','value','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','scale','currency','team','description','fiscalYear','fiscalQuarter','tradingItemName','versionId','companyId','feedFileId','filingDate']].drop_duplicates()
    
                temp1_revised=temp1.dropna()
                temp2_revised=temp2.dropna()

                if len(temp1_revised)>0:
                    
                    for ind, row in temp1_revised.iterrows():
                            
                        result = {"highlights": [], "error": "Tag has greater value than Related Tag"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":filingMetadata['metadata']['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'],"versionId": filingMetadata['metadata']['versionId'],"filingDate": filingMetadata['metadata']['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
                        errors.append(result)
    
                    for ind, row in temp2_revised.iterrows():

                        result = {"highlights": [], "error": "Tag has greater value than Related Tag"}
                        result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid":row['companyId']},"cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": row['versionId'],"versionId": row['versionId'],"filingDate": row['filingDate']})
                        result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": row['versionId'],"companyid": row['companyId'], "feedFileId": row['feedFileId'], "peo": row["peo"], "diff": 'NA', "percent": 'NA'}
                        result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"],"refFilingId":row["versionId"],"refFilingDate":row["filingDate"],"estimatePeriodId":row["estimatePeriodId"], "diff": 'NA', "percent": 'NA'}]
                        errors.append(result)
                                   
        print(errors) 
        return errors                                                                   
    except Exception as e:
        print(e)
        return errors

#Estimates Error Checks 
@add_method(Validation)
def AG_18(historicalData,filingMetadata,extractedData,parameters):
    errors = []
    dataItemId_list=get_dataItemIds_list('LHSdataItemIds', parameters)
    operator=get_dataItemIds_list('Operation', parameters)
    try:

        temp0 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','scale','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()
        temp1 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemId_list))&(extractedData_parsed['value']!="")&(extractedData_parsed['value'].notnull()))][['dataItemId','peo','value_scaled','currency','scale','parentFlag','accountingStandardDesc','tradingItemId','fiscalChainSeriesId','periodTypeId','fiscalYear','fiscalQuarter']].drop_duplicates()

        merged_df=pd.merge(temp0,temp1,on=['parentFlag','peo','accountingStandardDesc','fiscalChainSeriesId','fiscalYear','periodTypeId','tradingItemId'],how='inner')
                                                        
 
        dataItemIds=[]
        peos=[]
        tid=[]
        parentflag=[]
        accounting=[]
        fyc=[]
        diff=[]
        perc=[]
        
        for ind, row in merged_df.iterrows():           
            if (execute_operator(row['dataItemId_x'],row['dataItemId_y'],operator[0]) & execute_operator(row['value_scaled_x'],row['value_scaled_y'],operator[1])):
                dataItemIds.append(row['dataItemId_x'])
                dataItemIds.append(row['dataItemId_y'])
                peos.append(row['peo'])
                tid.append(row['tradingItemId'])
                parentflag.append(row['parentFlag']) 
                accounting.append(row['accountingStandardDesc']) 
                fyc.append(row['fiscalChainSeriesId'])
                diff='NA'
                perc='NA'
                
        diff_df=pd.DataFrame({"diff":diff,"perc":perc,'peo':peos}) 

        temp2 = extractedData_parsed[((extractedData_parsed['dataItemId'].isin(dataItemIds)) & (extractedData_parsed['peo'].isin(peos))&(extractedData_parsed['tradingItemId'].isin(tid))
                                                      &(extractedData_parsed['parentFlag'].isin(parentflag))&(extractedData_parsed['accountingStandardDesc'].isin(accounting))&(extractedData_parsed['fiscalChainSeriesId'].isin(fyc)))]
        
        for ind, row in temp2.iterrows():
            result = {"highlights": [], "error": "Same values collected for Different Data Items"}
            result["highlights"].append({"row": {"name": row['dataItemId'], "id": row["dataItemId"],"companyid": filingMetadata['metadata']['companyId']}, "cell": {"peo": row['peo'], "scale": row['scale'], "value": row['value'], "currency": row['currency']},"section": row['team'],"filingId": filingMetadata['metadata']['versionId'], "versionId": filingMetadata['metadata']['versionId'], "filingDate": filingMetadata['metadata']['filingDate']})
            result["checkGeneratedFor"]={"statement": "", "tag": row['dataItemId'], "description": row["description"], "versionId": filingMetadata['metadata']['versionId'],"companyid": filingMetadata['metadata']['companyId'], "feedFileId": filingMetadata['metadata']['feedFileId'], "peo": row["peo"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}
            result["checkGeneratedForList"]=[{"tag": row['dataItemId'], "description": row["description"], "tradingItemId": row["tradingItemId"],"fiscalYear": row["fiscalYear"], "fiscalQuarter":row["fiscalQuarter"], "peo": row["peo"],"value": row["value"],"units": row["scale"],"currency": row["currency"],"tradingItemName": row["tradingItemName"],"accountingStdDesc": row["accountingStandardDesc"],"parentConsolidatedFlag": row["parentFlag"],"fiscalChainSeries": row["fiscalChainSeriesId"], "diff": diff_df[diff_df['peo']==row["peo"]]['diff'].iloc[0], "percent": diff_df[diff_df['peo']==row["peo"]]['perc'].iloc[0]}]
            errors.append(result)
            
        print(errors)
        return errors
    except Exception as e:
        print(e)
        return errors



def runValidations(extractedData, executionData):
    
    global extractedData_parsed, historicalData_parsed, isDataParsed,currencyConversion_parsed
    import json

    extractedData = json.loads(extractedData)
    executionData = json.loads(executionData)
    # currencyConversion = json.loads(currencyConversion)

    # Calling json parser functions
    try:
        isDataParsed = False
        historicalData_parsed = parse_historical_data(extractedData['historicalData'])
        extractedData_parsed = parse_extracted_data(extractedData['extractedData'], convert_to_df = True)
        # currencyConversion_parsed = parse_conversion_data(extractedData['currencyConversion'],True)
        isDataParsed = True
    except:
        isDataParsed = False

    return Validation().validate(extractedData, executionData)

if __name__=="__main__":
    from sys import argv
    import os
    import json
    
    executionDataFile = r'C:\Users\gsravane\Downloads\ExecutionDataTesting.json'
    # executionDataFile = r'C:\Users\gsravane\Downloads\ExecutionDataTesting.json'
    
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksData (35).json'
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksData (36).json'
    extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksData (37).json'
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksData (39).json'
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksData (40).json'
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksDataActuals (21).json'
    # extractedDataFile = r'C:\Users\gsravane\Downloads\ErrorChecksDataActuals (20).json'
    #extractedDataFile = r'C:\Users\gsravane\Downloads\EstimatesErrorChecks.json'

    errors={}

    if os.path.exists(executionDataFile) and os.path.exists(extractedDataFile):
        with open(executionDataFile) as fp:
            #executionData = json.loads(fp.read().encode().decode("utf-8"))
            executionData = fp.read().encode().decode("utf-8")
        with open(extractedDataFile) as fp:
            #extractedData = json.loads(fp.read().encode().decode("utf-8"))
            extractedData = fp.read().encode().decode("utf-8")

            #v = Validation()
            #errors = v.validate(extractedData, executionData)
            errors = runValidations(extractedData, executionData)
            # print(str(errors))

    with open(os.path.join(executionDataFile+".validation"),'w+') as fp:
        import json
        result = json.dumps(errors, indent=4)
        fp.write(result)